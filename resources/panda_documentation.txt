PanDAWMS
PanDA
Nov 22, 2021
INTRODUCTION:
1
Introduction
3
2
Basic Concepts
5
3
Publications
13
4
User Guide
15
5
Administrator Guide
95
6
Developer Guide
103
7
System Architecture
109
8
Database
263
9
Installation
281
10 System Configuration Parameters in Database
291
11 Using PanDA Daemon
293
12 Brokerage
297
13 Job Sizing
305
14 Computing Resource Allocations
307
15 Dynamic Optimization of Task Parameters
309
16 Using Message Processor
313
17 Integration with CRIC
317
18 Deployment of Custom IAM
319
19 Working with iDDS
321
20 Job Retry Module
323
21 JEDI Watchdogs
325
Python Module Index
329
i
Index
331
ii
PanDAWMS
The Production and Distributed Analysis (PanDA) is a data-driven workload management system capable of operat-
ing at massive data processing scale, designed to have the flexibility to adapt to emerging computing technologies in
processing, storage, networking and distributed computing middleware.
INTRODUCTION:
1
PanDAWMS
2
INTRODUCTION:
CHAPTER
ONE
INTRODUCTION
The Production and Distributed Analysis (PanDA) system has been developed to meet ATLAS production and analysis
requirements for a data-driven workload management system capable of operating at the LHC data processing scale.
PanDA scalability has been demonstrated in ATLAS through the rapid increase in usage over the last decade. PanDA
was designed to have the flexibility to adapt to emerging computing technologies in processing, storage, networking,
and distributed computing middleware.
Fig. 1: PanDA system overview
The flexibility has been successfully demonstrated through the past decade of evolving technologies adopted by com-
puting centers in ATLAS, spanning many continents. PanDA performed very well, including the LHC data-taking
3
PanDAWMS
period. The system has been producing high volumes of Monte Carlo samples and making large-scale diverse comput-
ing resources available for individual analysis. A wide range of computing resources have been seamlessly integrated,
such as WLCG grid, commercial cloud, volunteer computing, and high-performance computing resources. There are
typically 600,000 jobs concurrently running in the system, and more than 5 million jobs are processed in total per week.
Fig. 2: The number of jobs concurrently running in ATLAS PanDA system
This proven scalability and flexibility make PanDA well suited for adoption by various exabyte scale sciences. The
interest in PanDA by other big data sciences brought the primary motivation to generalize PanDA, providing loca-
tion transparency of processing and data management, for High Energy Physics community and other data-intensive
sciences and a wider exascale community.
4
Chapter 1. Introduction
CHAPTER
TWO
BASIC CONCEPTS
• Computing and storage resources
• Virtual organization
• PanDA components
• Task
• Job
• Scout job
• Brokerage
• Pull and push
• Heartbeat
• Users
• Global share
• Priority
• Task and job retry
2.1 Computing and storage resources
Computing resource providers offer the computing resources with processing capabilities, such as the grid, HPC centers,
and commercial cloud services. A worker node is the minimum unit in each computing resource, which is a (virtual)
host, a host cluster, or a slot on a host, depending on workload or resource configuration, and represents a combination
of CPUs, memory, and a scratch disk to process workload. Storage resource providers accommodate data storage needs.
A storage resource is composed of a persistent data storage with disk, tape, or their hybrid, and a storage management
service running on top of it. Association between computing and storage resources can be arbitrary but in most cases
resources from the same provider are associated with each other.
PanDA integrates heterogeneous computing and storage resources to provide a consistent interface to users. Users can
seamlessly process their workload on computing resources while taking input data from storage resources and uploading
output data to storage resources, without paying attention to the details of computing and storage technologies.
5
PanDAWMS
2.2 Virtual organization
A virtual organization (VO) refers to a dynamic set of individuals defined around a set of resource-sharing rules and
conditions. Its members are geographically apart but working for a common objective, such as scientific project,
research program, etc.
2.3 PanDA components
There are five components in the PanDA system, as shown in the schematic view above.
• JEDI is a high-level engine to tailor workload for optimal usages of heterogeneous resources dynamically.
• PanDA server is the central hub implemented as a stateless RESTful web service to allow asynchronous com-
munication from users, Pilot, and Harvester over HTTPS.
• Pilot is a transient agent to execute a tailored workload (= a job: to be explained in a following section) on a
worker node, reporting periodically various metrics to the PanDA server throughout its lifetime.
• Harvester provisions the Pilot on resources using the relevant communication protocol for each resource provider
and communicates with PanDA server on behalf of the Pilot if necessary.
6
Chapter 2. Basic Concepts
PanDAWMS
• PanDA monitor is a web-based monitoring of tasks and jobs processed by PanDA, providing a common interface
for end users, central operations team and remote site administrators.
JEDI and the PanDA server share the central database to record the status of tasks and jobs. PanDA monitor reads this
central database to offer the different views. Harvester uses its own more lightweight and more transient database, which
can be either central or local depending on the deployment model. PanDA components and database are explained in
System Architecture and Database pages, respectively.
2.4 Task
A task is a unit of workload to accomplish an indivisible scientific objective. If an objective is done in multiple steps,
each step is mapped to a task. A task takes input and produces output. The goal of the task is to process the input
entirely. Generally, input and output are collections of data files, but there are also other formats, such as a group of
sequence numbers, metadata, notification, void, etc. Each task has a unique identifier JediTaskID in the system.
Task status changes as shown in the following figure.
Yellow boxes in the figure show the commands sent to PanDA by external actors to trigger task status transition. Here
is the list of task statuses and their descriptions.
2.4. Task
7
PanDAWMS
registered The task was injected into PanDA.
defined All task parameters were properly parsed.
assigning The task is being assigned to a storage resource.
ready The task is ready to generate jobs.
pending The task has a temporary problem, e.g., there are no free computing resources for new jobs.
scouting The task is running scout jobs to gather job metrics.
scouted Enough number of scout jobs were successfully finished and job metrics were calculated.
running The task avalanches to generate more jobs.
prepared The workload of the task was done, and the task is ready to run the postprocessing step.
done The entire workload of the task was successfully processed.
failed The entire workload of the task was failed.
finished The workload of the task partially succeeded.
aborting The task got the kill command.
aborted The task was killed.
finishing The task got the finish command to terminate processing while it was still running.
topreprocess The task is ready to run the preprocessing step.
preprocessing The task is running the preprocessing step.
tobroken The task is going to be broken.
broken The task is broken, e.g., due to wrong parameters.
toretry The task got the retry command.
toincexec The task got the incexec (incremental execution) command to retry a task with new task parameters after
looking up the input data. This is typically useful when new data are appended to the input data and require
changes in some task parameters.
rerefine The task is changing parameters for incremental execution.
paused The task is paused and doesn’t do anything until it gets the resume command.
throttled The task is throttled not to generate new jobs.
2.5 Job
A job is an artificial workload sub-unit partitioned from a task. A single task is composed of multiple jobs, and each
job runs on the minimum set of the computing resource. Each job is tailored based on the user’s preference (if any)
and/or constraints of the computing resource. For example, if the job size is flexible, jobs are generated to have a short
execution time and produce small output files when being processed on resources with limited time slots and local
scratch disk spaces. The task input is logically split into multiple subsets, and each job gets a subset to produce output.
The collection of job output is the task output. Each job has a unique identifier PanDA ID in the system. Generally,
one pilot processes one job on a worker node. However, it is possible to configure the pilot to process multiple jobs
sequentially or concurrently on a worker node if the computing resources allow such configurations, reducing the
number of interactions with those resources.
8
Chapter 2. Basic Concepts
PanDAWMS
The status of jobs sequentially changes as follows:
pending The job is generated.
defined The job is ready to work for global input data motion if necessary. E.g., data transfer from a remote storage
resource to the “local” storage resource close to the computing resource.
assigned Input data are being transferred to the “local” storage resource. This status is skipped if the job doesn’t need
global input data motion or physical input data.
activated The input data has been transferred correctly and the job is ready to be fetched by a running pilot.
sent The job was fetched by a pilot running on the computing resource.
starting The job is working for the last-mile input data motion, such as data stage-in from the “local” storage to the
scratch disk attached to the computing resource.
running The job is processing input data.
holding The job finished processing, released the computing resource, reported the final metrics to the PanDA server,
uploaded output files to the local storage. Note that jobs don’t use any computing resources any longer in this
and subsequent job statuses.
merging Output data are being merged. This status is skipped unless the task is configured to merge job output.
transferring Output data are being transferred from the local storage to the final destination.
And goes to one of the final statues described below:
finished The job successfully produced output, and it is available at the final destination.
failed The job failed during execution or data management.
closed The system terminated the job before running on a computing resource.
cancelled The job was manually aborted.
2.6 Scout job
Each task generates a small number of jobs using a small portion of input data. They are scout jobs to collect various
metrics such as data processing rate and memory footprints. Tasks use those metrics to generate jobs for remaining
input data more optimally.
2.7 Brokerage
There are two brokerages in JEDI: task brokerage and job brokerage.
The task brokerage assigns tasks to storage resources if those tasks are configured to aggregate output, but final desti-
nations are undefined.
On the other hand, the job brokerage assigns jobs to computing resources. A single task can generate many jobs, and
they can be assigned to multiple computing resources unless the task is configured to process the whole workload at a
single computing resource. The details of brokerage algorithms are described in the Brokerage page.
2.6. Scout job
9
PanDAWMS
2.8 Pull and push
Users submit tasks to JEDI through the PanDA server, JEDI generates jobs on behalf of users and passes them to the
PanDA server and the PanDA server centrally pools the jobs. There are two modes for the PanDA server to dispatch
jobs to computing resources: the pull and push modes.
In pull mode, blank pilots are provisioned first on computing resources, and they fetch jobs once CPUs become avail-
able. It is possible to trigger the pilot provisioning well before generating jobs. Thus jobs can start processing as soon
as they are generated, even if there is long latency for provisioning in the computing resource. Another advantage is
the capability to postpone the decision making to bind jobs with CPUs until the last minute, which allows fine-grained
job scheduling with various job attributes, e.g. increasing the chance for new jobs with a higher priority share to jump
over old jobs in a lower priority share.
On the other hand, in push mode pilots are provisioned on computing resources together with a preassigned jobs. Job
scheduling merely relies on the scheduling mechanisms in the computing resources. The pilot specifies requirements
for each job. The mechanisms dynamically configure a worker with CPUs, memory size, execution time limit, and so
on, which is typically more optimal for special resources like HPCs and GPU clusters.
2.9 Heartbeat
The pilot periodically sends heartbeat messages to the PanDA server via a short-lived HTTPS connection to report
various metrics while executing a job on a worker node. Heartbeats guarantee that the pilot is still alive as the PanDA
server and the pilot don’t maintain a permanent network connection. If the PanDA server doesn’t receive heartbeats
from the pilot during a specific period, the PanDA server presumes that the pilot is dead and kills the job being executed
by the pilot.
2.10 Users
Users process workloads on PanDA to accomplish their objectives. PanDA authenticates and authorizes them to access
the computing and storage resources based on their profile information. The Identity and access management page
explains the details of PanDA’s authentication and authorization mechanism. Users can be added to one or more
working groups in the identity and access management system to process “public” workloads for those communities.
Resource usages of private and public workloads are separated. Tasks and jobs have the working group attribute to
indicate for which working groups they are.
10
Chapter 2. Basic Concepts
PanDAWMS
2.11 Global share
Global shares define the allocation of computing resources among various working groups and/or user activities. The
aggregation of available computing resources are dynamically partitioned to multiple global shares. Each task is
mapped to a global share according to its working group and activity type. Many components in JEDI and the PanDA
server work with global shares. See the Resource Allocations page for the details.
2.12 Priority
The priority of a task or job determines which task or job has precedence over other competing tasks or jobs in the
same global share. Their priorities are relevant in each global share: i.e. high-priority tasks in a global share don’t
interfere with low-priority tasks in another global share. Generally jobs inherit the priority of its task, but scout jobs
have higher priorities to collect various metrics as soon as possible.
2.13 Task and job retry
It is possible to retry tasks if a part of input data were not successfully processed or new data were added to input data.
The task status changes from finished or done back to running, and output data are appended to the same output data
collection. Tasks cannot be retried if they end up with a fatal status, such as broken and failed since they are hopeless
and not worth retrying.
On the other hand, the job status is irreversible, i.e., jobs don’t change their status once they go to a final status. JEDI
generates new jobs to re-process the input data portion, which was not successfully processed by previous jobs. Config-
uration of retried jobs can be optimized based on experiences with previous jobs (e.g. increased memory requirements).
It is also possible to configure rules to avoid the job retrial for hopeless error codes/messages.
2.11. Global share
11
PanDAWMS
12
Chapter 2. Basic Concepts
CHAPTER
THREE
PUBLICATIONS
Table of Contents
• 2016
• 2018
• 2019
3.1 2016
ATLAS WORLD-cloud and networking in PanDA, CHEP 2016
3.2 2018
ATLAS Global Shares implementation in PanDA, CHEP 2018
The future of distributed computing systems in ATLAS: Boldly venturing beyond grids, CHEP 2018
Harvester : an edge service harvesting heterogeneous resources for ATLAS, CHEP 2018
The Data Ocean project : An ATLAS and Google R&D collaboration, CHEP 2018
3.3 2019
Managing the ATLAS Grid through Harvester, CHEP 2019
Using Kubernetes as an ATLAS computing site, CHEP 2019
13
PanDAWMS
14
Chapter 3. Publications
CHAPTER
FOUR
USER GUIDE
There are client modules and tools for users to send commands to the PanDA server using standard HTTP methods.
The PanDA server, by default, listens on a port 25080 for plain HTTP and another port 25443 for HTTP over SSL.
Make sure that your local firewall doesn’t block access to those ports.
The first part of this page is for end-users to use PanDA for their analysis, while the second part is for developers and
can be skipped unless you intend to develop applications on top of Python API.
PanDA also provides advanced Web-based monitoring, so-called PanDA monitoring, for all kinds of users. The third
part of this page describes how to use PanDA monitoring.
4.1 For end-users
4.1.1 panda-client
The panda-client package includes python modules and command-line tools to allow end-users to submit/manage their
analysis on the PanDA system. This package is supposed to be used by end-users as functionality is simplified and
limited. System administrators or developers should refer to API reference.
Installation
panda-client works either python 2 and 3, and is self-contained so that you don’t have to install an external package
or software. The installation step can be skipped if panda-client has been centrally installed like ATLAS. Otherwise,
simply run the following pip command in a virtual environment to install all python modules, command-line tools and
configuration files:
$ pip install panda-client
If you install panda-client in JupyterLab,
$ pip install panda-client[jupyter]
will install extra packages in addition to panda-client.
15
PanDAWMS
Setup
The setup file panda_setup.(c)sh is automatically generated under $VIRTUAL_ENV/etc/panda when panda-client
is installed via pip. You need to source the file to setup the required environment variables before using the python
modules and command-line tools. It is good to define a shell function to source the setup file as shown in the ATLAS
users tag.
pip-installed
ATLAS users
source $VIRTUAL_ENV/etc/panda/panda_setup.sh
export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase
setupATLAS
lsetup panda
The following environment variables need to change if necessary.
Name
Description
Example
PANDA_URL_SSL
Base HTTPS URL of PanDA server
https://ai-idds-01.cern.
ch:25443/server/panda
PANDA_URL
Base HTTP URL of PanDA server
http://ai-idds-01.cern.ch:
25080/server/panda
PANDA_AUTH_VO
Virtual
organization
name
(required
only
when
PANDA_AUTH=oidc)
wlcg
PANDA_AUTH
Authentication mechanism. oidc to enable OIDC/OAuth2.0.
x509_no_grid to use X509 without grid niddleware
oidc
PANDA_VERIFY_HOST
Set off to disable the host verification
off
PANDA_USE_NATIVE_HTTPLIB
Set 1 to use native http lib instead of curl
1
4.1.2 Running ordinary analysis
prun is the command-line tool to allow users to run any application on PanDA. The application could be ROOT (CINT,
C++, pyRoot), python, user’s executable, shell script, and so on, and must be made available on computing resources
beforehand. prun sends all files except too large files under the current directory to computing resources. Users
can construct arbitrary runtime environments there and can do anything in principle. However, please avoid careless
network operations connecting to remote servers (e.g., git clone and wget) unless the remote servers permit them. Your
task is split into many jobs, and these jobs are executed in parallel, so that those operations tend to become a DDoS
attack and easily break the remote servers.
Table of Contents
• How to run
– Hello world
– Running with input files and a separate build step
– Running python
– Running stand-alone containers
16
Chapter 4. User Guide
PanDAWMS
• FAQ
– Output with wildcards
– Send jobs to particular computing resources
– Filtering files in the input dataset
– How to use multiple input datasets
– Merge output files
– Adding job metadata
– How to run the task in parallel while the parent task is still running
– Difference between --useAthenaPackage and --athenaTag
How to run
$ prun --helpGroup ALL
shows all available options.
Hello world
Here is an Hello world example.
$ prun --exec "pwd; ls; echo Hello-world > myout.txt" --outDS user.hoge.`uuidgen` --nJobs 3 --output myo
where --exec takes the execution string which is executed on remote computing resources, and --outDS takes the
basename of the output collection.
This task generates 1 build job to setup and 3 run jobs to produce myout.txt, and creates two collections (e.g., dataset
containers); one for output data and the other for log files. The next section explains what the build job is. Each run
job executes pwd; ls; echo Hello-World > myout.txt and produces an output file, myout.txt. The output file is renamed
to <jediTakID>._<serial_number>.myout.txt to be unique in the entire system and is then added to the output data
collection. Each run job also produces a tarball of log files <jediTakID>._<serial_number>.log.tgz and adds it to the
log collection.
This example doesn’t take input data, so it will generate 3 identical jobs. It is possible to give unique numbers to jobs
by adding %RNDM:basenumber in --exec which is incremented per job. E.g.,
$ prun --exec "echo %RNDM:123 %RNDM:456"" ...
The first job executes “echo 123 456”, the second job executes “echo 124 457”, and so on.
4.1. For end-users
17
PanDAWMS
Running with input files and a separate build step
Large input data must be transferred via data management systems instead of directly sending them from current directly.
You can specify the name of input data collection (e.g., input dataset name or container name) in --inDS. %IN is
a placeholder in --exec to be replaced with a comma-concatenated list of input data (e.g., filenames) when being
executed on computing resources.
It would be suboptimal to build runtime environment and run the application in each job, if the build step is time-
consuming. It is possible to execute the build step only once in a build job and let many run jobs share the runtime.
The following example compiles a ROOT C++ source file in build jobs on remote computing resources and starts many
run jobs with input files once build jobs finished.
First, you need to prepare source files and Makefile locally. For example,
$ cat cpptest.cc
#include <string>
#include <vector>
#include <iostream>
#include <stdlib.h>
#include "TROOT.h"
#include "TFile.h"
#include "TTree.h"
#include "TChain.h"
#include "TBranch.h"
int main(int argc, char **argv)
{
// split by ','
std::string argStr = argv[1];
std::vector<std::string> fileList;
for (size_t i=0,n; i <= argStr.length(); i=n+1)
{
n = argStr.find_first_of(',',i);
if (n == std::string::npos)
n = argStr.length();
std::string tmp = argStr.substr(i,n-i);
fileList.push_back(tmp);
}
// open input files
TChain fChain("CollectionTree");
for (unsigned int iFile=0; iFile<fileList.size(); ++iFile)
{
std::cout << "open " << fileList[iFile].c_str() << std::endl;
fChain.Add(fileList[iFile].c_str());
}
Int_t
EventNumber;
TBranch
*b_EventNumber;
fChain.SetBranchAddress("EventNumber", &EventNumber, &b_EventNumber);
(continues on next page)
18
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
// main loop
Long64_t nentries = fChain.GetEntriesFast();
for (Long64_t jentry=0; jentry<nentries;jentry++)
{
Long64_t ientry = fChain.LoadTree(jentry);
if (ientry < 0)
break;
fChain.GetEntry(jentry);
std::cout << EventNumber << std::endl;
}
}
Make file could be something like
$ cat Makefile
ROOTCFLAGS
= $(shell root-config --cflags)
ROOTLIBS
= $(shell root-config --libs)
ROOTGLIBS
= $(shell root-config --glibs)
CXX
= g++
CXXFLAGS
=-I$(ROOTSYS)/include -O -Wall -fPIC
LD
= g++
LDFLAGS
= -g
SOFLAGS
= -shared
CXXFLAGS
+= $(ROOTCFLAGS)
LIBS
= $(ROOTLIBS)
GLIBS
= $(ROOTGLIBS)
OBJS
= cpptest.o
cpptest: $(OBJS)
$(CXX) -o $@ $(OBJS) $(CXXFLAGS) $(LIBS)
# suffix rule
.cc.o:
$(CXX) -c $(CXXFLAGS) $(GDBFLAGS) $<
# clean
clean\:
rm -f *~ *.o *.o~ core
Then
$ prun --exec "cpptest %IN" --bexec "make" --inDS valid1.006384.PythiaH120gamgam.recon.AOD.e322_s412_r57
prun sends files including cpptest.cc and Makefile in the current directory to remote computing resources. Note that a
build job is generated for each computing resource if the task is split to multiple comput resources for parallel execution.
The build job executess the argument of --bexec to produce binary files, and then run jobs get started with those
binary files. %IN is dynamically converted to a commma-concatenated filenames in the input data collection specified
by --inDS.
4.1. For end-users
19
PanDAWMS
Running python
This example runs a python job.
$ cat purepython.py
import sys
print sys.argv
f = open('out.dat','w')
f.write('hello')
f.close()
sys.exit(0)
Then
$ prun --exec "python purepython.py %IN" --inDS ...
It will run with the system python on the remote resource.
Running stand-alone containers
It is possible run standalone containers by using --containerImage option.
$ prun --containerImage docker://alpine --exec "echo Hello World" --outDS user.hoge.`uuidgen`
Your job will download the docker image and execute echo in the container. --containerImage can also take the
CVMFS path if the the image is unpacked in CVMFS. This has the advantage for each job to avoid downloading the
image.
$ prun --containerImage /cvmfs/unpacked.cern.ch/registry.hub.docker.com/atlasml/ml-base:latest --exec "e
IO is done through the initial working directory $PWD where the container is launched. The working directly is
mounted to /srv/workDir. It is recommended to dynamically get the path of the initial working directory using
os.getcwd(), echo $PWD, and so on, when the application is executed in the container rather than hard-coding /
srv/workDir in the application, since the convention might be changed in the future.
$ prun --containerImage docker://atlasml/ml-base --exec "my_command %IN" --outputs my-output-file.h5 --f
Input files are copied to $PWD even if the computing resource is configured to read files directly from the storage
resource since --forceStaged" option is used. `%IN` in ``--exec is replaced to a comma-concatenated
list of the copied input files. It is user’s responsibility to copy output files to $PWD, i.e., my_command in this example
has to put my-output-file.h5 to $PWD, then the system takes care of subsequent procedures like renaming and stage-out.
FAQ
Output with wildcards
When the number of output files produced by each job or a part of their filenames is unknown, it is possible to specify
their names with wildcards in --outputs option.
$ prun --outputs "abc.data,JiveXML_*.xml" ...
20
Chapter 4. User Guide
PanDAWMS
Each job will have two output files, <jediTaskID>.<serial number>.abc.data and <jediTaskID>.<serial num-
ber>.JiveXML_XYZ.xml.tgz. The latter is a tarball of all JiveXML_*.xml produced by the job. Note that you need
to escape the wildcard character using \ or “” to disable shell-globing, i.e. JiveXML_\*.xml or “JiveXML_*.xml”.
Send jobs to particular computing resources
The system automatically chooses appropriate computing resources by using various information like data locality,
resource occupancy, and user’s profile. However, users can still send jobs to particular sites using --site option. e.g.,
$ prun --site TRIUMF ...
Filtering files in the input dataset
The --match option allows user to choose files matching a given pattern. The argument is a comma-separated string.
$ prun --match "*AOD*" ...
$ prun --match "*r123*,*r345*" ...
If you need to skip specific files, use the --antiMatch option.
How to use multiple input datasets
If you just want to submit a single task running on multiple datasets, you just need to specify a comma-separated list
of input datasets.
$ prun --inDS dsA,dsB,dsC,dsD ...
However, if you want to read multiple datasets in each job, i.e., one for signal and the other for background, you need
something more complicated. The --secondaryDSs option specifies secondary dataset names. The argument is a
comma-separated list of StreamName:nFilesPerJob:DatasetName[:MatchingPattern[:nSkipFiles]] where
StreamName the name of stream in the –exec argument
nFilesPerJob the number of files per subjob
DatasetName the dataset name
MatchingPattern to use files matching a pattern (can be omitted)
nSkipFiles to skip files (can be omitted)
For example,
$ prun --exec "test %IN %IN2 %IN3" --secondaryDSs IN2:3:data19.106017.gg2WW0240_JIMMY_WW_taunutaunu.reco
%IN2 and %IN3 will be replaced with actual filenames in data19.blah and mc08.blah, respectively, when jobs get
started, while %IN is replaced with files in --inDS.
Note that when dataset containers are used for secondaryDSs like StreamName:nFilesPerJob:ContainerName they are
expanded to constituent datasets and each job takes nFilesPerJob files from each constituent dataset. This means that if
a dataset container has M constituent datasets a single job cound take M x nFilesPerJob files from the dataset container.
There are --notExpandInDS and --notExpandSecDS options so that jobs don’t expand dataset containers, use files
across dataset boundaries in dataset containers, and take only nFilesPerJob files from each dataset container.
4.1. For end-users
21
PanDAWMS
Merge output files
The --mergeOutput option merges output files on the fly. E.g.,
$ prun ... --mergeOutput --mergeScript="your_merger.py -o %OUT -i %IN"
Merge jobs (pmerge jobs) are generated once run jobs produce premerged files. Each merge job executes the application
described above to merge %IN will be replaced with a comma-separated list of premerged filenames, and %OUT
replaced with the final output filename, when merge jobs get started. Each merge job merges the premerged files using
Merging_trf.py for pool files, hadd for ROOT hist and ntuple, gzip for log and text, or the application specified in the
--mergeScript option.
Adding job metadata
Users can add metadata to each job in PanDA. If jobs produce json files userJobMetadata.json in the run directory it is
uploaded to PanDA and you can see it in pandamon or pbook. This is typically useful if jobs have very small outputs,
such as hyperparameter optimization for machine learning where each job could produce only one value. Users can
get results directly from PanDA rather than uploading/downloading small files to/from storages. Note that the size of
each metadata must be less than 1MB and metadata are available only for successfully finished jobs. First you need to
change your application to produce a json file, e.g.
$ cat a.py
# do something useful and then
import json
json.dump({'aaaaaa':'bbbbbb', 'ccc':[1,2,5]}, open('userJobMetadata.json', 'w'))
Then submit tasks as usual. You don’t need any special option. E.g.,
$ prun --exec 'python a.py' --outDS user.hage.`uuidgen`
Once jobs have successfully finished you can see metadata in the job metadata field in the job page of PanDA monitor.
You can fetch a json dump through https://bigpanda.cern.ch/jobs/?jeditaskid=<taskID>&fields=metastruct&json or in
pbook
$ pbook
>>> getUserJobMetadata(taskID, output_json_filename)
or through end-user python API.
How to run the task in parallel while the parent task is still running
It is possible to sequentially chain tasks using the --parentTaskID option. A typical use-case is as follows:
1. A parent task is running to produce some datasets.
2. A child task is submitted to use one or more datasets as input which the parent is producing.
3. The child task periodically checks the input datasets and generates jobs if new files are available.
22
Chapter 4. User Guide
PanDAWMS
4. Finally, the child task is finished once the parent is finished and all files produced by the parent have been pro-
cessed.
The --parentTaskID option takes the taskID of the parent task that is producing --inDS. Note that if the child task
is submitted without the --parentTaskID option, it will run only on the available files when the task is submitted.
Difference between --useAthenaPackage and --athenaTag
Both options set up Athena on remote compute nodes. The main difference is as follows. --useAthenaPackage
requires Athena runtime environment on your local computer to automatically configure the task by parsing environ-
ment variables and make a sandbox file by using cpack, which is included in Athena, according to Athena’s directory
structure. On the other hand, --athenaTag doesn’t need Athena locally. It gathers files in the current directory when
making a sandbox file and passes the argument string to asetup executed on remote compute nodes.
4.1.3 Running hyperparameter optimization
Table of Contents
• Introduction
• Preparation
• phpo
• How to submit HPO tasks
• FAQ
– Protection against bad hyperparameter points
– Visualization of the search results
– Relationship between nPointsPerIteration and minUnevaluatedPoints
– What “Logged status: skipped since no HP point to evaluate or enough concurrent HPO jobs” means
in PanDA monitor
– Checkpointing
– Segmented HPO
4.1. For end-users
23
PanDAWMS
Introduction
PanDA and iDDS integrate geographically distributed GPU/CPU resources, so that users can run special analysis to
automatically optimize hyperparameters of machine learning models.
Each HPO workflow is typically composed of iterations of three steps:
• Sampling step To choose hyperparameter points in a search space.
• Training step To evaluate each hyperparameter point with an objective function and a training dataset.
• Optimization step To redefine the search space based on loss values of evaluated hyperparameter points.
In the system, the sampling and optimization steps are executed on central resources while the training step is executed
on distributed resources. The former is called steering and the latter is called evaluation, i.e., steering = sampling
+ optimization, and evaluation = training. Users can submit HPO tasks to PanDA using phpo, which is available in
panda-client-1.4.24 or higher.
Once tasks are injected into the system, iDDS orchestrates JEDI, PanDA, Harvester, and the pilot to timely execute
HPO steps on relevant resources, as shown in the figure above. Users can see what’s going on in the system using
PanDA monitoring. The iDDS document explains how the system works, but end-users don’t have to know all the
details. However, one important thing is that a single PanDA job evaluates one or more hyperparameter points and thus
it is good to have a look at log files in PanDA monitoring if there is something wrong.
24
Chapter 4. User Guide
PanDAWMS
Preparation
The main trick to run hyperparameter optimization (HPO) on distributed resources is the separation of steering and
evaluation and their asynchronous execution, as shown in the figure below.
Users require two types of containers, one for steering and the other for evaluation.
For steering, users can use predefined containers or their own containers. Note that users need to use ML packages such
as skopt and nevergrad which support the ask-and-tell pattern, when making own steering containers. This page explains
how steering containers communicate with the system. Users provide execution strings to specify how to generate new
hyperparameter points in their steering containers. Each execution string would contain several placeholders which are
dynamically replaced with actual values when the containers are executed. Input and output are done through json files
in the initial working directly $PWD so that the directory needs to be initially mounted.
For evaluation, users also provide execution strings to specify what is executed in their containers to train the ML
model. There are two files for input (one for a hyperparameter point to be evaluated and the other for training data) and
three files for output (the first one to report the loss, the second one to report job metadata, and the last one to preserve
training metrics). The input file for a hyperparameter point and the output file to report the loss are mandatory, while
other files are optional. See the this page for the details of their format. Note that evaluation containers are executed
in the read-only mode, so that file-writing operations have to be done in the initial working directory /srv/workDir
which is bound to the host working directory where containers and the system communicate using the input and output
files. It is better to dynamically get the path of the initial working directory using os.getcwd(), echo $PWD, and so on,
when applications are executed in evaluation containers, rather than hard-coding /srv/workDir in the applications,
since the convention might be changed in the future.
phpo
phpo is a command-line tool specialized for HPO task submission. The following options are available in addition to
usual options such as --site and --verbose. All options can be loaded from a json using --loadJson if preferable.
--nParallelEvaluation NPARALLELEVALUATION
The number of hyperparameter points being evaluated
concurrently. 1 by default
--maxPoints MAXPOINTS
The max number of hyperparameter points to be
evaluated in the entire search. 10 by default.
--maxEvaluationJobs MAXEVALUATIONJOBS
(continues on next page)
4.1. For end-users
25
PanDAWMS
(continued from previous page)
The max number of evaluation jobs in the entire
search. 2*maxPoints by default. The task is terminated
when all hyperparameter points are evaluated or the
number of evaluation jobs reaches MAXEVALUATIONJOBS
For steering,
--nPointsPerIteration NPOINTSPERITERATION
The max number of hyperparameter points generated in
each iteration. 2 by default Simply speaking, the
steering container is executed
maxPoints/nPointsPerIteration times when
minUnevaluatedPoints is 0. The number of new points is
nPointsPerIteration-minUnevaluatedPoints
--minUnevaluatedPoints MINUNEVALUATEDPOINTS
The next iteration is triggered to generate new
hyperparameter points when the number of unevaluated
hyperparameter points goes below minUnevaluatedPoints.
0 by default
--steeringContainer STEERINGCONTAINER
The container image for steering run by docker
--steeringExec STEERINGEXEC
Execution string for steering. If --steeringContainer
is specified, the string is executed inside of the
container. Otherwise, the string is used as command-
line arguments for the docker command
--searchSpaceFile SEARCHSPACEFILE
External json filename to define the search space.
None by default. If this option is used together with
--segmentSpecFile the json file contains a list of
search space dictionaries. It is possible to contain
only one search space dictionary if all models use the
search space. In this case the search space dictionary
is cloned for every segment
For evaluation,
--evaluationContainer EVALUATIONCONTAINER
The container image for evaluation
--evaluationExec EVALUATIONEXEC
Execution string to run evaluation in singularity.
--evaluationInput EVALUATIONINPUT
Input filename for evaluation where a json-formatted
hyperparameter point is placed. input.json by default
--evaluationTrainingData EVALUATIONTRAININGDATA
Input filename for evaluation where a json-formatted
list of training data filenames is placed.
input_ds.json by default. Can be omitted if the
payload directly fetches the training data using wget
or something
--evaluationOutput EVALUATIONOUTPUT
Output filename of evaluation. output.json by default
(continues on next page)
26
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
--evaluationMeta EVALUATIONMETA
The name of metadata file produced by evaluation
--evaluationMetrics EVALUATIONMETRICS
The name of metrics file produced by evaluation
--trainingDS TRAININGDS
Name of training dataset
--checkPointToSave CHECKPOINTTOSAVE
A comma-separated list of files and/or directories to
be periodically saved to a tarball for checkpointing.
Note that those files and directories must be placed
in the working directory. None by default
--checkPointToLoad CHECKPOINTTOLOAD
The name of the saved tarball for checkpointing. The
tarball is given to the evaluation container when the
training is resumed, if this option is specified.
Otherwise, the tarball is automatically extracted in
the working directories
--checkPointInterval CHECKPOINTINTERVAL
Frequency to check files for checkpointing in minute.
5 by default
To see latest or full list of options,
$ phpo --helpGroup ALL
How to submit HPO tasks
There are a couple of concrete examples in this HPO page.
The most important options of phpo are --steeringContainer, --steeringExec, --evaluationContainer, and
--evaluationExec, i.e., container names for steering and evaluation, and what is executed in each container. Here is
an example to show how those options look like.
$ cat config_dev.json
{
"evaluationContainer": "docker://gitlab-registry.cern.ch/zhangruihpc/
˓→evaluationcontainer:mlflow",
"evaluationExec": "bash ./exec_in_container.sh",
"evaluationMetrics": "metrics.tgz",
"searchSpaceFile": "search_space_example2.json",
"steeringExec": "/bin/bash -c \"hpogrid generate --n_point=%NUM_POINTS --max_point=
˓→%MAX_POINTS --infile=$PWD/%IN
--outfile=$PWD/%OUT -l=nevergrad\"",
"steeringContainer": "gitlab-registry.cern.ch/zhangruihpc/steeringcontainer:latest",
"trainingDS": "user.hoge.my_training_dataset",
}
Note that the execution string for the evaluation container is written in a local file exec_in_container.sh. All files
with *.json, *.sh, *.py, *.yaml in the local current directory are automatically sent to the remote working directory. So
users don’t have to specify a complicated execution string in --evaluationExec. E.g.
4.1. For end-users
27
PanDAWMS
$ cat exec_in_container.sh
export CURRENT_DIR=$PWD
export CALO_DNN_DIR=/ATLASMLHPO/payload/CaloImageDNN
export PYTHONPATH=$PYTHONPATH:$CALO_DNN_DIR/deepcalo
curl -sSL https://cernbox.cern.ch/index.php/s/HfHYEsmJNWiefu3/download | tar -xzvf -;
python $CALO_DNN_DIR/scripts/make_input.py input.json input_new.json
cp -r $CALO_DNN_DIR/exp_scalars $CURRENT_DIR/
python /ATLASMLHPO/payload/CaloImageDNN/run_model.py -i input_new.json --exp_dir
˓→$CURRENT_DIR/exp_scalars/ --data_path $CURRENT_DIR/dataset/event100.h5 --rm_bad_reco␣
˓→True --zee_only True -g 0
rm -fr $CURRENT_DIR/exp_scalars/
tar cvfz $CURRENT_DIR/metrics.tgz mlruns/*
rm -fr mlruns dataset
ls $CURRENT_DIR/
The initial search space can be described in a json file.
$ cat search_space_example2.json
{
"auto_lr": {
"method": "categorical",
"dimension": {
"categories": [
true,
false
],
"grid_search": 0
}
},
"batch_size": {
"method": "uniformint",
"dimension": {
"low": 10,
"high": 30
}
},
"epoch": {
"method": "uniformint",
"dimension": {
"low": 5,
"high": 10
}
},
"cnn_block_depths_1": {
"method": "categorical",
"dimension": {
"categories": [1, 1, 2],
"grid_search": 0
}
},
"cnn_block_depths_2": {
(continues on next page)
28
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
"method": "uniformint",
"dimension": {
"low": 1,
"high": 3
}
}
}
Then
$ phpo --loadJson config_dev.json --site XYZ --outDS user.blah.`uuidgen`
Once tasks are submitted, users can see what’s going on in the system by using PanDA monitor.
If --trainingDS is specified each PanDA job gets all files in the dataset unless the task is segmented. Segmented
HPO is explained later.
FAQ
Protection against bad hyperparameter points
Each hyperparameter point is evaluated 3 times at most. If all attempts are timed-out, the system considers that the
hyperparameter point is hopeless and a very large loss is registered, so that the task continues.
Visualization of the search results
It is possible to upload a tarball of metrics files to a grid storage when evaluating each hyperparameter point. For exam-
ple, the above example uses MLflow for logging parameters and metrics, collects all files under ./mlflow into tarballs,
and uploads them to grid storages. The filename of the tarball needs to be specified using the --evaluationMetrics
option. Tarballs are registered in the output dataset so that they can be download using rucio client. It is easy to combine
MLflow metrics files. The procedure is as follows:
$ rucio download --no-subdir <output dataset>
$ tar xvfz *
$ tar xvfz metrics*
$ mlflow ui
Then access to http://127.0.0.1:5000 using your own browser will show something like the figure below.
There is an on-going development activity to dynamically spin-up MLFlow services on !PanDA monitoring or some-
thing which would do the above procedure on behalf of users and centrally provide MLFlow UI to users.
4.1. For end-users
29
PanDAWMS
30
Chapter 4. User Guide
PanDAWMS
Relationship between nPointsPerIteration and minUnevaluatedPoints
The relationship between nPointsPerIteration and minUnevaluatedPoints is illustrated in the above figure. The
steering is executed to generate new hyperparameter points every time the number of unevaluated points goes below
minUnevaluatedPoints. The number of new points is nPointsPerIteration-minUnevaluatedPoints. The
main idea to set a non-zero value to minUnevaluatedPoints is to keep the task running even if some hyperparameter
points take very long to be evaluated.
What “Logged status: skipped since no HP point to evaluate or enough concurrent HPO jobs” means
in PanDA monitor
PanDA jobs are generated every 10 min, when the number of active PanDA jobs is less than nParallelEvaluation
and there is at least one unevaluated hyperparameter point. The logging message means that there are enough PanDA
jobs running/queued in the system, or the system has evaluated or is evaluating all hyperparameter points which have
been generated so far. Note that there is a delay for iDDS to trigger the next iteration after enough hyperparameter
points were evaluated in the previous iteration.
Checkpointing
If evaluation containers support checkpointing it is possible to terminate evaluation in the middle and resume it after-
ward, which is typically useful to run long training on short-lived and/or preemptive resources. Evaluation containers
need to
• periodically produce checkpoint file(s) in the initial working directory or in sub-directories under the initial
working directory by using relevant functions of ML packages like keras example, and
• resume the training if checkpoint file(s) are available in the initial working directory, otherwise, start a fresh
training.
Users can specify the names of the checkpoint files and/or the sub-directories using the --checkPointToSave option.
The system periodically checks the files and/or sub-directories, and saves them in a persistent location if some of them
were updated after the previous check cycle. The check interval is defined by using --checkPointInterval which
is 5 minutes by default. Note that the total size of checkpoint files must be less than 100 MB. When PaDA jobs are
terminated while evaluating hyperparameter points, they are automatically retried. The latest saved checkpoint files are
provided to the retried PanDA jobs. If the --checkPointToLoad option is specified the checkpoint files/directories
4.1. For end-users
31
PanDAWMS
are archived to a tarball which is placed in the initial working directory, otherwise, they are copied to the initial working
directory with the original file/directory names.
Segmented HPO
It is possible to define multiple ML models in a single HPO task and optimize hyperparameters for each model inde-
pendently. This is typically useful when you have a target object, which can be logically or practically partitioned to
sub-objects, and want to optimize their ML models in one-go. For example, it would be reasonable to logically break
down a puppet to several parts, such as arms, body, and legs, in some use-cases, but it would be nightmare to submit a
HPO task for each part if there are so many. Instead, the user would submit a single task for the puppet and let the sys-
tem split workload based on the user-defined breakdown instruction, which would significantly simplify bookkeeping
from user’s point of view. It is enough to prepare a single training dataset which contains files for all models, but the
user needs to specify how the training dataset is partitioned by using the --segmentSpecFile option.
--segmentSpecFile SEGMENTSPECFILE
External json filename to define segments for
segmented HPO which has one model for each segment to
be optimized independently. The file contains a list of
dictionaries {'name': arbitrary_unique_segment_name,
'files': [filename_used_for_the_segment_in_the_training_dataset,
... ]}. It is also possible to specify 'datasets' instead of 'files
˓→'
in those dictionaries if the training dataset has constituent
datasets and is partitioned with constituent dataset boundaries.
None by default
For example, when a dataset contains file_1, file_2, file_3, ..., and file_N, the json would be something like
$ cat seg.json
[
{
"files": [
"file_1",
"file_3"
(continues on next page)
32
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
],
"name": "name_A"
},
{
"files": [
"file_2"
],
"name": "name_B"
}
]
so that there are two segments in the task. Note that it is also possible to specify ‘datasets’ instead of ‘files’ in dictionaries
in the json if the training dataset has constituent datasets and is partitioned with constituent dataset boundaries.
Then
$ phpo --segmentSpecFile seg.json --trainingDS blah ...
The first segment is called name_A and PanDA jobs for the segment takes only file_1 and file_3 from the training
dataset, while the second segment is called name_B and PanDA jobs for the segment takes only file_2. It is possible
to use %SEGMENT_NAME in --evaluationExec which is replaced with the actual segment name, such as name_A
and name_B, so that the evaluation container can dynamically choose the model relevant to the segment name as shown
in the figure below.
The segment name is prepended to metrics files to show for which segment the metrics file contains information. For
example,
"evaluationExec": "python toy.py %SEGMENT_NAME",
"evaluationMetrics": "metrics.tgz",
with those options, PanDA jobs for the first segment would execute the evaluation container with “python toy.py
name_A” so that toy.py would change configuration based on sys.argv[1], and the system would rename metrics.tgz to
name_A.XYZ.metrics.tgz.
4.1. For end-users
33
PanDAWMS
4.1.4 Running Athena analysis
This page explains how to run Athena analysis on PanDA.
Table of Contents
• Preparation
• How to run
– Running jobOptions
– Running transformations
– Running multiple transformations
– Reading particular events for data
• FAQ
– Contact
– How PanDA chooses sites where jobs run?
– What are buildJob and runJob?
– How job priorities are calculated?
– How can I send jobs to the site which holds the most number of input files?
– Why is my job pending in activated/defined state?
– How jobs get reassigned to other sites? Why were my jobs reassigned by JEDI?
– How to debug failed jobs
– Why did my jobs crash with “sh: line 1: XYZ Killed”?
– Why were my jobs closed ? What does ‘upstream job failed’ mean?
– Are there any small samples to test my job before run it on the whole dataset?
– What is the meaning of the ‘lost heartbeat’ error?
– What is the meaning of the ‘Looping job killed by pilot’ error?
– I want to process the whole dataset with N events per job. (integer N>0)
– I want to launch M jobs, each with N events per job
– Expected output file does not exist
– How to make group datasets
– How do I blacklist sites when submitting tasks?
– How do I get the output dataset at my favorite destination automatically
– pathena failed due to “ERROR : Could not parse jobOptions”
– My task got exhausted due to over_cpu_consumption
34
Chapter 4. User Guide
PanDAWMS
Preparation
First you need to setup the runtime environment of Athena which you want to use on PanDA. Here is an example with
AnalysisBase,21.2.40
$ export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase
$ alias setupATLAS='source $ATLAS_LOCAL_ROOT_BASE/user/atlasLocalSetup.sh'
$ setupATLAS
$ asetup AnalysisBase,21.2.40,here
Checkout packages, add modifications, and build them if necessary. Make sure that your Athena job runs properly.
Then you need to setup panda-client.
$ lsetup panda
Once everything is done you can try
$ pathena --helpGroup ALL
to see all available options.
How to run
Running jobOptions
When you run Athena with
$ athena jobO_1.py jobO_2.py jobO_3.py
all you need is
$ pathena jobO_1.py jobO_2.py jobO_3.py <--inDS inputDataset> --outDS outputDataset
where --inDS takes a dataset, a dataset container name, or a comma-concatenated list of input dataset/container names
if the job read input data, while --outDS takes the base name of output dataset containers. pathena parses jobOptions
to define output types, collects various environment variables, makes a relocatable sandbox file from your local built
packages to recreate your local runtime environment on remote resources, and submits a task together with those
materials to PanDA. EventSelector.InputCollections is automatically set to read files from --inDS so that you
don’t have to change anything in your jobOptions files. One output dataset container is created for each output type and
is registered in rucio. If you want to monitor your task, see Monitoring. Once your task is done you will get an email
notification, then you can download output files using rucio client. Your output files are available for 30 days. If you
want to retry your task, see Bookkeeping.
4.1. For end-users
35
PanDAWMS
Running transformations
The --trf option of pathena allows users to run official transformations such as Reco_tf.py with customized pack-
ages. The option takes an execution string which can be created as follows: If you locally run a trf like
$ Reco_tf.py inputAODFile=AOD.493610._000001.pool.root.1 outputNTUP_SUSYFile=my.NTUP.root"
replace some parameters with %XYZ using the following table
Pa-
rame-
ter
Placeholder
Input
%IN
Cavern
Input
%CAVIN
Mini-
mumbias
Input
%MININ
Low
pT
Mini-
mumbias
Input
%LOMBIN
High
pT
Mini-
mumbias
Input
%HIMBIN
BeamHalo
Input
%BHIN
Beam-
Gas
Input
%BGIN
Output
%OUT + suffix (e.g., %OUT.ESD.pool.root)
Max-
Events
%MAXEVENTS
SkipEvents%SKIPEVENTS
FirstEvent %FIRSTEVENT:basenumber (e.g., %FIRSTEVENT:100, the base number is given to the first job and it
will be incremented per job)
DBRe-
lease or
CDRelease
%DB:DatasetName:FileName
(e.g.,
%DB:ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-
5.1.1.tar.gz. %DB:LATEST if you use the latest DBR). Note that if your trf uses named parameters
(e.g., DBRelease=DBRelease-5.1.1.tar.gz) you will need DBRelease=%DB:DatasetName:FileName
(e.g., DBRelease=%DB:ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz)
Ran-
dom
seed
%RNDM:basenumber (e.g., %RNDM:100, this will be incremented per job)
Then you just need to give the string to --trf, e.g.
$ pathena --trf "Reco_trf.py inputAODFile=%IN outputNTUP_SUSYFile=%OUT.NTUP.root" --inDS ... --outDS ..
When your trf doesn’t take an input (e.g., evgen), use the --split option to specify how many jobs you need.
%SKIPEVENTS may be needed if you use the --nEventsPerJob or --nEventsPerFile options of pathena. Oth-
erwise, some jobs will run on the same event range in the same input file.
36
Chapter 4. User Guide
PanDAWMS
Note that you may need to explicitly specify maxEvents=XYZ or something in the execution string to set the number of
events processed in each job, since the value of --nEventsPerJob or --nEventsPerFile is used only to split files,
but is not appended to the execution string. Otherwise, each job will process all events in the input file.
pathena doesn’t interpret the argument for the –trf option although it replaces %XYZ. It is user’s responsibility to
consistently specify pathena options and the execution string.
If you want to add parameters to the transformation that are not listed above, just add them to the execution string.
pathena doesn’t replace anything except %XYZ, but it passes these parameters along to the transformation just the
same.
Running multiple transformations
One can run multiple transformations in a job by using semicolons in the --trf option like
$ pathena --trf "trf1.py ...; trf2.py ...; trf3.py ..." ...
Here is an example to run simul+digi;
$ pathena --trf "AtlasG4_trf.py inputEvgenFile=%IN outputHitsFile=tmp.HITS.pool.root maxEvents=10 skipEv
where AtlasG4_trf.py produces a HITS file (tmp.HITS.pool.root) which is used as an input by Digi_trf.py to pro-
duce RDO. In this case, only RDO is added to the output dataset since only RDO has the %OUT prefix (i.e.
%OUT.RDO.pool.root).
If you want to have HITS and RDO in the output dataset the above will be
$ pathena --trf "AtlasG4_trf.py inputEvgenFile=%IN outputHitsFile=%OUT.HITS.pool.root maxEvents=10 skipE
Note that both AtlasG4_trf.py and Digi_trf.py take %OUT.RDO.pool.root as a parameter. AtlasG4_trf.py uses it as an
output filename while Digi_trf.py uses it as an input filename.
Reading particular events for data
You can specify a run/event list as an input. First you need to prepare a list of runs/events of interest. You may get a
list by analysing D3PD, browsing event display, using ELSSI, and so on. A list looks like
$ cat rrr.txt
154514 21179
154514 29736
154558 448080
where each line contains a run number and an event number. Then, e.g.,
$ pathena AnalysisSkeleton_topOptions.py --eventPickEvtList rrr.txt --eventPickDataType AOD
--eventP
where events in the input file are internally converted to AOD (specifid by --eventPickDataType) with the
physics_CosmicCaloEM stream (specified by --eventPickStreamName). Your jobO is dynamically configured to
use event selection, so you don’t need to change your jobO. In principle, you can run any arbitrary jobO.
4.1. For end-users
37
PanDAWMS
FAQ
Contact
We have one egroup and one JIRA. Please submit all your help requests to hn-atlas-dist-analysis-help@cern.ch which
is maintained by AtlasDAST.
How PanDA chooses sites where jobs run?
PanDA chooses sites using the following information;
• input data locality
• the number of jobs in activated/defined/running state (site occupancy rate)
• the average number of CPUs per worker node at each site
• the number of active or available worker nodes
• pilot rate for last 3 hours. If no pilots, the site is skipped
• available disk space in SE
• Atlas release/cache matching
• site statue
and then calculate the weight for each site using the following formula.
𝑊= (1 + 𝐺/(𝑈+ 1)) * (𝑅+ 1) * 𝑃* 𝑋
𝐷+ 𝐴+ 𝑇
where
• W: Weight at the site
• G: The number of available worker nodes which have sent getJob requests for last 3 hours
• U: The number of active worker nodes which have sent updateJob requests for last 3 hours
• R: The maximum number of running jobs in last 24 hours
• D: The number of defined jobs
• A: The number of activated or starting jobs
• T: The number of assigned jobs which are transferring input files to the site
• X: Weight factor based on data availability. When input file transfer is disabled, X=1 if input data is locally
available, otherwise X=0. When input file transfer is enabled, X=1+(total size of input files on DISK)/10GB if
files are available on DISK, X=1+(total size of input files on TAPE)/1000GB if files are available on TAPE, X=1
otherwise
38
Chapter 4. User Guide
PanDAWMS
What are buildJob and runJob?
Once PanDA chooses sites to run jobs the relocatable sandbox file is sent to the sites. One buildJob is created at each
site to upload the sandbox file to the local storage at the site. The completion of buidJob triggers a bunch of runJobs.
Each runJob retrieves the sandbox file to run Athena.
It is possible to skip buildJob using --noBuild. In this case runJobs retrive the sandbox file from a web service but
the size of the sandbox file is limted to 50MB.
How job priorities are calculated?
Job priorities are calculated for each user by using the following formula. When a user submits a task composed of M
jobs,
𝑃𝑟𝑖𝑜𝑟𝑖𝑡𝑦(𝑛) = 1000 −𝑇+ 𝑛
5
where Priority(n) is the priority of n-th job (0n<M), and the total number of the user’s active jobs in the whole system.
For example, if a fresh user submits a task composed of 100 jobs, the first 5 jobs have priority=1000 while the last 5
jobs have priority=981. The idea of this gradual decrease is to prevent huge tasks from occupying the whole CPU slots.
When another fresh user submits a job with 10 jobs, these jobs have priority=1000,999 so that they will be executed
as soon as CPU becomes available even if other users have already queued many jobs. Priorities for waiting jobs in
the queue are recalculated every 20 minutes. Even if some jobs have very low priorities at the submission time their
priorities are increased periodically so that they are executed before they expire.
If the user submits jobs with the --voms and --official options to produce group datasets, those jobs are regarded
as group jobs. Priorities are calculated per group separately from the user who submitted, so group jobs don’t reduce
priorities of normal jobs which are submitted by the same user without those options.
There are a few kinds of jobs which have higher priorities, such as merge jobs (5000) and HummerCloud jobs (4000),
since they have to be processed quickly.
4.1. For end-users
39
PanDAWMS
How can I send jobs to the site which holds the most number of input files?
You can send jobs to a specific site using --site, but the option is not recommended, since Jobs should be automatically
sent to proper sites.
Why is my job pending in activated/defined state?
Jobs are in the activated state until CPU resources become available at the site. If the site is busy your jobs will have
to wait so long. runJobs are in defined state until corresponding buildJobs have finished.
How jobs get reassigned to other sites? Why were my jobs reassigned by JEDI?
Jobs are internally reassigned to another site at most 3 times, when
• they are waiting for 24 hours.
• HammerCloud set sites to the test or offline mode 3 hours ago
The algorithm for site selection is the same as normal brokerage described in the above section. Old jobs are closed.
When a new site is not found, jobs will stay at the original site.
How to debug failed jobs
You can see the error description in Monitoring.
When transExitCode is not zero, the job failed with an Athena problem. You may want to see log files. You can browse
the log files following links “Logs” →“Log files”.
Now you find various log files shown in the page.
E.g., there should be payload.stdout for stdout and payload.stderr for stderr, where you may get some clues.
40
Chapter 4. User Guide
PanDAWMS
4.1. For end-users
41
PanDAWMS
42
Chapter 4. User Guide
PanDAWMS
Note that some filed jobs don’t have log files. This typically happens when jobs are killed by the batch system before
uploading log files.
Why did my jobs crash with “sh: line 1: XYZ Killed”?
sh: line 1: 13955 Killed
athena.py -s ...
If you see something like the above message in the log file, perhaps your jobs were killed by the batch system due to
huge memory consumption. You may explicitly reduce the number of input files per job using --nFilesPerJob if
memory consumption scales with the number of files. However, not set a very small number to --nFilesPerJob.
If your jobs are very short the system will automatically ignore --nFilesPerJob since too many short jobs kill the
system.
4.1. For end-users
43
PanDAWMS
Why were my jobs closed ? What does ‘upstream job failed’ mean?
If a buildJob fails, corresponding runJobs will get closed.
Are there any small samples to test my job before run it on the whole dataset?
You can limit the number of files to be used in --inDS by using --nfiles.
What is the meaning of the ‘lost heartbeat’ error?
Each job sends heartbeat messages every 30 min to indicate it is alive. If there is no heartbeat message for 6 hours, the
job gets killed. The error typically happens when the job died due to temporary troubles in the backend batch system
or network. Generally jobs are automatically retried and the next attempts succeed.
44
Chapter 4. User Guide
PanDAWMS
What is the meaning of the ‘Looping job killed by pilot’ error?
If a job doesn’t update output files for 2 hours, it will be killed. This protection is intended to kill dead-locked jobs
or infinite-looping jobs. If your job doesn’t update output files very frequently (e.g., some heavy-ion job takes several
hours to process one event) you can relax this limit by using the –maxCpuCount option. However, sometimes even
normal jobs get killed due to this protection. When the storage element has a problem, jobs cannot copy input files
to run Athena and of course cannot update output files. When you think that your job was killed due to an storage
problem, you may report to DAST. Then shift people and the SE admin will take care of it.
I want to process the whole dataset with N events per job. (integer N>0)
Use --nEventsPerJob. This option checks with Rucio to retrieve the information about the number of events for each
file. Generally the information is available for official datasets. You may have to register the information for private
datasets if you want to use them.
I want to launch M jobs, each with N events per job
You can use the following command:
$ pathena --split M --nEventsPerJob N .....
Note that --nFilesPerJob and --nEventsPerJob can not be defined simultaneously, pathena will exit with an error
at startup. Please define only one or another.
Expected output file does not exist
Perhaps the output stream is defined in somewhere in your jobOs, but nothing uses it. In this case, Athena doesn’t
produce the file. The solutions could be to modify your jobO or to use the --supStream option. E.g., –supStream
hist1 will disable user.aho.TestDataSet1.hist1._00001.root.
How to make group datasets
Use --official and --voms options.
$ pathena --official --voms atlas:/atlas/groupName/Role=production --outDS group.groupName.[otherFields]
where groupName for SUSY is phys-susy, for example. See the document ATL-GEN-INT-2007-001 for dataset naming
convention. The group name needs to be officially approved and registered in ATLAS VOMS. Note that you need to
have the production role for the group to produce group-defined datasets. If not, please request it in the ATLAS VO
registration page. If you submit tasks with the --voms option, jobs are counted in the group’s quota.
How do I blacklist sites when submitting tasks?
Use --excludedSite. However, this option is not recommend since that would skew workload distrubution in the
whole system and decrease the system throughput.
4.1. For end-users
45
PanDAWMS
How do I get the output dataset at my favorite destination automatically
When --destSE option is used, output files are automatically aggregated to a RSE. e.g.,
$ pathena --destSE LIP-LISBON_LOCALGROUPDISK ...
Generally LOCALGROUPDISK (long term storage) or SCRATCHDISK (short term storage) can be used.
You
can check permission in each RSE page in CRIC. For example, only /atlas/pt users are allowed to write to LIP-
LISBON_LOCALGROUPDISK, so if you don’t belong to the pt group the above example will fail and you will have
to choose another RSE.
pathena failed due to “ERROR : Could not parse jobOptions”
The error message would be something like:
ABC/XYZ_LoadTools.py", line 65, in <module>
input_items = pf.extract_items(pool_file=
svcMgr.EventSelector.InputCollections[0])
IndexError: list index out of range
ERROR : Could not parse jobOptions
First, make sure that you jobOptions work on your local computer without any changes. Basically pathena doesn’t
work if Athena locally fails with the jobO.
For example, if it fails in InputFilePeeker, the solution is to have something like
svcMgr.EventSelector.InputCollections=["/somedir/mc08.108160.AlpgenJimmyZtautauNp0VBFCut.
˓→recon.ESD.e414_s495_r635_tid070252/ESD.070252._000001.pool.root.1"]
in your jobO, where the input file must be valid (i.e. can be accessed from your local computer). Note that input parame-
ter (essentially EventSelector.InputCollections and AthenaCommonFlags.FilesInput) will be automatically overwritten
to read input files in --inDS.
The local file doesn’t have to be from --inDS as long as the data type, such as AOD,ESD,RAW..., is identical.
My task got exhausted due to over_cpu_consumption
This message means that jobs may have abused more CPU cores than allocation since the CPU time of your jobs was
longer than their total execution time. Jobs can use more than one CPU core even if they run on single-core queues
since they physically run on multi-core CPUs. This typically happens when your application internally spawns multiple
threads/processes and spreads over multiple CPU cores. If this is the case, it would help send jobs to multi-core queues
using the --nCore option.
46
Chapter 4. User Guide
PanDAWMS
4.1.5 Bookkeeping
Table of Contents
• Introduction
• Misc commands
– Show all commands
– See help of each command
• Task bookkeeping
– Kill tasks
– Finish tasks
– Retry tasks
– Show all own tasks
– Show one or more tasks with JediTaskIDs
– Show in long detailed format
– Show tasks matching certain filters
– Show tasks in other format
• Workflow bookkeeping
– Show status of a workflow
– Finish a workflow
– Kill a workflow
– Retry a workflow
– Pause a workflow
– Resume a workflow
• Trouble shooting
Introduction
pbook is the command-line tool for users to manage their analysis, e.g., to check task status, and finish/kill/retry tasks.
pbook launches an interactive session on the terminal where the user enters bookkeeping commands such as show and
kill.
$ pbook
The interactive session can be terminated by entering Ctrl+D.
4.1. For end-users
47
PanDAWMS
Misc commands
Show all commands
>>> help()
See help of each command
>>> help(show)
Print task records. The first argument (non-keyword) can be an jediTaskID or reqID,␣
˓→or 'run' (show active tasks only), or 'fin' (show terminated tasks only), or can be␣
˓→omitted. The following keyword arguments are available in the way of panda monitor url␣
˓→query: [username, limit, taskname, days, jeditaskid].
If sync=True, it forces panda monitor to get the latest records rather than get␣
˓→from cache.
Specify display format with format='xxx', available formats are ['standard', 'long',
˓→'json', 'plain'].
The default filter conditions are: username=(name from user voms proxy), limit=1000,
˓→days=14, sync=False, format='standard'.
example:
>>> show()
>>> show(123)
>>> show(12345678, format='long')
>>> show(taskname='my_task_name')
>>> show('run')
>>> show('fin', days=7, limit=100)
>>> show(format='json', sync=True)
Task bookkeeping
Kill tasks
>>> kill(arg)
This command can take a jediTaskID, a list of jediTaskIDs, or ‘all’ as the input argument. If it is ‘all’, it kills all active
tasks of the user.
48
Chapter 4. User Guide
PanDAWMS
Finish tasks
>>> finish(arg, soft=False)
This command enforces running tasks to finish immediately. The arg is a jediTaskID, a list of jediTaskIDs, or ‘all’. If
soft is set to True, the system doesn’t generate new jobs but waits until all existing jobs are done.
Retry tasks
>>> retry(arg, newOpts=None)
This command is used to retry only failed PanDA jobs in a finished task. The arg is a jediTaskID or a list of jediTaskIDs.
It is possible to specify newOpts, which is None by default and can be a map of options and new arguments like
{‘nFilesPerJob’: 10,’excludedSite’: ‘ABC,XYZ’} to overwrite task parameters. If values of some arguments are None,
corresponding task parameters are removed. For example, {‘nFilesPerJob’: None,’excludedSite’: None} will remove
–nFilesPerJob and –excludedSite so that jobs will be generated and assigned without those constraints.
Show all own tasks
>>> show()
By default, it shows only tasks submitted within last 14 days and at most 1000 tasks. One can specify days and limit
keyword arguments to show more (or less) tasks.
Show one or more tasks with JediTaskIDs
>>> show(arg)
The arg can be a jediTaskID or a list of jediTaskIDs. Note that it is possible to use ReqID instead of jediTaskID,
however, mixture of JediTaskID and ReqID doesn’t work.
Show in long detailed format
>>> showl()
which is a wrapper of show(format=’long’).
Show tasks matching certain filters
>>> show(username='XYZ', limit=7, days=30)
which shows at most 7 tasks submitted by Max Barends for last 30 days.
4.1. For end-users
49
PanDAWMS
Show tasks in other format
>>> show(format='plain')
where available formats are ‘standard’, ‘long’, ‘json’, ‘plain’.
Workflow bookkeeping
All workflow bookkeeping commands take the request ID of the workflow as the argument.
Show status of a workflow
>>> show_workflow(request_id)
This command shows the workflow status of interest.
Finish a workflow
>>> finish_workflow(request_id)
This command enforces to finish all active tasks in the workflow.
Kill a workflow
>>> kill_workflow(request_id)
This command kills all active tasks in the workflow.
Retry a workflow
>>> retry_workflow(request_id)
This command retries tasks unsuccessful in the previous attempt and activate subsequent tasks if necessary.
Pause a workflow
>>> pause_workflow(request_id)
This command pauses all active tasks in the workflow.
50
Chapter 4. User Guide
PanDAWMS
Resume a workflow
>>> resume_workflow(request_id)
This command resume paused tasks in the workflow.
Trouble shooting
pbook goes into the verbose mode to show shows what’s exactly going on when being launched with -v option.
$ prun -v
which would give clues if there are problems.
4.1.6 End-user python API
This section explains python API in panda-client for end-users. All functions are available through the API object.
[ ]: from pandaclient import panda_api
c = panda_api.get_api()
Task submission API
• Submit a task (low-level)
You need to prepare a dictionary of task parameters following an example. This function gives you a taskID, which
is the unique identifier in the system, once it successfully submits the task. Note that it is highly recommended using
another high-level API, such as execute_prun, execute_pathena, and execute_phpo, and only developers should this
method since the task parameter dictionary is quite cryptic. Note that although all task submission functions have
similar interface to subporcess functions they are executed in the same python interpreter, i.e., they don’t spawn child
processes.
[ ]: from pandaclient.example_task import taskParamMap
communication_status, o = c.submit_task(taskParamMap)
if communication_status:
server_return_code, dialog_message, task_id = o
if o == 0:
print ("taskID={}".format(task_id))
submit_task(self, task_params, verbose=False)
Description: Submit a task to PanDA
args:
task_params: a dictionary of task parameters
verbose: True to see debug messages
returns:
status code
(continues on next page)
4.1. For end-users
51
PanDAWMS
(continued from previous page)
0: communication succeeded to the panda server
255: communication failure
tuple of return code, message from the server, and task ID if successful
0: request is processed
1: duplication in DEFT
2: duplication in JEDI
3: accepted for incremental execution
4: server error
• Submit a prun task
[ ]: import uuid
com = "--exec ls --outDS user.hoge.{} --vo sphenix".format(str(uuid.uuid4()))
status, task_dict = c.execute_prun(com.split(), console_log=False)
if status:
print ("taskID={}".format(task_dict['jediTaskID']))
execute_prun(args, console_log=True)
Description: Execute prun command
args:
args: The arguments used to execute prun. This is a list of strings.
console_log: False to disable console logging
returns:
status: True if succeeded. Otherwise, False
a dictionary: Task submission attributes including jediTaskID
• Submit a pathena task
execute_pathena(args, console_log=True)
Description: execute pathena command
args:
args: The arguments used to execute prun. This is a list of strings.
console_log: False to disable console logging
returns:
status: True if succeeded. Otherwise, False
a dictionary: Task submission attributes including jediTaskID
• Submit a phpo task
execute_phpo(args, console_log=True)
Description: execute phpo command
args:
args: The arguments used to execute prun. This is a list of strings.
console_log: False to disable console logging
returns:
status: True if succeeded. Otherwise, False
a dictionary: Task submission attributes including jediTaskID
52
Chapter 4. User Guide
PanDAWMS
Task management API
• Kill a task
[ ]: communication_status, o = c.kill_task(task_id)
if communication_status:
server_return_code, dialog_message = o
if o == 0:
print('OK')
else:
print ("Not good with {} : {}".format(server_return_code, dialog_message))
kill_task(taskID, verbose=False)
Description: kill a task
args:
jediTaskID: jediTaskID of the task to be killed
returns:
status code
0: communication succeeded to the panda server
255: communication failure
tuple of return code and diagnostic message
0: request is registered
1: server error
2: task not found
3: permission denied
4: irrelevant task status
100: non SSL connection
101: irrelevant taskID
• Finish a task
[ ]: communication_status, o = c.finish_task(task_id)
if communication_status:
server_return_code, dialog_message = o
if o == 0:
print('OK')
else:
print ("Not good with {} : {}".format(server_return_code, dialog_message))
finish_task(task_id, wait_running=False, verbose=False)
Description: finish a task
args:
task_id: jediTaskID of the task to finish
wait_running: True to wait until running jobs are done
verbose: True to see debug messages
returns:
status code
0: communication succeeded to the panda server
(continues on next page)
4.1. For end-users
53
PanDAWMS
(continued from previous page)
255: communication failure
tuple of return code and diagnostic message
0: request is registered
1: server error
2: task not found
3: permission denied
4: irrelevant task status
100: non SSL connection
101: irrelevant taskID
• Retry a task
[ ]: communication_status, o = c.retry_task(task_id)
if communication_status:
server_return_code, dialog_message = o
if o == 0:
print('OK')
else:
print ("Not good with {} : {}".format(server_return_code, dialog_message))
retry_task(task_id, new_parameters=Non, verbose=False)
Description: finish a task
args:
task_id: jediTaskID of the task to finish
new_parameters: a dictionary of task parameters to overwrite
verbose: True to see debug messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
tuple of return code and diagnostic message
0: request is registered
1: server error
2: task not found
3: permission denied
4: irrelevant task status
100: non SSL connection
101: irrelevant taskID
Bookkeeping API
• Get tasks
[ ]: tasks = c.get_tasks()
for task in tasks:
print ('taskID={} status={}'.format(task['jeditaskid'], task['status']))
54
Chapter 4. User Guide
PanDAWMS
get_tasks(self, task_ids, limit=1000, days=14, status=None, username=None)
Description: get a list of task dictionaries
args:
task_ids: a list of task IDs, or None to get recent tasks
limit: the max number of tasks to fetch from the server
days: tasks for last N days to fetch
status: filtering with task status
username: user name of the tasks, or None to get own tasks
returns:
a list of task dictionaries
• Show tasks
show_tasks(self, task_ids, limit=1000, days=14, status=None, username=None)
Description: show tasks on the console
args:
task_ids: a list of task IDs, or None to get recent tasks
limit: the max number of tasks to fetch from the server
days: tasks for last N days to fetch
status: filtering with task status
username: user name of the tasks, or None to get own tasks
returns:
None
• Get metadata of all jobs in a task
get_job_metadata(self, task_id, output_json_filename)
Description: get metadata of all jobs in a task
args:
task_id: task ID
output_json_filename: output json filename
4.1.7 Working with JupyterLab
Users can submit/manage analysis on PanDA using JupyterLab. This page explains how to setup JupyterLab on your
computer, how to install and setup panda-client on JupyterLab, and how to use client modules and tools in Jupyter
notebook.
4.1. For end-users
55
PanDAWMS
Preparation
First, you need Python 3 and JupyterLab. Downloading Python and JupyterLab installation guide would help if you
have to install them by yourself on your computer.
Here is an example of setup procedures with macOS X Big Sur, Python 3.9, venv, JupyterLab, and pip.
1. Download macOS 64-bit installer from python 3.9.0 download page and double-click the pkg file.
2. In Finder, go to Applications &rarr; Python 3.9 and double-click Install Certificates.command to install trusted
root certificates.
3. Open a Terminal and make a virtual environment.
$ python3 -m venv ~/mywork
$ cd ~/mywork
$ . bin/activate
4. Install JupyterLab via pip.
$ pip install jupyterlab
5. Start JupyterLab in a subdirectory.
$ mkdir jupyter_home
$ cd jupyter_home
$ jupyter lab
JupyterLab will open automatically in your browser. You may access JupyterLab by entering the local notebook server’s
URL into the browser.
You will do hereafter all procedures in Jupyter notebook.
56
Chapter 4. User Guide
PanDAWMS
Installation of panda-client
This section explains how to install panda-client on JupyterLab.
Launch a notebook and install panda-client using pip
[ ]: import sys
!{sys.executable} -m pip install panda-client[jupyter]
Copy the setup file
[ ]: !mkdir ~/.panda
!cp {sys.exec_prefix}/etc/panda/panda_setup.example.cfg ~/.panda/panda_setup.cfg
The setup file is copied to the subdirectory where you started JupyterLab and shows up in the Files tab on the Left
Sidebar.
Modify the setup file if necessary
The setup file contains all configuration parameters. You need to change the following parameters in the setup file if
any:
• PANDA_URL_SSL: Base HTTPS URL of PanDA server
• PANDA_URL: Base HTTP URL of PanDA server
• PANDA_AUTH_VO: Virtual organization name (required only when PANDA_AUTH = oidc)
• PANDA_AUTH: Authentication mechanism (oidc, x509, x509_no_grid)
• PANDA_USE_NATIVE_HTTPLIB: Use native httplib instead of curl for HTTP operations
• X509_USER_PROXY: Grid proxy file path (required only when PANDA_AUTH = x509_no_grid)
• PANDA_NICKNAME: Grid nickname (required only when PANDA_AUTH = x509_no_grid)
Setup example for DOMA using OIDC/OAuth2.0 authentication
PANDA_AUTH = oidc
PANDA_URL_SSL = https://ai-idds-01.cern.ch:25443/server/panda
PANDA_URL = http://ai-idds-01.cern.ch:25080/server/panda
PANDAMON_URL = https://panda-doma.cern.ch
PANDA_AUTH_VO = my_vo
Then you need to get an ID token as explained in the next page.
4.1. For end-users
57
PanDAWMS
Setup example for ATLAS using X509 authentication without grid middleware
PANDA_AUTH = x509_no_grid
X509_USER_PROXY = /Users/hage/x509up_u17959
PANDA_USE_NATIVE_HTTPLIB = 1
PANDA_NICKNAME = my_nickname
Make sure that you generate a grid proxy file somewhere and copy it to the filepath defined by X509_USER_PROXY.
Also, PANDA_NICKNAME needs to be consisitent with the nickname in ATLAS VOMS.
Download notebook
Setup and Quick Start
To use panda-client API in notebook, you need to set it up first.
Setup Jupyter interface
[ ]: from pandaclient import panda_jupyter
panda_jupyter.setup()
Import panda-client API
[ ]: from pandaclient import panda_api
c = panda_api.get_api()
At this stage all end-user python APIs are available in the notebook. Try a toy example,
[ ]: from pandaclient import panda_gui
x = panda_gui.show_task(23518002)
which will load data from PanDA, convert them to a pandas dataframe, and show some plots after some delay.
Get an OIDC ID token
You need to get an OIDC ID token unless you use X509 authentication with grid middleware. The following method
triggers the device code authentication flow and gives a PanDA IAM URL to redirect to your identity provider.
[ ]: c.hello()
If you already have a token you will immediately see an OK message. Otherwise, you will see something like
INFO : Please go to https://panda-iam-doma.cern.ch/device?user_code=EEPGDH and sign in.␣
˓→Waiting until authentication is completed
INFO : Ready to get ID token?
[y/n]
58
Chapter 4. User Guide
PanDAWMS
Note that you need to get a token to the URL before entering y in the prompt.
Once you go to the URL using your web browser, such as Chrome, FireFox, etc, you will be asked to sign in with your
own ID provider.
Then you will be navigated to a federated IAM platform, CILogon, to choose your own identity provider.
When you successfully sign in you need to approve the virtual organization to retrieve your profile, such as name and
email address, from your identity provider.
4.1. For end-users
59
PanDAWMS
You approved it and see a succeeded message in your browser, and now you can enter y in the notebook prompt to get
a token.
[y/n]
y
INFO : All set
OK
Generally you don’t have to repeat the procedure once you get a token since it is automatically renewed.
Download notebook
Using client tools in Jupyter notebook
You can use panda-client tools (pathena, prun, phpo, and pbook) in Jupyter notebook through the magic command
interface, *%command args*.
First, you need to setup Jupyter interface as usual.
[ ]: from pandaclient import panda_jupyter
panda_jupyter.setup()
Then, for example
[ ]: %prun -h
gives
usage: prun [options]
HowTo is available at https://twiki.cern.ch/twiki/bin/view/PanDA/PandaRun
optional arguments:
-h, --help
show this help message and exit
--helpGroup {PRINT,PRUN,CONFIG,INPUT,OUTPUT,JOB,BUILD,SUBMIT,EVTFILTER,EXPERT,
˓→CONTAINERJOB,ALL}
Print individual group help (the group name is not
case-sensitive), where "ALL" will print all groups
together. Some options such as --inOutDsJson may SPAN
several groups
(continues on next page)
60
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
Examples:
prun --exec "echo %IN > input.txt; root.exe; root -b -q macrotest.C" --athenaTag=22.0.
˓→0 --inDS ...
prun --exec "cpptest %IN" --bexec "make" --athenaTag=22.0.0 --inDS ...
prun --loadJson prunConfig.json
# read all prun options from one json file
and
[ ]: %prun -3 --exec ls --outDS user.hoge.`uuidgen` --vo sphenix
gives
INFO : gathering files under /Users/hoge/jupyter/home/panda
INFO : upload source files
INFO : submit user.hoge.73818405-09D2-4BA0-8186-E3EB22BC9AFD/
INFO : succeeded. new jediTaskID=473
respectively.
Note that it wold also be possible to call those tools as shell commands *!command args*, but they will be hang up if
you need to enter information, such as passphrase, on the console. Especially this is the case for pbook, so you need to
use %pbook instead of !pbook.
[ ]: %pbook
Then you go into the interactive session and can use pbook commands on the command prompt.
PBook user: hoge
Start pBook 1.4.47
>>> show()
Showing only max 1000 tasks in last 14 days. One can set days=N to see tasks in last N␣
˓→days, and limit=M to see at most M latest tasks
JediTaskID
Status
Fin%
TaskName
________________________________________________________________
23084322
done
100.0%
user.hoge.61cdfaf1-5cea-4a94-8f38-f8f2eb035303/
23083910
done
100.0%
user.hoge.f2b8736f-6471-4fe2-a0a0-13b67ee63ac0/
23083909
done
100.0%
user.hoge.5db466a4-f04f-4a7a-94ba-80722f7f9639/
23012863
done
100.0%
user.hoge.aeab8f8b-271a-4c63-9763-695d77d09b61/
23012862
done
100.0%
user.hoge.f2ddc8cf-9892-410c-9d6d-5db577e84ebc/
>>> kill(475)
Kill command registered: reqID=475 will be killed soon.
[True]
>>>
Download notebook
4.1. For end-users
61
PanDAWMS
4.1.8 Running workflow
A workflow is a set of tasks whose relationship is described with a directed acyclic graph (DAG). In a DAG, a parent
task can be connected to one or more child tasks with each edge directed from the parent task to a child task. A child
task processes the output data of the parent task. It is possible to configure each child task to get started when the parent
task produces the entire output data or partially produces the output data, depending on the use-case. If a child task is
configured as the latter both parent and child tasks will run in parallel for a while, which will reduce the total execution
time of the workflow. Currently tasks have to be PanDA tasks, but future versions will support more backend systems
such as local batch systems, production system, kubernetes-based resources, and other workflow management systems,
to run some tasks very quickly or outsource sub-workflows.
The user describes a workflow using the Common Workflow Language (CWL) and submits it to PanDA using pchain.
This page explains how to use pchain as well as how to describe workflows.
Table of Contents
• Workflow examples
– Simple task chain
– More complicated chain
– Sub-workflow and parallel execution with scatter
– Using Athena
– Conditional workflow
– Involving hyperparameter optimization
– Loops in workflows
– Loop + scatter
• Debugging locally
• Monitoring
• Bookkeeping
Workflow examples
Simple task chain
The following cwl code shows a parent-child chain of two prun tasks.
Listing 1: simple_chain.cwl
cwlVersion: v1.0
class: Workflow
inputs: []
(continues on next page)
62
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
outputs:
outDS:
type: string
outputSource: bottom/outDS
steps:
top:
run: prun
in:
opt_exec:
default: "echo %RNDM:10 > seed.txt"
opt_args:
default: "--outputs seed.txt --nJobs 3 --avoidVP"
out: [outDS]
bottom:
run: prun
in:
opt_inDS: top/outDS
opt_exec:
default: "echo %IN > results.root"
opt_args:
default: "--outputs results.root --forceStaged --avoidVP"
out: [outDS]
4.1. For end-users
63
PanDAWMS
The class field must be Workflow to indicate this code describes a workflow. There are two prun tasks in the workflow
and defined as top and bottom steps in steps section. The inputs section is empty since the workflow doesn’t take
any input data. The outputs section describes the output parameter of the workflow, and it takes only one string
type parameter with an arbitrary name. The outputSource connects the output parameter of the bottom step to the
workflow output parameter.
In the steps section, each step represents a task with an arbitrary task name, such as top and bottom. The run filed of
a prun task is prun. The in section specifies a set of parameters correspond to command-line options of prun.
Here is a list of parameters in the in section to run a prun task.
Parameter
Corresponding prun option
opt_inDS
—inDS (string)
opt_inDsType
No correspondence. Type of inDS (string)
opt_secondaryDSs
—secondaryDSs (a list of strings)
opt_secondaryDsTypes
No correspondence. Types of secondaryDSs (a string array)
opt_exec
—exec (string)
opt_useAthenaPackages
—useAthenaPackages (bool)
opt_containerImage
—containerImage (string)
opt_args
all other prun options except for listed above (string)
All options opt_xyz except opt_args and opt_xyzDsTypes can be mapped to —xyz of prun. opt_args specifies
all other prun options such as —outputs, —nFilesPerJob, and —nJobs. Essentially,
run: prun
in:
opt_exec:
default: "echo %RNDM:10 > seed.txt"
opt_args:
default: "--outputs seed.txt --nJobs 3"
corresponds to
prun --exec "echo %RNDM:10 > seed.txt" --outputs seed.txt --nJobs 3
The out section specifies the task output with an arbitrary string surrendered by brackets. Note that it is always a single
string even if the task produces multiple outputs. The output of the top task is passed to opt_inDS of the bottom task.
The bottom task starts processing once the top task produces enough output data, waits if all data currently available
has been processed but the top task is still running, and finishes once all data from the top task is processed.
The user can submit the workflow to PanDA using pchain that is included in panda-client. First, create a file called
simple_chain.cwl containing the cwl code above. Next, you need to create an empty yaml file since cwl files work with
yaml files that describe workflow inputs. This example doesn’t take an input, so the yaml file can be empty.
$ touch dummy.yaml
$ pchain --cwl simple_chain.cwl --yaml dummy.yaml --outDS user.<your_nickname>.blah
pchain automatically sends local *.cwl, *.yaml, and *.json files to PanDA together with the workflow. --outDS is
the basename of the datasets for output and log files. Once the workflow is submitted, the cwl and yaml files are
parsed on the server side to generate tasks with sequential numbers in the workflow. The system uses a combination
of the sequential number and the task name, such as 000_top and 001_bottom, as a unique identifier for each task.
The actual output dataset name is a combination of --outDS, the unique identifier, and —outputs in opt_args. For
example, the output dataset name of the top task is user.<your_nickname>.blah_000_top_seed.txt and that of the bottom
is user.<your_nickname>.blah_001_bottom_results.root. If —outputs is a comma-separate output list, one dataset is
created for each output type.
To see all options of pchain
64
Chapter 4. User Guide
PanDAWMS
$ pchain --helpGroup ALL
More complicated chain
The following cwl example describes more complicated chain as shown in the picture below.
Listing 2: sig_bg_comb.cwl
cwlVersion: v1.0
class: Workflow
requirements:
MultipleInputFeatureRequirement: {}
inputs:
signal: string
background: string
outputs:
outDS:
type: string
outputSource: combine/outDS
steps:
make_signal:
run: prun
in:
(continues on next page)
4.1. For end-users
65
PanDAWMS
(continued from previous page)
opt_inDS: signal
opt_containerImage:
default: docker://busybox
opt_exec:
default: "echo %IN > abc.dat; echo 123 > def.zip"
opt_args:
default: "--outputs abc.dat,def.zip --nFilesPerJob 5"
out: [outDS]
make_background_1:
run: prun
in:
opt_inDS: background
opt_exec:
default: "echo %IN > opq.root; echo %IN > xyz.pool"
opt_args:
default: "--outputs opq.root,xyz.pool --nGBPerJob 10"
out: [outDS]
premix:
run: prun
in:
opt_inDS: make_signal/outDS
opt_inDsType:
default: def.zip
opt_secondaryDSs: [make_background_1/outDS]
opt_secondaryDsTypes:
default: [xyz.pool]
opt_exec:
default: "echo %IN %IN2 > klm.root"
opt_args:
default: "--outputs klm.root --secondaryDSs %IN2:2:%{DS1}"
out: [outDS]
generate_some:
run: prun
in:
opt_exec:
default: "echo %RNDM:10 > gen.root"
opt_args:
default: "--outputs gen.root --nJobs 10"
out: [outDS]
make_background_2:
run: prun
in:
opt_inDS: background
opt_containerImage:
default: docker://alpine
opt_secondaryDSs: [generate_some/outDS]
opt_secondaryDsTypes:
default: [gen.root]
(continues on next page)
66
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
opt_exec:
default: "echo %IN > ooo.root; echo %IN2 > jjj.txt"
opt_args:
default: "--outputs ooo.root,jjj.txt --secondaryDSs %IN2:2:%{DS1}"
out: [outDS]
combine:
run: prun
in:
opt_inDS: make_signal/outDS
opt_inDsType:
default: abc.dat
opt_secondaryDSs: [premix/outDS, make_background_2/outDS]
opt_secondaryDsTypes:
default: [klm.root, ooo.root]
opt_exec:
default: "echo %IN %IN2 %IN3 > aaa.root"
opt_args:
default: "--outputs aaa.root --secondaryDSs %IN2:2:%{DS1},%IN3:5:%{DS2}"
out: [outDS]
The workflow takes two inputs, signal and background. The signal is used as input for the make_signal task, while the
background is used as input for the make_background_1 and make_background_2 tasks. The make_signal task runs in
the busybox container as specified in opt_containerImage, to produce two types of output data, abc.dat and def.zip,
as specified in opt_args. If the parent task produces multiple types of output data and the child task uses some of
them, their types need to be specified in opt_inDsType. The premix task takes def.zip from the make_signal task and
xyz.pool from the make_background_1 task.
Output data of parent tasks can be passed to a child task as secondary inputs. In this case, they are specified in
opt_secondaryDSs and their types are specified in opt_secondaryDsTypes. Note that the stream name, the number
of files per job, etc, for each secondary input are specified using —secondaryDSs in opt_args where %{DSn} can
be used as a placeholder for the n-th secondary dataset name. MultipleInputFeatureRequirement is required if
opt_secondaryDsTypes take multiple input data.
The workflow inputs are described in a yaml file. E.g.,
$ cat inputs.yaml
signal: mc16_valid:mc16_valid.900248.PG_singlepion_flatPt2to50.simul.HITS.e8312_s3238_tid26378578_00
background: mc16_5TeV.361238.Pythia8EvtGen_A3NNPDF23LO_minbias_inelastic_low.merge.HITS.e6446_s3238_s325
Then submit the workflow.
$ pchain --cwl sig_bg_comb.cwl --yaml inputs.yaml --outDS user.<your_nickname>.blah
If you need to run the workflow with different input data it enough to submit it with a different yaml file.
4.1. For end-users
67
PanDAWMS
Sub-workflow and parallel execution with scatter
A workflow can be used as a step in another workflow. The following cwl example uses the above sig_bg_comb.cwl in
the many_sig_bg_comb step.
Listing 3: merge_many.cwl
cwlVersion: v1.0
class: Workflow
requirements:
ScatterFeatureRequirement: {}
SubworkflowFeatureRequirement: {}
inputs:
signals: string[]
backgrounds: string[]
outputs:
outDS:
type: string
outputSource: merge/outDS
steps:
many_sig_bg_comb:
run: sig_bg_comb.cwl
scatter: [signal, background]
scatterMethod: dotproduct
in:
signal: signals
background: backgrounds
out: [outDS]
(continues on next page)
68
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
merge:
run: prun
in:
opt_inDS: many_sig_bg_comb/outDS
opt_exec:
default: "python merge.py --type aaa --level 3 %IN"
opt_args:
default: "--outputs merged.root"
out: [outDS]
Note that sub-workflows require SubworkflowFeatureRequirement.
It is possible to run a task or sub-workflow multiple times over a list of inputs using ScatterFeatureRequirement.
A popular use-case is to perform the same analysis step on different samples in a single workflow. The step takes the
input(s) as an array and will run on each element of the array as if it were a single input. The many_sig_bg_comb
step above takes two string arrays, signals and backgrounds, and specifies in the scatter field that it loops over those
arrays. Output data from all many_sig_bg_comb tasks are fed into the merge task to produce the final output.
The workflow inputs are string arrays like
$ cat inputs2.yaml
signals:
- mc16_valid:mc16_valid.900248.PG_singlepion_flatPt2to50.simul.HITS.e8312_s3238_tid26378578_00
- valid1.427080.Pythia8EvtGen_A14NNPDF23LO_flatpT_Zprime.simul.HITS.e5362_s3718_tid26356243_00
background:
- mc16_5TeV.361238.Pythia8EvtGen_A3NNPDF23LO_minbias_inelastic_low.merge.HITS.e6446_s3238_s3250/
- mc16_5TeV:mc16_5TeV.361239.Pythia8EvtGen_A3NNPDF23LO_minbias_inelastic_high.merge.HITS.e6446_s3238_s
Then submit the workflow.
$ pchain --cwl merge_many.cwl --yaml inputs2.yaml --outDS user.<your_nickname>.blah
Using Athena
One or more tasks in a single workflow can use Athena as shown in the example below.
Listing 4: athena.cwl
cwlVersion: v1.0
class: Workflow
inputs: []
outputs:
outDS:
type: string
outputSource: third/outDS
(continues on next page)
4.1. For end-users
69
PanDAWMS
(continued from previous page)
steps:
first:
run: prun
in:
opt_exec:
default: "Gen_tf.py --maxEvents=1000 --skipEvents=0 --ecmEnergy=5020 --
˓→firstEvent=1 --jobConfig=860059 --outputEVNTFile=evnt.pool.root --randomSeed=4 --
˓→runNumber=860059 --AMITag=e8201"
opt_args:
default: "--outputs evnt.pool.root --nJobs 3"
opt_useAthenaPackages:
default: true
out: [outDS]
second:
run: prun
in:
opt_inDS: top/outDS
opt_exec:
default: "echo %IN > results.txt"
opt_args:
default: "--outputs results.txt"
out: [outDS]
third:
run: prun
in:
opt_inDS: second/outDS
opt_exec:
default: "echo %IN > poststep.txt"
opt_args:
default: "--outputs poststep.txt --athenaTag AnalysisBase,21.2.167"
opt_useAthenaPackages:
default: true
out: [outDS]
opt_useAthenaPackages corresponds to --useAthenaPackages of prun to remotely setup Athena with your
locally-built packages. You can use a different Athena version by specifying —athenaTag in opt_args.
To submit the task, first you need to setup Athena on local computer, and execute pchain with --useAthenaPackages
that automatically collect various Athena-related information from environment variables and uploads a sandbox file
from your locally-built packages.
$ pchain --cwl athena.cwl --yaml inputs.yaml --outDS user.<your_nickname>.blah --useAthenaPackages
70
Chapter 4. User Guide
PanDAWMS
Conditional workflow
Workflows can contain conditional steps executed based on their input. This allows workflows to wait execution of
subsequent tasks until previous tasks are done, and to skip subsequent tasks based on results of previous tasks. The fol-
lowing example contains conditional branching based on the result of the first step. Note that this workflows conditional
branching require InlineJavascriptRequirement and CWL version 1.2 or higher.
Listing 5: cond.cwl
cwlVersion: v1.2
class: Workflow
requirements:
InlineJavascriptRequirement: {}
inputs: []
outputs:
outDS_OK:
type: string
outputSource: bottom_OK/outDS
(continues on next page)
4.1. For end-users
71
PanDAWMS
(continued from previous page)
outDS_NG:
type: string
outputSource: bottom_NG/outDS
steps:
top:
run: prun
in:
opt_exec:
default: "echo %RNDM:10 > seed.txt"
opt_args:
default: "--outputs seed.txt --nJobs 2"
out: [outDS]
bottom_OK:
run: prun
in:
opt_inDS: top/outDS
opt_exec:
default: "echo %IN > results.txt"
opt_args:
default: "--outputs results.txt --forceStaged"
out: [outDS]
when: $(self.opt_inDS)
bottom_NG:
run: prun
in:
opt_inDS: top/outDS
opt_exec:
default: "echo hard luck > bad.txt"
opt_args:
default: "--outputs bad.txt"
out: [outDS]
when: $(!self.opt_inDS)
Both bottom_OK and bottom_NG steps take output data of the top step as input. The new property when specifies
the condition validation expression that is interpreted by JavaScript. self.blah in the expression represents the input
parameter blah of the step that is connected to output data of the parent step. If the parent step is successful self.blah
gives True while !self.blah gives False. It is possible to create more complicated expressions using logical operators
(&& for AND and || for OR) and parentheses. The step is executed when the whole expression gives True.
The bottom_NG step is executed when the top step fails and $(!self.opt_inDS) gives True. Note that in this case output
data from the top step is empty and the prun task in the bottom_NG step is executed without --inDS.
72
Chapter 4. User Guide
PanDAWMS
Involving hyperparameter optimization
It is possible to run Hyperparameter Optimization (HPO) and chain it with other tasks in the workflow. The following
example shows a chain of HPO and prun tasks.
Listing 6: hpo.cwl
cwlVersion: v1.0
class: Workflow
inputs: []
outputs:
outDS:
type: string
outputSource: post_proc/outDS
steps:
pre_proc:
run: prun
in:
opt_exec:
default: "echo %RNDM:10 > seed.txt"
opt_args:
default: "--outputs seed.txt --nJobs 2 --avoidVP"
out: [outDS]
main_hpo:
run: phpo
in:
opt_trainingDS: pre_proc/outDS
opt_args:
default: "--loadJson conf.json"
out: [outDS]
when: $(self.opt_trainingDS)
post_proc:
run: prun
in:
opt_inDS: main_hpo/outDS
opt_exec:
default: "echo %IN > anal.txt"
opt_args:
default: "--outputs anal.txt --nFilesPerJob 100 --avoidVP"
out: [outDS]
when: $(self.opt_inDS)
where the output data of the pre_proc step is used as the training data for the main_hpo step, and the output data
metrics.tgz of the main_hpo step is used as the input for the post_proc step. Both main_hpo and post_proc steps
specify when since they waits until the upstream step is done.
The run filed of a phpo task is phpo. Here is a list of parameters in the in section to run a prun task.
4.1. For end-users
73
PanDAWMS
Parameter
Corresponding phpo option
opt_trainingDS
—trainingDS (string)
opt_trainingDsType
No correspondence. Type of trainingDS (string)
opt_args
all other phpo options except for listed above (string)
opt_trainingDS can be omitted if the HPO task doesn’t take a training dataset. Note that you can put most phpo
options in a json and specify the json filename in —loadJson in opt_args, rather than constructing a complicated
string in opt_args.
$ cat config.json
"evaluationContainer": "docker://gitlab-registry.cern.ch/zhangruihpc/evaluationcontainer:mlflow",
"ev
$ pchain -cwl hpo.cwl --yaml dummy.yaml --outDS user.<your_nickname>.blah
Loops in workflows
Users can have loops in their workflows. Each loop is represented as a sub-workflow with a parameter dictionary. All
tasks in the sub-workflow share the dictionary to generate actual steps. There are special tasks, called junction, which
read outputs from upstream tasks, update the parameter dictionary, and make a decision to exit from the sub-workflow.
The sub-workflow is iterated until one of junctions decides to exit. The new iteration is executed with the updated
values in the parameter dictionary, so that each iteration can bring different results.
For example, the following pseudo-code snippet has a single loop
out1 = work_start()
xxx = 123
yyy = 0.456
while True:
out2 = inner_work_top(out1, xxx)
out3 = inner_work_bottom(out2, yyy)
xxx = out2 + 1
yyy *= out3
if yyy > xxx:
break
out4 = work_end(out3)
The code is described using CWL as follows.
Listing 7: loop.cwl
cwlVersion: v1.0
class: Workflow
requirements:
SubworkflowFeatureRequirement: {}
inputs: []
outputs:
outDS:
type: string
(continues on next page)
74
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
outputSource: work_end/outDS
steps:
work_start:
run: prun.cwl
in:
opt_exec:
default: "echo %RNDM:10 > seed.txt"
opt_args:
default: "--outputs seed.txt --nJobs 2 --avoidVP"
out: [outDS]
work_loop:
run: loop_body.cwl
in:
dataset: work_start/outDS
out: [outDS]
hints:
- loop
work_end:
run: prun.cwl
in:
opt_inDS: work_loop/outDS
opt_exec:
default: "echo %IN > results.root"
opt_args:
(continues on next page)
4.1. For end-users
75
PanDAWMS
(continued from previous page)
default: "--outputs results.root --forceStaged"
out: [outDS]
The work_loop step describes the looping stuff in the while block of the pseudo-code snippet. It runs a separate CWL
file loop_body.cwl and has loop in the hints section to iterate.
Listing 8: loop_body.cwl
cwlVersion: v1.0
class: Workflow
inputs:
dataset:
type: string
param_xxx:
type: int
default: 123
param_yyy:
type: float
default: 0.456
outputs:
outDS:
type: string
outputSource: inner_work_bottom/outDS
steps:
inner_work_top:
run: prun
in:
opt_inDS: dataset
opt_exec:
default: "echo %IN %{xxx} > seed.txt"
opt_args:
default: "--outputs seed.txt --avoidVP"
out: [outDS]
inner_work_bottom:
run: prun
in:
opt_inDS: inner_work_top/outDS
opt_exec:
default: "echo %IN %{yyy} > results.root"
opt_args:
default: "--outputs results.root --forceStaged"
out: [outDS]
checkpoint:
run: junction
in:
(continues on next page)
76
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
opt_inDS:
- inner_work_top/outDS
- inner_work_bottom/outDS
opt_exec:
default: "echo 1"
out: []
The local variables in the loop like xxx and yyy are defined in the inputs section of loop_body.cwl with the param_
prefix and their initial values. They are internally translated to the parameter dictionary shared by all tasks in the sub-
workflow. In each iteration, %{blah} in opt_args is replaced with the actual value in the dictionary. A loop count
is inserted to the output dataset names, like user.<your_nickname>.blah_<loop_count>_<output>. The loop count is
incremented for each iteration, so tasks in a loop produce unique output datasets in each iteration.
The checkpoint step runs junction to read outputs from inner_work_top and inner_work_bottom step in the iteration,
and update values in the parameter dictionary. The payload in the checkpoint step produces a json file with key-values
to update in the dictionary, and a special key-value to_terminate: True to exit from the loop and execute subsequent
steps outside of the loop.
Loop + scatter
A loop is sequential iteration of a sub-workflow, while a scatter is a horizontal parallelization of independent sub-
workflows. They can be combined to describe complex workflows.
The following example runs multiple loops in parallel.
Listing 9: multi_loop.cwl
cwlVersion: v1.0
class: Workflow
requirements:
ScatterFeatureRequirement: {}
SubworkflowFeatureRequirement: {}
inputs:
signals: string[]
outputs:
outDS:
type: string
outputSource: work_end/outDS
steps:
work_loop:
run: loop_body.cwl
scatter: [dataset]
scatterMethod: dotproduct
in:
dataset: signals
out: [outDS]
(continues on next page)
4.1. For end-users
77
PanDAWMS
78
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
hints:
- loop
merge:
run: prun
in:
opt_inDS: work_loop/outDS
opt_exec:
default: "echo %IN > results.root"
opt_args:
default: "--outputs results.root --forceStaged"
out: [outDS]
The work_loop step has the loop hint and is scattered over the list of inputs.
Here is another example of the loop+scatter combination that sequentially iterates parallel execution of multiple tasks.
Listing 10: sequential_loop.cwl
cwlVersion: v1.0
class: Workflow
requirements:
ScatterFeatureRequirement: {}
SubworkflowFeatureRequirement: {}
inputs:
signal: string
outputs:
outDS:
type: string
outputSource: seq_loop/outDS
(continues on next page)
4.1. For end-users
79
PanDAWMS
(continued from previous page)
steps:
seq_loop:
run: scatter_body.cwl
in:
dataset: signal
out: [outDS]
hints:
- loop
The seq_loop step iterates scatter_body.cwl which defines an array of parameter dictionaries with the the param_ prefix
and initial values; param_xxx and param_xxx. The parallel_work step is scattered over the dictionary array. The
dictionary array is vertically sliced so that each execution of loop_main.cwl gets only one parameter dictionary. The
checkpoint step takes all outputs from the blue:parallel_work step to update the entire dictionary array and make a
decision to exit the sub-workflow.
Listing 11: scatter_body.cwl
cwlVersion: v1.0
class: Workflow
requirements:
ScatterFeatureRequirement: {}
SubworkflowFeatureRequirement: {}
inputs:
dataset: string
param_xxx:
type: int[]
default: [123, 456]
param_yyy:
type: float[]
default: [0.456, 0.866]
outputs:
outDS:
type: string
outputSource: /outDS
steps:
parallel_work:
run: loop_main.cwl
scatter: [param_xxx, param_yyy]
scatterMethod: dotproduct
in:
dataset: dataset
param_xxx: param_xxx
param_yyy: param_yyy
out: [outDS]
(continues on next page)
80
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
checkpoint:
run: junction
in:
opt_inDS: parallel_work/outDS
opt_exec:
default: "echo 1"
out: []
The looping parameters like param_xxx and param_xxx must be re-defined in loop_main.cwl as well as scat-
ter_body.cwl, to have a parameter dictionary in the nested sub-workflow. Note that they must have the same names,
while their initial values are scalars instead of arrays. In each iteration the checkpoint step above updates the values
in the parameter dictionary, so that %{blah} in opt_args is replaced with the updated value when the task is actually
executed.
Listing 12: loop_main.cwl
cwlVersion: v1.0
class: Workflow
inputs:
dataset:
type: string
param_xxx:
type: int
param_yyy:
type: float
outputs:
outDS:
type: string
outputSource: core/outDS
steps:
core:
run: prun.cwl
in:
opt_inDS: dataset
opt_exec:
default: "echo %IN %{xxx} %{yyy} > seed.txt"
opt_args:
default: "--outputs seed.txt --avoidVP"
out: [outDS]
4.1. For end-users
81
PanDAWMS
Debugging locally
Workflow descriptions can be error-prone. It is better to check workflow descriptions before submitting them. pchain
has the --check option to verify the workflow description locally. You just need to add the --check option when
running pchain. For example,
$ pchain --cwl test.cwl --yaml dummy.yaml --outDS user.<your_nickname>.blah --check
which should give a message like
INFO : uploading workflow sandbox
INFO : check workflow user.tmaeno.c63e2e54-df9e-402a-8d7b-293b587c4559
INFO : messages from the server
internally converted as follows
ID:0 Name:top Type:prun
Parent:
Input:
opt_args: --outputs seed.txt --nJobs 2 --avoidVP
opt_exec: echo %RNDM:10 > seed.txt
Output:
...
INFO : Successfully verified workflow description
Your workflow description is sent to the server to check whether the options in opt_args are correct, dependencies
among steps are valid, and input and output data are properly resolved.
Monitoring
Bookkeeping
pbook provides the following commands for workflow bookkeeping
show_workflow
kill_workflow
retry_workflow
finish_workflow
pause_workflow
resume_workflow
Please refer to the pbook documentation for their details.
82
Chapter 4. User Guide
PanDAWMS
4.1.9 Using secrets
Introduction
A secret is a small amount of sensitive data such as an access token and a password. PanDA allows users to define
arbitrary key-value strings to feed secrets to jobs. Note that they are random strings from PanDA’s point of view, and
users can even encrypt those strings as they like, so the system should not abuse the sensitive information.
How to manage secrets
pbook provides following functions to manage secrets.
set_secret
list_secrets
delete_secret
delete_all_secrets
You can define a set of key-value strings using set_secret.
>>> set_secret('MY_SECRET', 'random_string')
INFO : OK
The value must be a string. If you want to define non-string data, serialize it using json.dumps or something. E.g.,
>>> import json
>>> set_secret('MY_SECRET_SER', json.dumps({'a_key': 'a_value'}))
INFO : OK
list_secrets shows all secrets.
>>> list_secrets()
Key
: Value
------------- : --------------------
MY_SECRET
: random_string
MY_SECRET_SER : {"a_key": "a_value"}
You can delete secrets using delete_secret and/or delete_all_secrets.
Using secrets in your jobs
prun has the –useSecrets option to feed secrets into jobs running on computing resources. Once jobs get started
panda_secrets.json should be available in the current directory. Your applications would do something like
import json
with open('panda_secrets.json') as f:
secrets = json.load(f)
do_something_with_a_secret(secrets['MY_SECRET'])
4.1. For end-users
83
PanDAWMS
4.2 Python API references
4.2.1 PanDA system python API reference
Low-level system API are available via the pandaclient.Client module.
from pandaclient import Client
Client.function_xyz(...)
System API
finishTask(jediTaskID, soft=False, verbose=False)
finish a task
args:
jediTaskID: jediTaskID of the task to finish
soft: True to wait until running jobs are done
verbose: True to see debug messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
tuple of return code and diagnostic message, or None if failed
0: request is registered
1: server error
2: task not found
3: permission denied
4: irrelevant task status
100: non SSL connection
101: irrelevant taskID
getFullJobStatus(ids, verbose=False)
Get detailed status of jobs
args:
ids: a list of PanDA IDs
verbose: True to see verbose messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
a list of job specs, or None if failed
getJobStatus(ids, verbose=False)
Get status of jobs
args:
ids: a list of PanDA IDs
verbose: True to see verbose messages
returns:
status code
0: communication succeeded to the panda server
(continues on next page)
84
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
255: communication failure
a list of job specs, or None if failed
getPandaIDsWithTaskID(jediTaskID, verbose=False)
Get PanDA IDs with TaskID
args:
jediTaskID: jediTaskID of the task to get lit of PanDA IDs
returns:
status code
0: communication succeeded to the panda server
255: communication failure
the list of PanDA IDs, or error message if failed
getTaskParamsMap(jediTaskID)
Get task parameters
args:
jediTaskID: jediTaskID of the task to get taskParamsMap
returns:
status code
0: communication succeeded to the panda server
255: communication failure
return: a tuple of return code and taskParamsMap, or error message if failed
1: logical error
0: success
None: database error
getTaskStatus(jediTaskID, verbose=False)
Get task status
args:
jediTaskID: jediTaskID of the task to get lit of PanDA IDs
verbose: True to see verbose messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
the status string, or error message if failed
getUserJobMetadata(task_id, verbose=False)
Get metadata of all jobs in a task
args:
jediTaskID: jediTaskID of the task
verbose: True to see verbose message
returns:
status code
0: communication succeeded to the panda server
255: communication failure
a list of job metadata dictionaries, or error message if failed
(continues on next page)
4.2. Python API references
85
PanDAWMS
(continued from previous page)
get_user_name_from_token()
Extract user name and groups from ID token
returns:
a tuple of username and groups
hello(verbose=False)
Health check with the PanDA server
args:
verbose: True to see verbose message
returns:
status code
0: communication succeeded to the panda server
255: communication failure
diagnostic message
insertTaskParams(taskParams, verbose=False, properErrorCode=False)
Insert task parameters
args:
taskParams: a dictionary of task parameters
verbose: True to see verbose messages
properErrorCode: True to get a detailed error code
returns:
status code
0: communication succeeded to the panda server
255: communication failure
tuple of return code, message from the server, and taskID if successful, or␣
˓→error message if failed
0: request is processed
1: duplication in DEFT
2: duplication in JEDI
3: accepted for incremental execution
4: server error
killJobs(ids, verbose=False)
Kill jobs
args:
ids: a list of PanDA IDs
verbose: True to see verbose messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
a list of server responses, or None if failed
killTask(jediTaskID, verbose=False)
Kill a task
args:
jediTaskID: jediTaskID of the task to be killed
(continues on next page)
86
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
verbose: True to see debug messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
tuple of return code and diagnostic message, or None if failed
0: request is registered
1: server error
2: task not found
3: permission denied
4: irrelevant task status
100: non SSL connection
101: irrelevant taskID
putFile(file, verbose=False, useCacheSrv=False, reuseSandbox=False)
Upload a file with the size limit on 10 MB
args:
file: filename to be uploaded
verbose: True to see debug messages
useCacheSrv: True to use a dedicated cache server separated from the PanDA server
reuseSandbox: True to avoid uploading the same sandbox files
returns:
status code
0: communication succeeded to the panda server
255: communication failure
diagnostic message
reactivateTask(jediTaskID, verbose=False)
Reactivate task
args:
jediTaskID: jediTaskID of the task to be reactivated
verbose: True to see verbose messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
return: a tupple of return code and message, or error message if failed
0: unknown task
1: succeeded
None: database error
resumeTask(jediTaskID, verbose=False)
Resume task
args:
jediTaskID: jediTaskID of the task to be resumed
verbose: True to see verbose messages
returns:
status code
0: communication succeeded to the panda server
255: communication failure
(continues on next page)
4.2. Python API references
87
PanDAWMS
(continued from previous page)
return: a tupple of return code and message, or error message if failed
0: request is registered
1: server error
2: task not found
3: permission denied
4: irrelevant task status
100: non SSL connection
101: irrelevant taskID
None: database error
retryTask(jediTaskID, verbose=False, properErrorCode=False, newParams=None)
retry a task
args:
jediTaskID: jediTaskID of the task to retry
verbose: True to see debug messages
newParams: a dictionary of task parameters to overwrite
properErrorCode: True to get a detailed error code
returns:
status code
0: communication succeeded to the panda server
255: communication failure
tuple of return code and diagnostic message, or None if failed
0: request is registered
1: server error
2: task not found
3: permission denied
4: irrelevant task status
100: non SSL connection
101: irrelevant taskID
send_command_to_job(panda_id, com)
args:
panda_id: PandaID of the job
com: a command string passed to the pilot. max 250 chars
returns:
status code
0: communication succeeded to the panda server
255: communication failure
return: a tuple of return code and message
False: failed
True: the command received
send_file_recovery_request(task_id, dry_run=False, verbose=False)
Send a file recovery request
args:
task_id: task ID
dry_run: True to run in the dry run mode
verbose: True to see verbose message
returns:
status code
0: communication succeeded to the panda server
255: communication failure
(continues on next page)
88
Chapter 4. User Guide
PanDAWMS
(continued from previous page)
a tuple of (True/False and diagnostic message). True if the request was accepted
4.2.2 Using iDDS API through PanDA
iDDS delegates user authentication and authorization to the PanDA server so that iDDS APIs are available through
panda-client. The PanDA server forwards requests to iDDS after receiving them from users and propagates responses
from iDDS back to the users. The following code snippets show how iDDS native codes migrate to panda-client based
codes:
• iDDS native
from idds.client.client import Client
from idds.client.clientmanager import ClientManager
import idds.common.constants
import idds.common.utils
data = {
'requester': 'panda',
'request_type': idds.common.constants.RequestType.HyperParameterOpt,
'transform_tag': idds.common.constants.RequestType.HyperParameterOpt.value,
'status': idds.common.constants.RequestStatus.New,
'priority': 0,
'lifetime': 30,
'request_metadata': {},
}
# using Client API
cl = Client(idds.common.utils.get_rest_host())
try:
request_id = cl.add_request(**data)
except Except:
# error
# using ClientManager
cm = ClientManager(idds.common.utils.get_rest_host())
try:
req = cm.get_requests(request_id=request_id)
except Exception:
# error
• panda-client based
import pandaclient.idds_api
import idds.common.constants
import idds.common.utils
data = {
'requester': 'panda',
'request_type': idds.common.constants.RequestType.HyperParameterOpt,
'transform_tag': idds.common.constants.RequestType.HyperParameterOpt.value,
'status': idds.common.constants.RequestStatus.New,
(continues on next page)
4.2. Python API references
89
PanDAWMS
(continued from previous page)
'priority': 0,
'lifetime': 30,
'request_metadata': {},
}
# using Client API
cl = pandaclient.idds_api.get_api(idds.common.utils.json_dumps, compress=True)
ret = cl.add_request(**data)
if ret[0] == 0 and ret[1][0]:
request_id = ret[1][-1]
else:
# error
# using ClientManager
cm = pandaclient.idds_api.get_api(idds.common.utils.json_dumps, compress=True,␣
˓→manager=True)
ret = cm.get_requests(request_id=request_id)
if ret[0] == 0 and ret[1][0]:
req = ret[1][-1]
else:
# error
All client functions of idds.client.client.Client and idds.client.clientmanager.ClientManager are
available in the API object, which is returned by pandaclient.idds_api.get_api(), with the same arguments.
Check with iDDS documentation for the details of iDDS API. Here is the description of pandaclient.idds_api.
get_api().
get_api(dumper=None, verbose=False, idds_host=None, compress=False, manager=False)
Get an API object to access iDDS through PanDA
args:
dumper: function object to json-serialize data
verbose: True to see verbose messages
idds_host: iDDS host. e.g. https://aipanda160.cern.ch:443/idds
compress: True to compress request body
manager: True to use ClientManager API. False by default to use Client API
return:
an API object
The returns from any function of the API object are always as follows.
returns:
status code
0: communication succeeded to the panda server
255: communication failure
a tuple of (True, the original response from iDDS), or (False, diagnostic message) if␣
˓→failed
90
Chapter 4. User Guide
PanDAWMS
4.3 Monitoring
4.3.1 PanDA Monitoring
PanDA provides advanced Web based monitoring for different groups of PanDA users: scientists, developers, operators,
managers, shifters. Panda monitor can also serve a data source for users scripts and custom automatizing. In this section
we basic information, needed to monitor jobs and tasks submitted into PanDA.
Task monitoring
Task is the basic entity creating when a new payload comes to PanDA. There are two views in the PanDA monitoring
for navigation over tasks: Tasks list and a Task view. The former view displays the selection of tasks:
ATLAS PanDA
DOMA PanDA
Arbitary monitoring instance
https://bigpanda.cern.ch/tasks/
http://panda-doma.cern.ch/tasks/
https://<monitoringhost>/tasks/
There are different parameters could be used together with tasks list to provide narrow selection of tasks:
• days=<number>, hours=<number> defines the left boundary of the time window used for the query. Right
boundary is the current time.
• date_from=<YYYY-MM-DDThh:mm or YYYY-MM-DD>, date_to=(YYYY-MM-DDThh:mm or YYYY-
MM-DD) defines exact time range of modification time for tasks selection
• endtime_from=<YYYY-MM-DDThh:mm or YYYY-MM-DD>, endtime_to=<YYYY-MM-DDThh:mm
or YYYY-MM-DD>, endtimerange=<YYYY-MM-DDThh:mm|YYYY-MM-DDThh:mm> defines time
boundaries for task end time.
• earlierthan=<number>, earlierthandays=<number> defines the right boundary in hours or days of the time
window of tasks selection relative to the current time.
• username=<string> selects tasks by user name of a person who submitted them. This parameter supports
asterics, e.g. username=James*.
• tasktype=<prod, anal> filters tasks by the payload origin - production or analysis.
• limit=<number> limits the size of the data to be retrieved from the PanDA database in order to serve the current
query. This parameter could require some big value (e.g. 100000) in order to deliver more data. By default this
value is limited to 20000.
• display_limit=<number> number of tasks with extended information provided to the query results.
• status=<failed, done, running, > selects tasks which are in one of the status enlisted here.
• taskname=<string>
filters
tasks
by
its
name.
This
parameter
supports
asterics,
e.g.
taskname=shared_pipecheck_20210301T161238Z*.
Here are few examples of such queries:
ATLAS PanDA
DOMA PanDA
Arbitary monitoring instance
4.3. Monitoring
91
PanDAWMS
https://bigpanda.cern.ch/tasks/?display_limit=100
https://bigpanda.cern.ch/tasks/?date_from=2021-02-01&date_to=2021-02-03&limit=1000
http://panda-doma.cern.ch/tasks/?display_limit=100
https://panda-doma.cern.ch/tasks/?days=120&taskname=shared_pipecheck_20210301T161238Z*
https://<monitoringhost>/tasks/?<task_filter_parameters>
An individual task is accessible by its ID:
ATLAS PanDA
DOMA PanDA
Arbitary monitoring instance
https://bigpanda.cern.ch/task/24559935/
https://panda-doma.cern.ch/task/909/
https://<monitoringhost>/task/?<task_id>
Jobs monitoring
Task view provides links to associated jobs in the “Job status summary” table. However jobs could be also accessed
independently to the task view. Jobs list query parameters are the similar to ones as for the tasks list:
ATLAS PanDA
DOMA PanDA
Arbitary monitoring instance
https://bigpanda.cern.ch/jobs/?jobstatus=finished
https://bigpanda.cern.ch/jobs/?jobstatus=failed&endtimerange=2021-03-15T10:00|2021-03-15T10:30
https://bigpanda.cern.ch/jobs/?jobstatus=failed&date_from=2021-03-15T10:00&date_to=2021-03-15T10:30
https://panda-doma.cern.ch/jobs/?jobstatus=finished
https://panda-doma.cern.ch/jobs/?jobstatus=failed&endtimerange=2021-03-15T10:00|2021-03-15T10:30
https://panda-doma.cern.ch/jobs/?jobstatus=failed&date_from=2021-03-15T10:00&date_to=2021-03-15T10:30
https://<monitoringhost>/jobs/?<jobs_filter_parameters>
An individual job is accessible by its ID:
ATLAS PanDA
DOMA PanDA
Arbitary monitoring instance
https://bigpanda.cern.ch/job?pandaid=5000107972
https://panda-doma.cern.ch/job?pandaid=253627
https://\T1\textless{}monitoringhost\T1\textgreater{}/job?pandaid=\T1\textless{}panda_id\T1\textgreater{}
92
Chapter 4. User Guide
PanDAWMS
Retrieving job log
PanDA monitoring provides access to logs generated by a payload or/and correspondent Pilot:
ATLAS PanDA
DOMA PanDA
Logs become available when a job in the final state.
Information retrieval
PanDA monitoring could be used as a source of information for user’s scripts and applications. To fetch data in JSON
format an &jobs flag should be applied to a query, e.g. https://bigpanda.cern.ch/task/24559935/?json .
4.3. Monitoring
93
PanDAWMS
94
Chapter 4. User Guide
CHAPTER
FIVE
ADMINISTRATOR GUIDE
Here is a quick tutorial to setup a minimum PanDA system.
Table of Contents
• 0. Hardware Requirements
• 1. Database Setup
• 2. PanDA Server Setup
• 3. JEDI Setup
• 4. Registration of Resource Groups, Global Shares, and computing resources in the Database
– 4.1. Resource Group Registration
– 4.2. Global Share Registration
– 4.3. Computing resource Registration
– 4.4. Resource Type Registration
• 5. Testing JEDI and the PanDA server
• 6. Harvester Setup
– 6.1. Queue Configuration
– 6.2 Testing Harvester
• 7. PanDA Monitor Setup
5.1 0. Hardware Requirements
It is recommended to install JEDI and the PanDA server on separate virtual machines (VMs), but it is possible to install
them on a single VM for small testing purposes. A minimum PanDA system would be composed of 3 VMs; the first
VM for JEDI and the PanDA server, the second VM for Harvester, and the third VM for the PanDA monitor. The
following table shows the minimum hardware configuration.
95
PanDAWMS
Table 1: Minimum hardware configuration
Component
Cores
RAM (GB)
Disk (GB)
JEDI + PanDA server
4
8
100
Harvester
4
8
100
BigPandaMon
8
16
70
5.2 1. Database Setup
The database is the backbone of the PanDA server and JEDI, so it needs to be setup before start installation of those
components. You should go through the Database page.
5.3 2. PanDA Server Setup
The next step is to install the PanDA server on a VM following PanDA server installation guide. You need to decide the
userid and group under which the PanDA server runs before editing configuration files. Make sure that the userid and
group are consistent in panda_server.cfg and panda_server-httpd.conf, the permission of log directories is set
accordingly. It would be good to optimize the number of processes in the httpd.conf based on your VM’s configuration,
e.g,
StartServers
4
MinSpareServers
4
ServerLimit
64
MaxSpareServers
64
MaxClients
64
MaxRequestsPerChild
2000
WSGIDaemonProcess pandasrv_daemon processes=4 threads=1 home=/home/iddssv1 inactivity-
˓→timeout=600
Then add a new virtual organization following this section. Make sure that the organization is added to PanDA IAM.
We use the wlcg organization in this tutorial. You also need to configure the firewall on the VM to allow access to
25080 and 25443 from outside.
5.4 3. JEDI Setup
Once the PanDA server is ready, you can install JEDI on the same VM following JEDI installation guide. You need to
use the name of the virtual organization when configuring plugins in panda_jedi.cfg. For testing purposes it would
be enough to use generic plugins as shown below:
96
Chapter 5. Administrator Guide
PanDAWMS
[ddm]
modConfig = wlcg:1:pandajedi.jediddm.GenDDMClient:GenDDMClient
[confeeder]
procConfig = wlcg:any:1
[taskrefine]
modConfig = wlcg:any:pandajedi.jedirefine.GenTaskRefiner:GenTaskRefiner
procConfig = ::1
[jobbroker]
modConfig = wlcg:any:pandajedi.jedibrokerage.GenJobBroker:GenJobBroker
[jobthrottle]
modConfig = wlcg:any:pandajedi.jedithrottle.GenJobThrottler:GenJobThrottler
[jobgen]
procConfig = wlcg:any:1:
[postprocessor]
modConfig = wlcg:any:pandajedi.jedipprocess.GenPostProcessor:GenPostProcessor
procConfig = ::1
[watchdog]
modConfig = wlcg:any:pandajedi.jedidog.GenWatchDog:GenWatchDog
procConfig = wlcg:any:1
[taskbroker]
modConfig = wlcg:any:pandajedi.jedibrokerage.GenTaskBroker:GenTaskBroker
procConfig = wlcg:any:1
[tcommando]
procConfig = ::1
[tasksetup]
modConfig = wlcg:any:pandajedi.jedisetup.GenTaskSetupper:GenTaskSetupper
5.5 4. Registration of Resource Groups, Global Shares, and comput-
ing resources in the Database
You need to manually register VO, global shares, and computing resources unless they are automatically registered
through information system. If you integrate CRIC as explained at CRIC integration guide, you can register them
through CRIC.
5.5. 4. Registration of Resource Groups, Global Shares, and computing resources in the Database
97
PanDAWMS
5.5.1 4.1. Resource Group Registration
It is possible to define grouping among computing resources but generally it is enough to have one group for each
organization. Groups are registered in the CLOUDCONFIG table in the PANDAMETA schema using the following SQL
statement.
INSERT INTO DOMA_PANDAMETA.CLOUDCONFIG (NAME,DESCRIPTION,TIER1,TIER1SE,WEIGHT,SERVER,
˓→STATUS,
TRANSTIMELO,TRANSTIMEHI,WAITTIME,SPACE,MODTIME,MCSHARE,NPRESTAGE)
VALUES('A_GROUP0','some description','NA','NA',0,'NA','online',0,0,0,0,CURRENT_DATE,0,
˓→0);
where NAME is an arbitrary group name and STATUS needs to be set to “online”. Replace “PANDAMETA” with your
schema name for the meta tables.
5.5.2 4.2. Global Share Registration
Each organization defines computing resource allocation among various working groups and/or user activities using
global shares. Normal global shares are registered in the GLOBAL_SHARES table, while special and/or resource-specific
shares are registered in the JEDI_WORK_QUEUE table. The following SQL statement adds a special test share.
INSERT INTO DOMA_PANDA.JEDI_WORK_QUEUE (QUEUE_ID,QUEUE_NAME,QUEUE_TYPE,VO,QUEUE_FUNCTION)
VALUES(1,'test_queue','test','wlcg','Resource');
where VO and QUEUE_TYPE are organization and activity names, respectively. Replace “PANDA” with your schema
name for the JEDI tables.
5.5.3 4.3. Computing resource Registration
The following SQL statement adds a test resource.
INSERT INTO DOMA_PANDA.SITE (SITE_NAME) VALUES('TEST_SITE');
INSERT INTO DOMA_PANDA.PANDA_SITE (PANDA_SITE_NAME,SITE_NAME) VALUES('TEST_SITE','TEST_
˓→SITE');
INSERT INTO DOMA_PANDA.SCHEDCONFIG_JSON (PANDA_QUEUE,DATA) values('TEST_TEST','{"status":
˓→"online", "cloud": "WLCG"}');
where cloud is the group name, and status needs to be ‘online’.
5.5.4 4.4. Resource Type Registration
You need the catchall resource type at least.
INSERT INTO DOMA_PANDA.RESOURCE_TYPES (RESOURCE_NAME,MINCORE) VALUES('Undefined',1);
98
Chapter 5. Administrator Guide
PanDAWMS
5.6 5. Testing JEDI and the PanDA server
At this stage, you can submit a test task to the PanDA server and let JEDI generate jobs. Before start testing, start the
PanDA server and JEDI.
$ /sbin/service httpd-pandasrv start
$ /sbin/service panda-jedi start
Then setup panda-client as explained at panda-client setup guide. You need to set PANDA_URL_SSL and PANDA_URL
after sourcing panda_setup.sh, to point to your PanDA server, e.g.,
export PANDA_URL_SSL=https://ai-idds-01.cern.ch:25443/server/panda
export PANDA_URL=http://ai-idds-01.cern.ch:25080/server/panda
in addition to the parameters mentioned at client setup for OIDC-based auth, e.g.,
export PANDA_AUTH=oidc
export PANDA_AUTH_VO=wlcg
export PANDA_VERIFY_HOST=off
An example of a test task is available at this link.
$ wget https://raw.githubusercontent.com/PanDAWMS/panda-jedi/master/pandajedi/jeditest/addNonAtlasTask.p
In this script
taskParamMap['vo'] = 'wlcg'
taskParamMap['prodSourceLabel'] = 'test'
taskParamMap['site'] = 'TEST_SITE'
they would need to be changed to organization, activity, computing resource names registered in the previous step.
Then
$ python addNonAtlasTask.py
You will see a jediTaskID if successful.
The task is passed to JEDI through the PanDA server, and goes through TaskRefiner, ContentsFeeder, and
JobGenerator agents in JEDI. Each agent should give logging messages in logdir/panda-AgentName.log like
2021-02-24 07:34:13,694 panda.log.TaskRefiner: DEBUG
< jediTaskID=24326915 > start
And once jobs are submitted there should be messages like
2021-02-24 07:34:52,905 panda.log.JobGenerator: INFO
<jediTaskID=24326915␣
˓→datasetID=359212908> submit njobs=1 jobs
in logdir/panda-JobGenerator.log. There should be also many messages in logdir/panda-JediDBProxy.log about
database interactions.
Jobs are passed to the PanDA server. If you see something like
2021-02-24 07:34:29,399 panda.log.DBProxy: DEBUG
activateJob : 4981974846
in logdir/panda-DBProxy.log this means that the job successfully went through PanDA server components and is
ready to be pickup by the pilot.
5.6. 5. Testing JEDI and the PanDA server
99
PanDAWMS
5.7 6. Harvester Setup
In this tutorial we use HTCondor as submission backend, so first you need to install HTCondor on the VM where
Harvester will be installed. HTCondor documentation will help.
Then refer to Harvester installation guide to install Harvester on the same VM. For small scale tests it is enough to use
the sqlite3 database backend. Make sure that harvester_id in panda_harvester.cfg can be an arbitrary unique
string but it needs to be registered in the database of JEDI and the PanDA server (i.e., not the harvester database),
INSERT INTO DOMA_PANDA.HARVESTER_INSTANCES (HARVESTER_ID,DESCRIPTION) VALUES('your_
˓→harvester_id','some description');
5.7.1 6.1. Queue Configuration
In this tutorial, queues are specified in a local json file, so panda_harvester.cfg has
[qconf]
configFile = panda_queueconfig.json
queueList =
ALL
panda_queueconfig.json could be something like a config example where the computing resource defined in the
previous step TEST_SITE is set to “online”.
"TEST_SITE": {
"queueStatus": "online",
"prodSourceLabel": "test",
"templateQueueName": "production.pull",
"maxWorkers": 1,
"nQueueLimitWorkerMin": 1,
"nQueueLimitWorkerMax": 2,
"submitter": {
"templateFile": "/opt/panda/misc/grid_submit_pilot.sdf"
}
},
}
where the templateFile is a template file to generate sdf files like an sdf template example Each sdf file has
executable = /opt/panda/misc/runpilot2-wrapper.sh
arguments = -s {computingSite} -r {computingSite} -q {pandaQueueName} -j
˓→{prodSourceLabel} -i {pilotType} \
-t -w generic --pilot-user generic --url https://ai-idds-01.cern.ch -d --harvester-
˓→submit-mode PULL \
--allow-same-user=False --job-type={jobType} {pilotResourceTypeOption}
˓→{pilotUrlOption}
to launch the pilot on a worker node. runpilot2-wrapper.sh is available in the pilot-wrapper repository. You need
to put a template file and the pilot wrapper on the VM, and edit the template file and panda_queueconfig.json
accordingly. Note that the --url argument must take the URL of your PanDA server so that the pilot will talk to your
PanDA server.
100
Chapter 5. Administrator Guide
PanDAWMS
5.7.2 6.2 Testing Harvester
Now you can start Harvester to submit the pilot and see if the pilot properly communicates with the PanDA server.
$ etc/rc.d/init.d/panda_harvester start
Harvester logs are available in the directory specified in panda_common.cfg.
It is good to check
panda_harvester_stdout.log, panda_harvester_stderr.log, and panda-submitter.log. Once the pilot
is sent out through HTCondor, there should be log files in the directly specified in the sdf template file.
log = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).log
output = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).out
error = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).err
where {logDir} is specified in panda_queueconfig.json and {logSubdir} is automatically defined by Harvester
based on the timestamp.
If communication between the pilot and the PanDA server is successful there will be messages in PanDA server’s log
files such as panda_server_access_log`, `panda-JobDispatcher.log, and panda-DBProxy.log.
5.8 7. PanDA Monitor Setup
5.8. 7. PanDA Monitor Setup
101
PanDAWMS
102
Chapter 5. Administrator Guide
CHAPTER
SIX
DEVELOPER GUIDE
The following documentation tells you how you can be involved with the PanDA project, adding new features, improving
documentation, and fixing bugs.
6.1 Contributing Changes
PanDA components are developed based on GitHub Flow with small customization. The sequence is as follows:
1. Creation of a branch or fork. In git, branches are lightweight things that are often temporary and may be deleted
anytime. The master branch is protected so that only a couple of persons can push commits there.
2. Adding commits to the branch. You can experiment with any changes since they don’t affect the master branch.
3. Deployment of changes on a test instance. All changes must be verified before being merged to the master branch.
4. Submission of a Pull Request which will initiate a discussion about your changes.
5. Discussion and review on your code. Once a Pull Request has been opened, your changes are reviewed to check
functionalities, the coding style, test results, and so on. Note that your code must follow PEP8.
6. Merging changes. Once your changes are approved, they will be merged to the master branch. The branch can
be deleted at this stage since commit logs are also merged.
6.1.1 How to install your local changes to your instances
You can install and test your changes locally before submitting pull requests. The following example shows how to
install local changes to your own PanDA server.
$ # checkout the repository
$ git clone https://github.com/PanDAWMS/panda-server.git
$ # add changes in panda-server
$ cd panda-server
$ ...
$ # stop the PanDA server
$ /sbin/service httpd-pandasrv stop
$ # go to virtual env if necessary
$ . <venv_dir>/bin/activate
103
PanDAWMS
$ # make sdist and install it twice with different pip options since pip doesn't install it
$ # without those steps when the version number is incremented
$ cd panda-server
$ rm -rf dist
$ python setup.py sdist
$ pip install dist/p*.tar.gz --upgrade --force-reinstall --no-deps; pip install dist/p*.tar.gz --upgrade
6.1.2 Publishing a new version of panda-* package on PyPI
The procedures are as follows:
1. Increment the version number, which is typically defined in panda-*/PandaPkgInfo.py.
2. Push it to the master branch of the git repository.
3. Make a new release on the master branch of the git repository in GitHub.
which automatically triggers a git action to publish the version on PyPI.
6.2 Writing Documentation
This section explains how to write PanDA documentation. If you just want to add changes to existing documentation
see Adding Changes to Documentation.
6.2.1 1. Preparation
First, you need to install sphinx packages
$ pip install sphinx
$ pip install sphinx-rtd-theme
$ pip install sphinx-prompt
and pandoc . Then fork the main or dev branch of the panda-docs repository following GitHub HowTo .
6.2.2 2. Repository structure
You can see the following structure in the repository.
/panda-docs
/docs
/build
make.bat
Makefile
/source
conf.py
index.rst
/images
/_static
/section_name
(continues on next page)
104
Chapter 6. Developer Guide
PanDAWMS
(continued from previous page)
section_name.rst
subsection_name.rst
...
You edit or add RST files under the source directory or sub-directories in the source directory. index.rst is the RST file
for the main page, and conf.py controls how sphinx builds documents. There is a sub-directory for each section in the
source directory. Each section is composed of one main RST file with the section name plus .rst extension and other
RST files for subsections.
6.2.3 3. Build documents
Once you edit some RST files you need to build documents.
$ cd panda-docs/docs
$ make html
This will build html documents in the build directory. You can check how documents look like by opening panda-
docs/docs/build/html/index.html via your web browser.
6.2.4 4. Submitting pull requests
Once you are comfortable with the changes, you should push them to your forked repository and submit a pull request
following the github doc . Then requests are reviewed, and the changes will be merged to the main branch once approved.
6.2.5 5. Publish the latest documents
Actions on the git repository such as push and tagging trigger automatic-builds on the Read The Docs. The latest (main)
and dev documents show up at https://panda-wms.readthedocs.io/en/latest/ and https://panda-wms.readthedocs.io/en/
dev/ respectively.
6.3 Adding Changes to Documentation
It is enough to submit pull requests directly from GitHub to add minor changes to existing documentation pages.
6.3.1 1. Go to the corresponding code page
First, you need to go to the code page in the GitHub repository corresponding to the documentation you want to edit.
Generally, https://panda-wms.readthedocs.io/en/latest/X/Y.html corresponds to X/Y.rst in the repository.
6.3. Adding Changes to Documentation
105
PanDAWMS
6.3.2 2. Edit the code page
You can directly edit the page by clicking the pencil icon as shown below.
Preview the page before submitting a pull request.
6.3.3 3. Submit a pull request
If everything looks good, go to the bottom of the code page and choose a radio button to create a new branch for this
commit, and start a pull request, as shown in the picture below.
You need to write the reason for the changes in the commit title field and use an arbitrary string as the branch name.
Then click the Propose changes button. Then the request is reviewed, and the changes will be merged to the main
branch once approved.
6.3.4 4. Publish the latest documents
The merge action triggers automatic-builds on the Read The Docs and the latest documents show up at https:
//panda-wms.readthedocs.io/en/latest/ .
106
Chapter 6. Developer Guide
PanDAWMS
6.4 Code Repositories
PanDA software are available in the following GitHub repositories:
• PanDA server
• JEDI
• PanDA Pilot
• Harvester
• PanDA client
• Common modules
6.4. Code Repositories
107
PanDAWMS
108
Chapter 6. Developer Guide
CHAPTER
SEVEN
SYSTEM ARCHITECTURE
The PanDA system is composed of the following components.
7.1 JEDI
JEDI (Job Execution and Definition Interface) is a high-level engine to tailor workload for optimal usage of hetero-
geneous resources dynamically. It processes tasks and generates jobs for PanDA server. The main functions are as
follows:
• To receive and parse task specifications, which clients submit through the RESTful interface of the PanDA server.
• To collect information about task input data.
• To decide the destination for each task output data.
• To choose the computing resources based on the characteristics and requirements of each task.
• To generate and assign jobs to computing resources by taking global shares into account.
• To reassign jobs if workload distribution becomes unbalanced among computing resources.
• To take actions on tasks according to various timeout configurations or user commands.
• To finalize tasks once their input data are done.
JEDI is composed of a master process, stateless agents running on multiple threads/processes, and a fine-grained ex-
clusive lock mechanism. Those agents run independently and don’t directly communicate with each other. They take
objects from the database, take actions on those objects, and update the database. Each agent is designed around a
plugin structure with the core and experiment/activity-specific plugins.
The exclusive lock mechanism allows operations to be distributed across threads, processes and machines, so that JEDI
horizontally scales with multiple machines. For example, while one agent process is working on a particular task,
the task is locked and other agent processes are prevented from updating the task. This is typically useful to avoid
inconsistent modifications caused by concurrently running processes.
The figure above shows the architecture of JEDI. The details of the master process, agents, their functions, and essential
internal objects are explained in the following sections.
• JEDI Master
• Agents
– Task Refiner
– Contents Feeder
– Task Broker
109
PanDAWMS
– Job Generator
– Post Processor
– Watch Dog
– Task Commando
– Message Processor
• Internal objects
– Task
– Dataset
– File
– Event
110
Chapter 7. System Architecture
PanDAWMS
7.1.1 JEDI Master
JEDI Master is the main process of JEDI to launch other agents according to modConfig and procConfig specified
in the configuration file. There is only one JEDI Master process on each machine, and all agents independently run
as child processes of the JEDI Master. There are two connection pools in JEDI Master, one for connections to the
database backend and another for connections to the data management system. Agents share connections in those pools.
The number of connections to the database or data management system is limited even if their accesses are quite busy
so that those external services are protected. When JEDI Master gets started, it
• initializes connection pools first
• launches agents
• provides access to the connection pools to the agents
• waits an eventual SIGTERM or SIGKILL signal when JEDI is stopped
• kills all agents and itself
7.1.2 Agents
Task Refiner
Clients specify tasks in json dictionaries and feed them into the database through the RESTful interface of the PanDA
server. Task Refiner parses them to instantiate JediTaskSpec objects. Each JediTaskSpec object represents
a task. The core code defines common attributes of JediTaskSpec, while plugins set experiment/activity specific
attributes. One of the crucial attributes is splitRule concatenating two-letter codes to specify various characteris-
tics and requirements of the task. JediTaskSpec objects are inserted into the database once they are successfully
instantiated.
Contents Feeder
The Contents Feeder retrieves the contents of input data, such as a list of data filenames, from the external data
management service and records them to the database for subsequent processing by other agents. If the input data
is not a collection of data files, e.g. a list of random seeds, Contents Feeder records a list of pseudo files in the
database.
Task Broker
When tasks specify to aggregate their output, but the final destination is undefined, then the Task Broker will decide
the final destinations of the task output. It is skipped otherwise. The final destination is chosen for each site by
considering its input data locality, free disk spaces and downtime of storage resources, transfer backlog over the network,
and requirements on data processing.
7.1. JEDI
111
PanDAWMS
Job Generator
The Job Generator is the busiest agent in JEDI. It chooses the computing resources for each task, generates jobs, and
submits them to the PanDA server. Job Generator is composed of Job Throttler, Job Broker, Job Splitter,
Task Setupper, and the job submission code. It is highly parallelized since the performance of Job Generator
directly affects the throughput of the whole system. It must scale well since, for example, a single task can generate
millions of jobs.
The entire task pool is first partitioned by global share and resource requirements such as the number of cores and
memory size. Each Job Generator agent takes one partition in a single processing cycle. Job Throttler runs in
the agent and checks whether there are enough jobs running or queued on computing resources for the partition. If not,
the agent spawns multiple threads. The Job Broker running on each thread takes one task in the partition based on its
priority and selects appropriate computing resources. The selection algorithm takes into consideration multiple factors
such as
• data locality
• requirements for data processing and transfers
• constraints and downtime of computing resources
• and transfer backlog over the network
If one or more computing resources are available, Job Broker passes the task to Job Splitter which generates jobs
to respect task requirements and various constraints of computing resources. Finally, the job submission code submits
those jobs to the PanDA server after Task Setupper prepares output data collections. Then Job Broker takes the
next task. Once enough tasks are processed in the partition, the threads are terminated and the Job Generator agent
takes another partition.
Post Processor
Once tasks process all their input data, they are passed to the Post Processor to be finalized. The post-processing step
includes various procedures like validation, cleanup, duplication removal of output data, dispatch of email notifications
to task owners, trigger processing of child tasks, etc.
Watch Dog
Watch Dog periodically takes actions throughout the task lifetime.
Task Commando
Users send the following commands to JEDI through the RESTful interface of the PanDA server. Task Commando
takes actions based on those commands.
• kill To kill a task. All running jobs of the task are killed.
• finish To finish a task. There are two modes of this command. The soft finish command disables to generate
new jobs for the task and waits until all running jobs are done, while the hard finish command kills all jobs
and finishes the task immediately.
• retry To retry a task. The task will process only input data that were unsuccessful in the previous attempt.
Hopeless tasks such as broken and failed tasks reject the retry command since there is no reason to retry.
• incexec To retry a task with new task parameters after looking up the input data. This is typically useful when
new data are appended to the input data and require changes in some task parameters.
112
Chapter 7. System Architecture
PanDAWMS
• pause To pause processing of a task. This command disables generating new jobs for the task and pauses queued
jobs.
• resume To resume a paused task. This command enables to generate new jobs for the task and releases paused
jobs.
• avalanche To skip the scouting state for a task. This command changes the task status to running and triggers
generation of remaining jobs for the task.
Message Processor
Message Processor consumes messages sent from various external components through ActiveMQ. Using Message
Processor describes it in detail.
7.1.3 Internal objects
Task
JediTaskSpec represents a task. The status transition chart and explanations of task statuses are available in the Task
section.
Dataset
JediDatasetSpec represents a data collection, which is called a dataset. The status transition charts of input and
output datasets are shown below.
Each dataset status is described as follows:
Input dataset
defined the dataset information is inserted into the database.
toupdate the dataset information needs to be updated.
pending the dataset is temporally unavailable.
broken the dataset is permanently unavailable.
ready the dataset is ready to be used.
done all files in the dataset were processed.
7.1. JEDI
113
PanDAWMS
114
Chapter 7. System Architecture
PanDAWMS
Output dataset
defined the dataset information is inserted into the database.
ready the dataset is ready for the main processing.
running files are being added to the dataset,
prepared the dataset is ready for post-processing.
done the final status.
There are six types of datasets; input, output, log, lib, tmpl_output, and tmpl_log. Log datasets contain log files
produced by jobs. Lib datasets contain auxiliary input files for jobs such as sandbox files that are not data. Tmpl_output
and tmpl_log datasets are pseudo template datasets to instantiate intermediate datasets where premerged output data
files and log files are added to get merged later. Those pseudo datasets are used only when tasks are specified to use
the internal merge capability.
File
JediFileSpec represents a file. A dataset is internally represented as a collection of files. Generally, files are physical
data files, but if tasks take other entities as input, such as collections of random seeds, they are also represented as
‘pseudo’ files. Files can be retied until they are successfully processed. JEDI makes a new replica of the file object for
each attempt and passes it to the PanDA server, i.e., file objects in JEDI are master copies of file objects in the PanDA
server,
The status transition charts of input and output files are shown below.
Each file status is described as follows:
7.1. JEDI
115
PanDAWMS
Input file
ready the file information is correctly retrieved from DDM and is inserted into the JEDI_Dataset_Contents table
missing the file is missing in the cloud/site where the corresponding task is assigned
lost the file was available in the previous lookup but is now unavailable
broken the file is corrupted
picked the file is picked up to generate jobs
running one or more jobs are using the file
finished the file was successfully used
failed the file was tried multiple times but not succeeded
partial the file was split at the event-level, and some of the event chunks were successfully finished
Output file
defined the file information is inserted into the JEDI_Dataset_Contents table
running the file is being produced
prepared the file is produced
merging the file is being merged
finished the file was successfully processed
failed the file was not produced or failed to be merged
Event
JEDI has the capability to keep track of processing at the sub-file level. A file is internally represented as a collection
of events. JediEventSpec represents an event that is the finest processing granularity.
The status transition chart of the event and each event status are shown below.
ready ready to be processed
sent sent to the pilot
running being processed on a worker node
finished successfully processed, and the corresponding job is still running
cancelled the job was killed before the even range was successfully processed
discarded the job was killed in the merging state after the event range had finished
done successfully processed and waiting to be merged. The corresponding job went to final job status.
failed failed to be processed
fatal failed with a fatal error or attempt number reached the max
merged the related ES merge job successfully finished
corrupted the event is flagged as corrupted to be re-processed since the corresponding zip file is problematic
116
Chapter 7. System Architecture
PanDAWMS
7.1. JEDI
117
PanDAWMS
7.2 PanDA server
The PanDA server is the central hub of the system. It consists of Apache-based RESTful Web servers and time-based
process schedulers, running on the database. It takes care of jobs throughout their lifetime. The main functions are as
follows:
• To receive jobs from JEDI and other job sources that directly generate jobs mainly for testing purposes.
• To prepare job input data once input data are ready.
• To dispatch jobs to worker nodes.
• To watch jobs while they are running on worker nodes.
• To post-process job output data once jobs are done on worker nodes.
• To take actions on jobs according to various timeout configurations or user’s commands.
• To report job updates to JEDI if those jobs were generated by JEDI.
The PanDA server horizontally scales by adding machines since Web servers are stateless and time-based processes
are fine-grained.
The figure above shows the architecture of the PanDA server on a single machine. PanDA Web applications are embed-
ded in WSGI daemons running behind an Apache HTTP server. The master Apache process spawns WSGI daemons
via mod_wsgi in addition to Apache MPM workers. The number of WSGI daemons is static, while the number of
MPM workers dynamically changes depending on the load to optimize resource usages on the machine. MPM workers
receive requests from actors such as users and the pilot. The requests are passed to PanDA Web applications through
an internal request queue and WSGI daemons. There are two types of requests:
118
Chapter 7. System Architecture
PanDAWMS
• Synchronous requests: Actors are blocked for a while and receive responses when PanDA Web applications
complete processing the requests.
• Asynchronous requests: Actors immediately receive a response and the requests are asynchronously processed.
This is typically done when the requests invoke heavy procedures like access to external services. This mode
avoids the HTTP server from being clogged.
When the entire PanDA server is composed of M machines receiving requests at rate R Hz and each PanDA server
machine runs W PanDA Web applications, the average processing time of the request is A sec, the following formula
must be satisfied:
𝐴× 𝑅< 𝑀× 𝑊
Otherwise the HTTP server will be overloaded and requests will be terminated due to timeout errors.
The time-based process scheduler, so-called PanDA daemon, is a daemon to launch various scripts periodically. Its
functionalities are very similar to the standard cron daemon, but it has the following advantages:
• No need to maintain an extra crontab config file.
• On each machine, the same script runs sequentially, i.e., only one process for each script, which is especially
helpful when the script may run longer than the period configured. No new process will spawn until the existing
one finishes, while the cron daemon blindly launches processes so that one has to fine-tune the frequency or let
the script itself kill old processes to avoid duplicated execution.
• There is an exclusive control mechanism to prevent multiple machines from running the same script in parallel.
If this is enabled for a script, only one machine can run the script at a time, which is useful for long-running
scripts that can run on any machine.
• Better system resource usages, e.g., limited total processes n_proc to run scripts, reduction of the overhead to
launch processes, and sharing of database connections among scripts to avoid making a new database connection
in every run.
7.2.1 PanDA Web application
7.2. PanDA server
119
PanDAWMS
The figure above shows the internal architecture of the PanDA Web application to process synchronous requests. The
panda module is the entry point of the PanDA Web application running in the main process and implementing the WSGI
protocol to receive requests through the WSGI daemon. Requests are fed into one of three modules, JobDispatcher,
UserIF, and Utils.
JobDispatcher and UserIF modules provide APIs for the pilot and users, respectively, and requests via those modules
end up with the database access through TaskBuffer, OraDBProxy, and other modules.
On the other hand, the Utils module provides utility APIs which don’t involve database access, such as API for file
uploading.
TaskBuffer and OraDBProxy modules provide high-level and low-level APIs for the database access respectively.
They are executed in separate processes and communicate through the ConBridge module. The ConBridge module
allows the child process, which runs the OraDBProxy module, to get killed due to timeout to avoid deadlock of the
main process. One ConBridge object talks to one OraDBProxy object.
The OraDBProxyPool is a pool of ConBridge objects where the TaskBuffer module picks up one ConBridge object
to call OraDBProxy APIs.
The WrappedCursor module implements Python DB-API 2.0 interface to allow uniform access to various database
backends and establishes one connection to the database. If OraDBProxyPool is configured to have C ConBridge
objects, the total number of database connections in all PanDA Web applications is statically
𝑀× 𝑊× 𝐶
When a group of jobs is submitted through UserIF, the PanDA Web application spawns another type of child process
running the Setupper module. In this case, Setupper will prepare their input data, such as data registration, trigger
the data distribution and so on. Those preparation procedures are experiment-dependent so that the Setupper has a
plugin structure to load an experiment-specific plugin.
7.2.2 PanDA daemon
PanDA daemon launches the following scripts. The execution frequency can be configured for each script.
add_main A script to post-process jobs’ output data after those jobs received the final heartbeat from the pilot.
add_sub A script to run high-frequency procedures.
configurator A script to fetch information about compute, storage, and network resources from the experiment’s in-
formation service.
copyArchive A script to take actions on jobs based on various timeout configurations.
datasetManager A script to take actions on input and output data of jobs.
panda_activeusers_query A script to cache user’s credentials.
tmpwatch A script to clean-up temporary files.
120
Chapter 7. System Architecture
PanDAWMS
7.2.3 Other PanDA modules
Other modules are mainly used to process asynchronous requests.
• The Activator module changes job status to activated when input data of the job is ready.
• The Adder module is the core for add_main to post-process jobs’ output data, such as data registration,
trigger data aggregation and so on. Those post-processing procedures are experiment-dependent so that
the Adder also has a plugin structure to load an experiment-specific plugin.
• The Watcher module checks whether jobs are getting heartbeats and kills them due to lost-heartbeat errors
if not.
• The Closer module works on collections of output data once jobs are done on worker nodes.
• The Finisher module finalizes jobs.
Roughly speaking, jobs go through UserIF Setupper Activator JobDispatcher ( Watcher) Adder Closer
Finisher. Note that they don’t always pass on-memory job objects directly to subsequent modules. For example,
the Setupper module leaves job objects in the database, and then the Activator module retrieves the job objects
from the database when launched in another process.
7.3 Pilot
The Pilot is component based, with each component being responsible for different tasks. The main tasks are handled
by controller components, such as Job Control, Payload Control and Data Control. There is also a set of components
with auxiliary functionalities, e.g. Pilot Monitor and Job Monitor - one for internal use which monitors threads and one
that is tied to the job and checks parameters that are relevant for the payload (e.g. size checks). The Information System
component presents an interface to a database containing knowledge about the resource where the Pilot is running (e.g.
which copy tool to use and where to read and write data).
The Pilot architecture is described on an external wiki page in details.
7.3.1 Getting started
Launch pilot 2
The pilot is a dependency-less Python application and relies on /usr/bin/env python. The minimum pilot can be
called like:
./pilot.py -d -q <QUEUE_NAME>
The QUEUE_NAME correspond to the ATLAS PandaQueue as defined in AGIS. This will launch the default generic
workflow with a default lifetime of 10 seconds.
The -d argument changes the logger to produce debug output.
The current range of implemented pilot options is:
-a: Pilot work directory (string; full path). This is the main work directory for the pilot. In this directory, the work direc-
tory of the payload will be created (./PanDA_Pilot2_%d_%s" % (os.getpid(), str(int(time.time())))).
7.3. Pilot
121
PanDAWMS
-d: Enable debug mode for logging messages. No value should be specified.
-w: Desired workflow (string). Default is generic, which currently means stage-in, payload execution and stage-
out will be performed. Other workflows to be defined. The workflow name should match an existing module in the
workflow Pilot 2 directory.
-l: Lifetime in seconds (integer). Default during Pilot 2 testing and implementation stage is currently 3600 s. It will
be increased at a later time.
-q: PanDA queue name (string). E.g. AGLT2_TEST-condor.
-s: PanDA site name (string). E.g. AGLT2_TEST. Note: the site name is only necessary for the dispatcher. The pilot
will send it to the dispatcher with the getJob command.
-j: Job label (string). A prod/source label which currently has default value ptest. For production jobs, set this to
managed while user is the value for user jobs. Setting it to test will result in a test job.
--cacert: CA certificate to use with HTTPS calls to server, commonly X509 proxy (string). Not needed on the grid.
--capath: CA certificates path (string). Not needed on the grid.
--url: PanDA server URL (string). Default is https://pandaserver.cern.ch.
-p: PanDA server port (integer). Default is 25443.
--config: Config file path (string). Path to pilot_config.cfg file.
--country_group: Country group option for getjob request (string).
--working_group: Working group option for getjob request (string).
--allow_other_country: Is the resource allowed to be used outside the privileged group (boolean)?
--allow_same_user: Multi-jobs will only come from same taskID and thus same user (boolean).
--pilot_user: Pilot user (string). E.g. name of experiment.
7.3.2 Components
api components
analytics
class pilot.api.analytics.Analytics(**kwargs)
Analytics service class.
__init__(**kwargs)
Init function.
Parameters kwargs –
__module__ = 'pilot.api.analytics'
_fit = None
chi2()
Return the chi2 of the fit.
Raises NotDefined – exception thrown if fit is not defined.
Returns chi2 (float).
extract_from_table(table, x_name, y_name)
122
Chapter 7. System Architecture
PanDAWMS
Parameters
• table – dictionary with columns.
• x_name – column name to be extracted (string).
• y_name – column name to be extracted (may contain ‘+’-sign) (string).
Returns x (list), y (list).
fit(x, y, model='linear')
Fitting function. For a linear model: y(x) = slope * x + intersect
Parameters
• x – list of input data (list of floats or ints).
• y – list of input data (list of floats or ints).
• model – model name (string).
Raises UnknownException – in case Fit() fails.
Returns
get_fitted_data(filename, x_name='Time', y_name='pss+swap', precision=2, tails=True)
Return a properly formatted job metrics string with analytics data. Currently the function returns a fit for
PSS+Swap vs time, whose slope measures memory leaks.
Parameters
• filename – full path to memory monitor output (string).
• x_name – optional string, name selector for table column.
• y_name – optional string, name selector for table column.
• precision – optional precision for fitted slope parameter, default 2.
• tails – should tails (first and last values) be used? (boolean).
Returns {“slope”: slope, “chi2”: chi2} (float strings with desired precision).
get_table(filename, header=None, separator='\t', convert_to_float=True)
Parameters
• filename – full path to input file (string).
• header – header string.
• separator – separator character (char).
• convert_to_float – boolean, if True, all values will be converted to floats.
Returns dictionary.
intersect()
Return the intersect of a linear fit, y(x) = slope * x + intersect.
Raises NotDefined – exception thrown if fit is not defined.
Returns intersect (float).
slope()
Return the slope of a linear fit, y(x) = slope * x + intersect.
Raises NotDefined – exception thrown if fit is not defined.
7.3. Pilot
123
PanDAWMS
Returns slope (float).
class pilot.api.analytics.Fit(**kwargs)
Low-level fitting class.
__dict__ = mappingproxy({'__module__': 'pilot.api.analytics', '__doc__': '\n
Low-level fitting class.\n ', '_model': 'linear', '_x': None, '_y': None, '_xm':
None, '_ym': None, '_ss': None, '_ss2': None, '_slope': None, '_intersect': None,
'_chi2': None, '__init__': <function Fit.__init__>, 'fit': <function Fit.fit>,
'value': <function Fit.value>, 'set_chi2': <function Fit.set_chi2>, 'chi2':
<function Fit.chi2>, 'set_slope': <function Fit.set_slope>, 'slope': <function
Fit.slope>, 'set_intersect': <function Fit.set_intersect>, 'intersect': <function
Fit.intersect>, '__dict__': <attribute '__dict__' of 'Fit' objects>, '__weakref__':
<attribute '__weakref__' of 'Fit' objects>, '__annotations__': {}})
__init__(**kwargs)
Init function.
Parameters kwargs –
Raises PilotException – NotImplementedError for unknown fitting model, NotDefined if in-
put data not defined.
__module__ = 'pilot.api.analytics'
__weakref__
list of weak references to the object (if defined)
_chi2 = None
_intersect = None
_model = 'linear'
_slope = None
_ss = None
_ss2 = None
_x = None
_xm = None
_y = None
_ym = None
chi2()
Return the chi2 value.
Returns chi2 (float).
fit()
Return fitting object.
Returns fitting object.
intersect()
Return the intersect value.
Returns intersect (float).
set_chi2()
Calculate and set the chi2 value.
124
Chapter 7. System Architecture
PanDAWMS
Returns
set_intersect()
Calculate and set the intersect of the linear fit.
Returns
set_slope()
Calculate and set the slope of the linear fit.
Returns
slope()
Return the slope value.
Returns slope (float).
value(t)
Return the value y(x=t) of a linear fit y(x) = slope * x + intersect.
Returns intersect (float).
benchmark
class pilot.api.benchmark.Benchmark(*args)
Benchmark service class.
__init__(*args)
Init function.
Parameters args –
__module__ = 'pilot.api.benchmark'
data
class pilot.api.data.StageInClient(infosys_instance=None, acopytools=None, logger=None,
default_copytools='rucio', trace_report=None)
__module__ = 'pilot.api.data'
check_availablespace(files)
Verify that enough local space is available to stage in and run the job
Parameters files – list of FileSpec objects.
Raise PilotException in case of not enough space or total input size too large
get_direct_access_variables(job)
Return the direct access settings for the PQ.
Parameters job – job object.
Returns allow_direct_access (bool), direct_access_type (string).
mode = 'stage-in'
7.3. Pilot
125
PanDAWMS
resolve_replica(fspec, primary_schemas=None, allowed_schemas=None, domain=None)
Resolve input replica (matched by domain if need) first according to primary_schemas, if not found
then look up within allowed_schemas Primary schemas ignore replica priority (used to resolve direct ac-
cess replica, which could be not with top priority set) :param fspec: input FileSpec objects :param al-
lowed_schemas: list of allowed schemas or any if None :return: dict(surl, ddmendpoint, pfn, domain) or
None if replica not found
set_status_for_direct_access(files, workdir)
Update the FileSpec status with ‘remote_io’ for direct access mode. Should be called only once since the
function sends traces
Parameters
• files – list of FileSpec objects.
• workdir – work directory (string).
Returns None
transfer_files(copytool, files, activity=None, **kwargs)
Automatically stage in files using the selected copy tool module.
Parameters
• copytool – copytool module
• files – list of FileSpec objects
• kwargs – extra kwargs to be passed to copytool transfer handler
Returns list of processed FileSpec objects
Raise PilotException in case of controlled error
class pilot.api.data.StageOutClient(infosys_instance=None, acopytools=None, logger=None,
default_copytools='rucio', trace_report=None)
__module__ = 'pilot.api.data'
classmethod get_path(scope, lfn, prefix='rucio')
Construct a partial Rucio PFN using the scope and the LFN
mode = 'stage-out'
prepare_destinations(files, activities)
Resolve destination RSE (filespec.ddmendpoint) for each entry from files according to requested activities
Apply Pilot-side logic to choose proper destination :param files: list of FileSpec objects to be processed
:param activities: ordered list of activities to be used to resolve astorages :return: updated fspec entries
resolve_surl(fspec, protocol, ddmconf, **kwargs)
Get final destination SURL for file to be transferred Can be customized at the level of specific copytool
:param protocol: suggested protocol :param ddmconf: full ddmconf data :param activity: ordered list of
preferred activity names to resolve SE protocols :return: dict with keys (‘pfn’, ‘ddmendpoint’)
transfer_files(copytool, files, activity, **kwargs)
Automatically stage out files using the selected copy tool module.
Parameters
• copytool – copytool module
• files – list of FileSpec objects
• activity – ordered list of preferred activity names to resolve SE protocols
126
Chapter 7. System Architecture
PanDAWMS
• kwargs – extra kwargs to be passed to copytool transfer handler
Returns the output of the copytool transfer operation
Raise PilotException in case of controlled error
class pilot.api.data.StagingClient(infosys_instance=None, acopytools=None, logger=None,
default_copytools='rucio', trace_report=None)
Base Staging Client
__dict__ = mappingproxy({'__module__': 'pilot.api.data', '__doc__': '\n Base Staging
Client\n ', 'mode': '', 'copytool_modules': {'rucio': {'module_name': 'rucio'},
'gfal': {'module_name': 'gfal'}, 'gfalcopy': {'module_name': 'gfal'}, 'xrdcp':
{'module_name': 'xrdcp'}, 'mv': {'module_name': 'mv'}, 'objectstore':
{'module_name': 'objectstore'}, 's3': {'module_name': 's3'}, 'gs': {'module_name':
'gs'}, 'lsm': {'module_name': 'lsm'}}, 'direct_remoteinput_allowed_schemas':
['root', 'https'], 'direct_localinput_allowed_schemas': ['root', 'dcache', 'dcap',
'file', 'https', 'davs'], 'remoteinput_allowed_schemas': ['root', 'gsiftp', 'dcap',
'davs', 'srm', 'storm', 'https'], '__init__': <function StagingClient.__init__>,
'set_acopytools': <function StagingClient.set_acopytools>, 'get_default_copytools':
<staticmethod object>, 'get_preferred_replica': <classmethod object>,
'prepare_sources': <function StagingClient.prepare_sources>, 'prepare_inputddms':
<function StagingClient.prepare_inputddms>, 'sort_replicas': <classmethod object>,
'resolve_replicas': <function StagingClient.resolve_replicas>, 'add_replicas':
<function StagingClient.add_replicas>, 'detect_client_location': <classmethod
object>, 'transfer_files': <function StagingClient.transfer_files>, 'transfer':
<function StagingClient.transfer>, 'require_protocols': <function
StagingClient.require_protocols>, 'resolve_protocols': <function
StagingClient.resolve_protocols>, 'resolve_protocol': <classmethod object>,
'__dict__': <attribute '__dict__' of 'StagingClient' objects>, '__weakref__':
<attribute '__weakref__' of 'StagingClient' objects>, '__annotations__': {}})
__init__(infosys_instance=None, acopytools=None, logger=None, default_copytools='rucio',
trace_report=None)
If acopytools is not specified then it will be automatically resolved via infosys. In this case infosys requires
initialization. :param acopytools: dict of copytool names per activity to be used for transfers. Accepts
also list of names or string value without activity passed. :param logger: logging.Logger object to use
for logging (None means no logging) :param default_copytools: copytool name(s) to be used in case of
unknown activity passed. Accepts either list of names or single string value.
__module__ = 'pilot.api.data'
__weakref__
list of weak references to the object (if defined)
add_replicas(fdat, replica)
Add the replicas to the fdat structure.
Parameters
• fdat –
• replica –
Returns updated fdat.
copytool_modules = {'gfal': {'module_name': 'gfal'}, 'gfalcopy': {'module_name':
'gfal'}, 'gs': {'module_name': 'gs'}, 'lsm': {'module_name': 'lsm'}, 'mv':
{'module_name': 'mv'}, 'objectstore': {'module_name': 'objectstore'}, 'rucio':
{'module_name': 'rucio'}, 's3': {'module_name': 's3'}, 'xrdcp': {'module_name':
'xrdcp'}}
7.3. Pilot
127
PanDAWMS
classmethod detect_client_location()
Open a UDP socket to a machine on the internet, to get the local IPv4 and IPv6 addresses of the request-
ing client. Try to determine the sitename automatically from common environment variables, in this or-
der: SITE_NAME, ATLAS_SITE_NAME, OSG_SITE_NAME. If none of these exist use the fixed string
‘ROAMING’.
direct_localinput_allowed_schemas = ['root', 'dcache', 'dcap', 'file', 'https',
'davs']
direct_remoteinput_allowed_schemas = ['root', 'https']
static get_default_copytools(default_copytools)
Get the default copytools.
Parameters default_copytools –
Returns default copytools (string).
classmethod get_preferred_replica(replicas, allowed_schemas)
Get preferred replica from the replicas list suitable for allowed_schemas :return: first matched replica or
None if not found
mode = ''
prepare_inputddms(files, activities=None)
Populates filespec.inputddms for each entry from files list :param files: list of FileSpec objects :param
activities: sting or ordered list of activities to resolve astorages (optional) :return: None
prepare_sources(files, activities=None)
Customize/prepare source data for each entry in files optionally checking data for requested activities (cus-
tom StageClient could extend the logic if need) :param files: list of FileSpec objects to be processed :param
activities: string or ordered list of activities to resolve astorages (optional) :return: None
remoteinput_allowed_schemas = ['root', 'gsiftp', 'dcap', 'davs', 'srm', 'storm',
'https']
require_protocols(files, copytool, activity, local_dir='')
Populates fspec.protocols and fspec.turl for each entry in files according to preferred fspec.ddm_activity
:param files: list of FileSpec objects :param activity: str or ordered list of transfer activity names to resolve
acopytools related data :return: None
classmethod resolve_protocol(fspec, allowed_schemas=None)
Resolve protocols according to allowed schema :param fspec: FileSpec instance :param allowed_schemas:
list of allowed schemas or any if None :return: list of dict(endpoint, path, flavour)
resolve_protocols(files)
Populates filespec.protocols for each entry from files according to preferred fspec.ddm_activity value :param
files: list of FileSpec objects fdat.protocols = [dict(endpoint, path, flavour), ..] :return: files
resolve_replicas(files, use_vp=False)
Populates filespec.replicas for each entry from files list
fdat.replicas = [{‘ddmendpoint’:’ddmendpoint’, ‘pfn’:’replica’, ‘domain’:’domain value’}]
Parameters
• files – list of FileSpec objects.
• use_vp – True for VP jobs (boolean).
Returns files
128
Chapter 7. System Architecture
PanDAWMS
set_acopytools()
Set the internal acopytools.
Returns
classmethod sort_replicas(replicas, inputddms)
Sort input replicas: consider first affected replicas from inputddms :param replicas: Prioritized list of repli-
cas [(pfn, dat)] :param inputddms: preferred list of ddmebdpoint :return: sorted replicas
transfer(files, activity='default', **kwargs)
Automatically stage passed files using copy tools related to given activity :param files: list of FileSpec
objects :param activity: list of activity names used to determine appropriate copytool (prioritized list)
:param kwargs: extra kwargs to be passed to copytool transfer handler :raise: PilotException in case of
controlled error :return: list of processed FileSpec objects
transfer_files(copytool, files, **kwargs)
Apply transfer of given files using passed copytool module Should be implemented by custom Staging Client
:param copytool: copytool module :param files: list of FileSpec objects :param kwargs: extra kwargs to be
passed to copytool transfer handler :raise: PilotException in case of controlled error
memorymonitor
class pilot.api.memorymonitor.MemoryMonitoring(**kwargs)
Memory monitoring service class.
__init__(**kwargs)
Init function.
Parameters kwargs –
__module__ = 'pilot.api.memorymonitor'
_cmd = ''
execute()
Execute the memory monitor command. Return the process.
Returns process.
get_command()
Return the full command for the memory monitor.
Returns command string.
get_filename()
Returns
get_results()
Returns
pid = 0
user = ''
workdir = ''
7.3. Pilot
129
PanDAWMS
services
class pilot.api.services.Services(*args)
High-level base class for Benchmark(), MemoryMonitoring() and Analytics() classes.
__dict__ = mappingproxy({'__module__': 'pilot.api.services', '__doc__': '\n
High-level base class for Benchmark(), MemoryMonitoring() and Analytics() classes.\n
', '__init__': <function Services.__init__>, '__dict__': <attribute '__dict__' of
'Services' objects>, '__weakref__': <attribute '__weakref__' of 'Services' objects>,
'__annotations__': {}})
__init__(*args)
Init function.
Parameters args –
__module__ = 'pilot.api.services'
__weakref__
list of weak references to the object (if defined)
common components
errorcodes
class pilot.common.errorcodes.ErrorCodes
Pilot error codes.
Note: Error code numbering is the same as in Pilot 1 since that is expected by the PanDA server and monitor.
Note 2: Add error codes as they are needed in other modules. Do not import the full Pilot 1 list at once as there
might very well be codes that can be reassigned/removed.
BADALLOC = 1223
BADMEMORYMONITORJSON = 1337
BADQUEUECONFIGURATION = 1341
BADXML = 1247
BLACKHOLE = 1332
CHKSUMNOTSUP = 1242
CHMODTRF = 1143
COMMUNICATIONFAILURE = 1318
CONVERSIONFAILURE = 1302
COREDUMP = 1355
DBRELEASEFAILURE = 1339
EMPTYOUTPUTFILE = 1350
ESFATAL = 1228
ESNOEVENTS = 1238
ESRECOVERABLE = 1224
EXCEEDEDMAXWAITTIME = 1317
130
Chapter 7. System Architecture
PanDAWMS
EXECUTEDCLONEJOB = 1234
FAILEDBYSERVER = 1236
FILEEXISTS = 1221
FILEHANDLINGFAILURE = 1303
GENERALCPUCALCPROBLEM = 1354
GENERALERROR = 1008
GETADMISMATCH = 1171
GETGLOBUSSYSERR = 1180
GETMD5MISMATCH = 1145
IMAGENOTFOUND = 1360
INTERNALPILOTPROBLEM = 1319
JOBALREADYRUNNING = 1336
JSONRETRIEVALTIMEOUT = 1330
KILLPAYLOAD = 1363
KILLSIGNAL = 1200
LFNTOOLONG = 1190
LOGFILECREATIONFAILURE = 1320
LOOPINGJOB = 1150
MESSAGEHANDLINGFAILURE = 1240
MIDDLEWAREIMPORTFAILURE = 1342
MISSINGCREDENTIALS = 1364
MISSINGINPUTFILE = 1331
MISSINGINSTALLATION = 1211
MISSINGOUTPUTFILE = 1165
MISSINGRELEASEUNPACKED = 1358
MISSINGUSERCODE = 1335
MKDIR = 1199
NFSSQLITE = 1115
NOCTYPES = 1365
NOLOCALSPACE = 1098
NONDETERMINISTICDDM = 1329
NOOUTPUTINJOBREPORT = 1343
NOPAYLOADMETADATA = 1187
NOPROXY = 1163
NORELEASEFOUND = 1244
NOREMOTESPACE = 1333
7.3. Pilot
131
PanDAWMS
NOREPLICAS = 1326
NOSOFTWAREDIR = 1186
NOSTORAGE = 1133
NOSTORAGEPROTOCOL = 1313
NOSUCHFILE = 1103
NOSUCHPROCESS = 1353
NOTDEFINED = 1311
NOTIMPLEMENTED = 1300
NOTSAMELENGTH = 1312
NOUSERTARBALL = 1246
NOVOMSPROXY = 1177
OUTPUTFILETOOLARGE = 1124
PANDAKILL = 1144
PANDAQUEUENOTACTIVE = 1359
PAYLOADEXCEEDMAXMEM = 1235
PAYLOADEXECUTIONEXCEPTION = 1310
PAYLOADEXECUTIONFAILURE = 1305
PAYLOADOUTOFMEMORY = 1212
PAYLOADSIGSEGV = 1328
POSTPROCESSFAILURE = 1357
PREPROCESSFAILURE = 1356
PUTADMISMATCH = 1172
PUTGLOBUSSYSERR = 1181
PUTMD5MISMATCH = 1141
QUEUEDATA = 1116
QUEUEDATANOTOK = 1117
REACHEDMAXTIME = 1213
REMOTEFILECOULDNOTBEOPENED = 1361
REPLICANOTFOUND = 1100
RESOURCEUNAVAILABLE = 1344
RUCIOLISTREPLICASFAILED = 1322
RUCIOLOCATIONFAILED = 1321
RUCIOSERVICEUNAVAILABLE = 1316
SERVICENOTAVAILABLE = 1324
SETUPFAILURE = 1110
SETUPFATAL = 1334
132
Chapter 7. System Architecture
PanDAWMS
SIGBUS = 1206
SIGQUIT = 1202
SIGSEGV = 1203
SIGTERM = 1201
SIGUSR1 = 1207
SIGXCPU = 1204
SINGULARITYBINDPOINTFAILURE = 1308
SINGULARITYFAILEDUSERNAMESPACE = 1345
SINGULARITYGENERALFAILURE = 1306
SINGULARITYIMAGEMOUNTFAILURE = 1309
SINGULARITYNEWUSERNAMESPACE = 1340
SINGULARITYNOLOOPDEVICES = 1307
SINGULARITYNOTINSTALLED = 1325
SINGULARITYRESOURCEUNAVAILABLE = 1348
SIZETOOLARGE = 1168
STAGEINAUTHENTICATIONFAILURE = 1338
STAGEINFAILED = 1099
STAGEINTIMEOUT = 1151
STAGEOUTFAILED = 1137
STAGEOUTTIMEOUT = 1152
STATFILEPROBLEM = 1352
STDOUTTOOBIG = 1106
TRANSFORMNOTFOUND = 1346
TRFDOWNLOADFAILURE = 1149
UNKNOWNCHECKSUMTYPE = 1314
UNKNOWNCOPYTOOL = 1323
UNKNOWNEXCEPTION = 1301
UNKNOWNPAYLOADFAILURE = 1220
UNKNOWNTRFFAILURE = 1315
UNREACHABLENETWORK = 1327
UNRECOGNIZEDTRFARGUMENTS = 1349
UNRECOGNIZEDTRFSTDERR = 1351
UNSUPPORTEDSL5OS = 1347
USERDIRTOOLARGE = 1104
USERKILL = 1205
XRDCPERROR = 1362
7.3. Pilot
133
PanDAWMS
ZEROFILESIZE = 1191
134
Chapter 7. System Architecture
PanDAWMS
__dict__ = mappingproxy({'__module__': 'pilot.common.errorcodes', '__doc__': '\n
Pilot error codes.\n\n Note: Error code numbering is the same as in Pilot 1 since
that is expected by the PanDA server and monitor.\n Note 2: Add error codes as they
are needed in other modules. Do not import the full Pilot 1 list at once as there\n
might very well be codes that can be reassigned/removed.\n ', 'GENERALERROR': 1008,
'NOLOCALSPACE': 1098, 'STAGEINFAILED': 1099, 'REPLICANOTFOUND': 1100, 'NOSUCHFILE':
1103, 'USERDIRTOOLARGE': 1104, 'STDOUTTOOBIG': 1106, 'SETUPFAILURE': 1110,
'NFSSQLITE': 1115, 'QUEUEDATA': 1116, 'QUEUEDATANOTOK': 1117, 'OUTPUTFILETOOLARGE':
1124, 'NOSTORAGE': 1133, 'STAGEOUTFAILED': 1137, 'PUTMD5MISMATCH': 1141, 'CHMODTRF':
1143, 'PANDAKILL': 1144, 'GETMD5MISMATCH': 1145, 'TRFDOWNLOADFAILURE': 1149,
'LOOPINGJOB': 1150, 'STAGEINTIMEOUT': 1151, 'STAGEOUTTIMEOUT': 1152, 'NOPROXY':
1163, 'MISSINGOUTPUTFILE': 1165, 'SIZETOOLARGE': 1168, 'GETADMISMATCH': 1171,
'PUTADMISMATCH': 1172, 'NOVOMSPROXY': 1177, 'GETGLOBUSSYSERR': 1180,
'PUTGLOBUSSYSERR': 1181, 'NOSOFTWAREDIR': 1186, 'NOPAYLOADMETADATA': 1187,
'LFNTOOLONG': 1190, 'ZEROFILESIZE': 1191, 'MKDIR': 1199, 'KILLSIGNAL': 1200,
'SIGTERM': 1201, 'SIGQUIT': 1202, 'SIGSEGV': 1203, 'SIGXCPU': 1204, 'USERKILL':
1205, 'SIGBUS': 1206, 'SIGUSR1': 1207, 'MISSINGINSTALLATION': 1211,
'PAYLOADOUTOFMEMORY': 1212, 'REACHEDMAXTIME': 1213, 'UNKNOWNPAYLOADFAILURE': 1220,
'FILEEXISTS': 1221, 'BADALLOC': 1223, 'ESRECOVERABLE': 1224, 'ESFATAL': 1228,
'EXECUTEDCLONEJOB': 1234, 'PAYLOADEXCEEDMAXMEM': 1235, 'FAILEDBYSERVER': 1236,
'ESNOEVENTS': 1238, 'MESSAGEHANDLINGFAILURE': 1240, 'CHKSUMNOTSUP': 1242,
'NORELEASEFOUND': 1244, 'NOUSERTARBALL': 1246, 'BADXML': 1247, 'NOTIMPLEMENTED':
1300, 'UNKNOWNEXCEPTION': 1301, 'CONVERSIONFAILURE': 1302, 'FILEHANDLINGFAILURE':
1303, 'PAYLOADEXECUTIONFAILURE': 1305, 'SINGULARITYGENERALFAILURE': 1306,
'SINGULARITYNOLOOPDEVICES': 1307, 'SINGULARITYBINDPOINTFAILURE': 1308,
'SINGULARITYIMAGEMOUNTFAILURE': 1309, 'PAYLOADEXECUTIONEXCEPTION': 1310,
'NOTDEFINED': 1311, 'NOTSAMELENGTH': 1312, 'NOSTORAGEPROTOCOL': 1313,
'UNKNOWNCHECKSUMTYPE': 1314, 'UNKNOWNTRFFAILURE': 1315, 'RUCIOSERVICEUNAVAILABLE':
1316, 'EXCEEDEDMAXWAITTIME': 1317, 'COMMUNICATIONFAILURE': 1318,
'INTERNALPILOTPROBLEM': 1319, 'LOGFILECREATIONFAILURE': 1320, 'RUCIOLOCATIONFAILED':
1321, 'RUCIOLISTREPLICASFAILED': 1322, 'UNKNOWNCOPYTOOL': 1323,
'SERVICENOTAVAILABLE': 1324, 'SINGULARITYNOTINSTALLED': 1325, 'NOREPLICAS': 1326,
'UNREACHABLENETWORK': 1327, 'PAYLOADSIGSEGV': 1328, 'NONDETERMINISTICDDM': 1329,
'JSONRETRIEVALTIMEOUT': 1330, 'MISSINGINPUTFILE': 1331, 'BLACKHOLE': 1332,
'NOREMOTESPACE': 1333, 'SETUPFATAL': 1334, 'MISSINGUSERCODE': 1335,
'JOBALREADYRUNNING': 1336, 'BADMEMORYMONITORJSON': 1337,
'STAGEINAUTHENTICATIONFAILURE': 1338, 'DBRELEASEFAILURE': 1339,
'SINGULARITYNEWUSERNAMESPACE': 1340, 'BADQUEUECONFIGURATION': 1341,
'MIDDLEWAREIMPORTFAILURE': 1342, 'NOOUTPUTINJOBREPORT': 1343, 'RESOURCEUNAVAILABLE':
1344, 'SINGULARITYFAILEDUSERNAMESPACE': 1345, 'TRANSFORMNOTFOUND': 1346,
'UNSUPPORTEDSL5OS': 1347, 'SINGULARITYRESOURCEUNAVAILABLE': 1348,
'UNRECOGNIZEDTRFARGUMENTS': 1349, 'EMPTYOUTPUTFILE': 1350, 'UNRECOGNIZEDTRFSTDERR':
1351, 'STATFILEPROBLEM': 1352, 'NOSUCHPROCESS': 1353, 'GENERALCPUCALCPROBLEM': 1354,
'COREDUMP': 1355, 'PREPROCESSFAILURE': 1356, 'POSTPROCESSFAILURE': 1357,
'MISSINGRELEASEUNPACKED': 1358, 'PANDAQUEUENOTACTIVE': 1359, 'IMAGENOTFOUND': 1360,
'REMOTEFILECOULDNOTBEOPENED': 1361, 'XRDCPERROR': 1362, 'KILLPAYLOAD': 1363,
'MISSINGCREDENTIALS': 1364, 'NOCTYPES': 1365, '_error_messages': {1008: 'General
pilot error, consult batch log', 1098: 'Not enough local space', 1099: 'Failed to
stage-in file', 1100: 'Replica not found', 1103: 'No such file or directory', 1104:
'User work directory too large', 1106: 'Payload log or stdout file too big', 1110:
'Failed during payload setup', 1115: 'NFS SQLite locking problems', 1116: 'Pilot
could not download queuedata', 1117: 'Pilot found non-valid queuedata', 1124:
'Output file too large', 1133: 'Fetching default storage failed: no activity related
storage defined', 1137: 'Failed to stage-out file', 1141: 'md5sum mismatch on output
file', 1145: 'md5sum mismatch on input file', 1143: 'Failed to chmod transform',
1144: 'This job was killed by panda server', 1165: 'Local output file is missing',
1168: 'Total file size too large', 1149: 'Transform could not be downloaded', 1150:
'Looping job killed by pilot', 1151: 'File transfer timed out during stage-in',
1152: 'File transfer timed out during stage-out', 1163: 'Grid proxy not valid',
1171: 'adler32 mismatch on input file', 1172: 'adler32 mismatch on output file',
7.3. Pilot
135
PanDAWMS
__module__ = 'pilot.common.errorcodes'
__weakref__
list of weak references to the object (if defined)
136
Chapter 7. System Architecture
PanDAWMS
_error_messages = {1008: 'General pilot error, consult batch log', 1098: 'Not enough
local space', 1099: 'Failed to stage-in file', 1100: 'Replica not found', 1103: 'No
such file or directory', 1104: 'User work directory too large', 1106: 'Payload log
or stdout file too big', 1110: 'Failed during payload setup', 1115: 'NFS SQLite
locking problems', 1116: 'Pilot could not download queuedata', 1117: 'Pilot found
non-valid queuedata', 1124: 'Output file too large', 1133: 'Fetching default storage
failed: no activity related storage defined', 1137: 'Failed to stage-out file',
1141: 'md5sum mismatch on output file', 1143: 'Failed to chmod transform', 1144:
'This job was killed by panda server', 1145: 'md5sum mismatch on input file', 1149:
'Transform could not be downloaded', 1150: 'Looping job killed by pilot', 1151:
'File transfer timed out during stage-in', 1152: 'File transfer timed out during
stage-out', 1163: 'Grid proxy not valid', 1165: 'Local output file is missing',
1168: 'Total file size too large', 1171: 'adler32 mismatch on input file', 1172:
'adler32 mismatch on output file', 1177: 'Voms proxy not valid', 1180: 'Globus
system error during stage-in', 1181: 'Globus system error during stage-out', 1186:
'Software directory does not exist', 1187: 'Payload metadata does not exist', 1190:
'LFN too long (exceeding limit of 255 characters)', 1191: 'File size cannot be
zero', 1199: 'Failed to create local directory', 1200: 'Job terminated by unknown
kill signal', 1201: 'Job killed by signal: SIGTERM', 1202: 'Job killed by signal:
SIGQUIT', 1203: 'Job killed by signal: SIGSEGV', 1204: 'Job killed by signal:
SIGXCPU', 1205: 'Job killed by user', 1206: 'Job killed by signal: SIGBUS', 1207:
'Job killed by signal: SIGUSR1', 1211: 'Missing installation', 1212: 'Payload ran
out of memory', 1213: 'Reached batch system time limit', 1220: 'Job failed due to
unknown reason (consult log file)', 1221: 'File already exists', 1223: 'Transform
failed due to bad_alloc', 1224: 'Event service: recoverable error', 1228: 'Event
service: fatal error', 1234: 'Clone job is already executed', 1235: 'Payload
exceeded maximum allowed memory', 1236: 'Failed by server', 1238: 'Event service: no
events', 1240: 'Failed to handle message from payload', 1242: 'Query checksum is not
supported', 1244: 'No release candidates found', 1246: 'User tarball could not be
downloaded from PanDA server', 1247: 'Badly formed XML', 1300: 'The class or
function is not implemented', 1301: 'An unknown pilot exception has occurred', 1302:
'Failed to convert object data', 1303: 'Failed during file handling', 1305: 'Failed
to execute payload', 1306: 'Singularity: general failure', 1307: 'Singularity: No
more available loop devices', 1308: 'Singularity: Not mounting requested bind
point', 1309: 'Singularity: Failed to mount image', 1310: 'Exception caught during
payload execution', 1311: 'Not defined', 1312: 'Not same length', 1313: 'No protocol
defined for storage endpoint', 1314: 'Unknown checksum type', 1315: 'Unknown
transform failure', 1316: 'Rucio: Service unavailable', 1317: 'Exceeded maximum
waiting time', 1318: 'Failed to communicate with server', 1319: 'An internal Pilot
problem has occurred (consult Pilot log)', 1320: 'Failed during creation of log
file', 1321: 'Failed to get client location for Rucio', 1322: 'Failed to get
replicas from Rucio', 1323: 'Unknown copy tool', 1324: 'Service not available at the
moment', 1325: 'Singularity: not installed', 1326: 'No matching replicas were found
in list_replicas() output', 1327: 'Unable to stage-in file since network is
unreachable', 1328: 'SIGSEGV: Invalid memory reference or a segmentation fault',
1329: 'Failed to construct SURL for non-deterministic ddm (update CRIC)', 1330:
'JSON retrieval timed out', 1331: 'Input file is missing in storage element', 1332:
'Black hole detected in file system (consult Pilot log)', 1333: 'No space left on
device', 1334: 'Setup failed with a fatal exception (consult Payload log)', 1335:
'User code not available on PanDA server (resubmit task with --useNewCode)', 1336:
'Job is already running elsewhere', 1337: 'Memory monitor produced bad output',
1338: 'Authentication failure during stage-in', 1339: 'Local DBRelease handling
failed (consult Pilot log)', 1340: 'Singularity: Failed invoking the NEWUSER
namespace runtime', 1341: 'Bad queue configuration detected', 1342: 'Failed to
import middleware (consult Pilot log)', 1343: 'Found no output in job report', 1344:
'Resource temporarily unavailable', 1345: 'Singularity: Failed to create user
namespace', 1346: 'Transform not found', 1347: 'Unsupported SL5 OS', 1348:
'Singularity: Resource temporarily unavailable', 1349: 'Unrecognized transform
arguments', 1350: 'Empty output file detected', 1351: 'Unrecognized fatal error in
transform stderr', 1352: 'Failed to stat proc file for CPU consumption calculation',
7.3. Pilot
137
PanDAWMS
add_error_code(errorcode, pilot_error_codes=[], pilot_error_diags=[], priority=False, msg=None)
Add pilot error code to list of error codes. This function adds the given error code to the list of all errors
that have occurred. This is needed since several errors can happen; e.g. a stage-in error can be followed by
a stage-out error during the log transfer. The full list of errors is dumped to the log, but only the first error
is reported to the server. The function also sets the corresponding error message.
Parameters
• errorcode – pilot error code (integer)
• pilot_error_codes – list of pilot error codes (list of integers)
• pilot_error_diags – list of pilot error diags (list of strings)
• priority – if set to True, the new errorcode will be added to the error code list first (highest
priority)
• msg – error message (more detailed) to overwrite standard error message (string).
Returns pilot_error_codes, pilot_error_diags
extract_stderr_error(stderr)
Extract the ERROR message from the payload stderr. :param stderr: string. :return: string.
extract_stderr_warning(stderr)
Extract the WARNING message from the payload stderr. :param stderr: string. :return: string.
format_diagnostics(code, diag)
Format the error diagnostics by adding the standard error message and the tail of the longer piloterrordiag.
If there is any kind of failure handling the diagnostics string, the standard error description will be returned.
Parameters
• code – standard error code (int).
• diag – dynamic error diagnostics (string).
Returns formatted error diagnostics (string).
get_error_message(errorcode)
Return the error message corresponding to the given error code.
Parameters errorcode –
Returns errormessage (string)
get_kill_signal_error_code(signal)
Match a kill signal with a corresponding Pilot error code.
Parameters signal – signal name (string).
Returns Pilot error code (integer).
get_message_for_pattern(patterns, stderr)
Parameters
• patterns – list of patterns.
• stderr – string.
Returns string.
classmethod is_recoverable(code=0)
Determine whether code is a recoverable error code or not.
138
Chapter 7. System Architecture
PanDAWMS
Parameters code – Pilot error code (int).
Returns boolean.
put_error_codes = [1135, 1136, 1137, 1141, 1152, 1181]
recoverable_error_codes = [0, 1135, 1136, 1137, 1141, 1152, 1181]
remove_error_code(errorcode, pilot_error_codes=[], pilot_error_diags=[])
Silently remove an error code and its diagnostics from the internal error lists. There is no warning or
exception thrown in case the error code is not present in the lists.
Parameters errorcode – error code (int).
Returns pilot_error_codes, pilot_error_diags
report_errors(pilot_error_codes, pilot_error_diags)
Report all errors that occurred during running. The function should be called towards the end of running a
job.
Parameters
• pilot_error_codes – list of pilot error codes (list of integers)
• pilot_error_diags – list of pilot error diags (list of strings)
Returns error_report (string)
resolve_transform_error(exit_code, stderr)
Assign a pilot error code to a specific transform error. :param exit_code: transform exit code. :param
stderr: transform stderr :return: pilot error code (int)
exception
Exceptions in pilot
exception pilot.common.exception.BadXML(*args, **kwargs)
Badly formed XML.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.CommunicationFailure(*args, **kwargs)
Failed to communicate with servers such as Panda, Harvester, ACT and so on.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.ConversionFailure(*args, **kwargs)
Failed to convert object data.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.ESFatal(*args, **kwargs)
Eventservice fatal exception.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
7.3. Pilot
139
PanDAWMS
exception pilot.common.exception.ESNoEvents(*args, **kwargs)
Eventservice no events exception.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.ESRecoverable(*args, **kwargs)
Eventservice recoverable exception.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
class pilot.common.exception.ExcThread(bucket, target, kwargs, name)
Support class that allows for catching exceptions in threads.
__init__(bucket, target, kwargs, name)
Init function with a bucket that can be used to communicate exceptions to the caller. The bucket is a
Queue.queue() or queue.Queue() object that can hold an exception thrown by a thread.
Parameters
• bucket – queue based bucket.
• target – target function to execute.
• kwargs – target function options.
__module__ = 'pilot.common.exception'
get_bucket()
Return the bucket object that holds any information about thrown exceptions.
Returns bucket (Queue object)
run()
Thread run function. Any exceptions in the threads are caught in this function and placed in the bucket of
the current thread. The bucket will be emptied by the control module that launched the thread. E.g. an
exception is thrown in the retrieve thread (in function retrieve()) that is created by the job.control thread.
The exception is caught by the run() function and placed in the bucket belonging to the retrieve thread. The
bucket is emptied in job.control().
Returns
exception pilot.common.exception.ExceededMaxWaitTime(*args, **kwargs)
Exceeded maximum waiting time (after abort_job has been set).
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.ExecutedCloneJob(*args, **kwargs)
Clone job executed exception.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.FileHandlingFailure(*args, **kwargs)
Failed during file handling.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
140
Chapter 7. System Architecture
PanDAWMS
exception pilot.common.exception.JobAlreadyRunning(*args, **kwargs)
Job is already running elsewhere.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
__str__()
Return str(self).
exception pilot.common.exception.LogFileCreationFailure(*args, **kwargs)
Log file could not be created.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.MKDirFailure(*args, **kwargs)
Failed to create local directory.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.MessageFailure(*args, **kwargs)
Failed to handle messages.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.NoGridProxy(*args, **kwargs)
Grid proxy not valid.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.NoLocalSpace(*args, **kwargs)
Not enough local space.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.NoSoftwareDir(*args, **kwargs)
Software applications directory does not exist.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.NoSuchFile(*args, **kwargs)
No such file or directory.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.NoVomsProxy(*args, **kwargs)
Voms proxy not valid.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.NotDefined(*args, **kwargs)
Not defined exception.
7.3. Pilot
141
PanDAWMS
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.NotSameLength(*args, **kwargs)
Not same length exception.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.PilotException(*args, **kwargs)
The basic exception class. The pilot error code can be defined here, where the pilot error code will be propageted
to job server.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
__str__()
Return str(self).
__weakref__
list of weak references to the object (if defined)
get_detail()
get_error_code()
get_last_error()
exception pilot.common.exception.QueuedataFailure(*args, **kwargs)
Failed to download queuedata.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.QueuedataNotOK(*args, **kwargs)
Corrupt queuedata.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.ReplicasNotFound(*args, **kwargs)
No matching replicas were found in list_replicas() output.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.RunPayloadFailure(*args, **kwargs)
Failed to execute payload.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.SetupFailure(*args, **kwargs)
Failed to setup environment.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.SizeTooLarge(*args, **kwargs)
Too large input files.
142
Chapter 7. System Architecture
PanDAWMS
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.StageInFailure(*args, **kwargs)
Failed to stage-in file.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.StageOutFailure(*args, **kwargs)
Failed to stage-out file.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.TrfDownloadFailure(*args, **kwargs)
Transform could not be downloaded.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
exception pilot.common.exception.UnknownException(*args, **kwargs)
Unknown exception.
__init__(*args, **kwargs)
__module__ = 'pilot.common.exception'
pilot.common.exception.is_python3()
Check if we are running on Python 3.
Returns boolean.
control components
data
pilot.control.data._do_stageout(job, xdata, activity, queue, title, output_dir='')
Use the StageOutClient in the Data API to perform stage-out.
Parameters
• job – job object.
• xdata – list of FileSpec objects.
• activity – copytool activity or preferred list of activities to resolve copytools
• title – type of stage-out (output, log) (string).
• queue – PanDA queue (string).
Returns True in case of success transfers
pilot.control.data._stage_in(args, job)
Returns True in case of success
pilot.control.data._stage_out_new(job, args)
Stage-out of all output files. If job.stageout=log then only log files will be transferred.
7.3. Pilot
143
PanDAWMS
Parameters
• job – job object.
• args – pilot args object.
Returns True in case of success, False otherwise.
pilot.control.data.control(queues, traces, args)
pilot.control.data.copytool_in(queues, traces, args)
Call the stage-in function and put the job object in the proper queue.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.data.copytool_out(queues, traces, args)
Main stage-out thread. Perform stage-out as soon as a job object can be extracted from the data_out queue.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.data.create_log(workdir, logfile_name, tarball_name, cleanup, input_files=[],
output_files=[], is_looping=False, debugmode=False)
Create the tarball for the job.
Parameters
• workdir – work directory for the job (string).
• logfile_name – log file name (string).
• tarball_name – tarball name (string).
• cleanup – perform cleanup (Boolean).
• input_files – list of input files to remove (list).
• output_files – list of output files to remove (list).
• is_looping – True for looping jobs, False by default (Boolean).
• debugmode – True if debug mode has been switched on (Boolean).
Raises LogFileCreationFailure – in case of log file creation problem.
Returns
pilot.control.data.create_trace_report(job, label='stage-in')
Create the trace report object.
Parameters
• job – job object.
• label – ‘stage-[in|out]’ (string).
144
Chapter 7. System Architecture
PanDAWMS
Returns trace report object.
pilot.control.data.filter_files_for_log(directory)
Create a file list recursi :param directory: :return:
pilot.control.data.get_input_file_dictionary(indata)
Return an input file dictionary. Format: {‘guid’: ‘pfn’, ..} Normally use_turl would be set to True if direct access
is used. Note: any environment variables in the turls will be expanded
Parameters indata – list of FileSpec objects.
Returns file dictionary.
pilot.control.data.get_rse(data, lfn='')
Return the ddmEndPoint corresponding to the given lfn. If lfn is not provided, the first ddmEndPoint will be
returned.
Parameters
• data – FileSpec list object.
• lfn – local file name (string).
Returns rse (string)
pilot.control.data.get_trace_report_variables(job, label='stage-in')
Get some of the variables needed for creating the trace report.
Parameters
• job – job object
• label – ‘stage-[in|out]’ (string).
Returns event_type (string), localsite (string), remotesite (string).
pilot.control.data.is_already_processed(queues, processed_jobs)
Skip stage-out in case the job has already been processed. This should not be necessary so this is a fail-safe but
it seems there is a case when a job with multiple output files enters the stage-out more than once.
Parameters
• queues – queues object.
• processed_jobs – list of already processed jobs.
Returns True if stage-out queues contain a job object that has already been processed.
pilot.control.data.queue_monitoring(queues, traces, args)
Monitoring of Data queues.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.data.skip_special_files(job)
Consult user defined code if any files should be skipped during stage-in. ATLAS code will skip DBRelease files
e.g. as they should already be available in CVMFS.
Parameters job – job object.
Returns
7.3. Pilot
145
PanDAWMS
pilot.control.data.stage_in_auto(files)
Separate dummy implementation for automatic stage-in outside of pilot workflows. Should be merged with
regular stage-in functionality later, but we need to have some operational experience with it first. Many things
to improve:
• separate file error handling in the merged case
• auto-merging of files with same destination into single copytool call
pilot.control.data.stage_out_auto(files)
Separate dummy implementation for automatic stage-out outside of pilot workflows. Should be merged with
regular stage-out functionality later, but we need to have some operational experience with it first.
pilot.control.data.update_indata(job)
In case file were marked as no_transfer files, remove them from stage-in.
Parameters job – job object.
Returns
pilot.control.data.write_output(filename, output)
Write command output to file.
Parameters
• filename – file name (string).
• output – command stdout/stderr (string).
Returns
pilot.control.data.write_utility_output(workdir, step, stdout, stderr)
Write the utility command output to stdout, stderr files to the job.workdir for the current step.
->
<step>_stdout.txt, <step>_stderr.txt Example of step: xcache.
Parameters
• workdir – job workdir (string).
• step – utility step (string).
• stdout – command stdout (string).
• stderr – command stderr (string).
Returns
job
pilot.control.job._validate_job(job)
Verify job parameters for specific problems.
Parameters job – job object.
Returns Boolean.
pilot.control.job.add_data_structure_ids(data, version_tag)
Add pilot, batch and scheduler ids to the data structure for getJob, updateJob.
Parameters data – data structure (dict).
Returns updated data structure (dict).
pilot.control.job.add_error_codes(data, job)
Add error codes to data structure.
146
Chapter 7. System Architecture
PanDAWMS
Parameters
• data – data dictionary.
• job – job object.
Returns
pilot.control.job.add_memory_info(data, workdir, name='')
Add memory information (if available) to the data structure that will be sent to the server with job updates Note:
this function updates the data dictionary.
Parameters
• data – data structure (dictionary).
• workdir – working directory of the job (string).
• name – name of memory monitor (string).
Returns
pilot.control.job.add_timing_and_extracts(data, job, state, args)
Add timing info and log extracts to data structure for a completed job (finished or failed) to be sent to server.
Note: this function updates the data dictionary.
Parameters
• data – data structure (dictionary).
• job – job object.
• state – state of the job (string).
• args – pilot args.
Returns
pilot.control.job.check_for_abort_job(args, caller='')
Check if args.abort_job.is_set().
Parameters
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
• caller – function name of caller (string).
Returns Boolean, True if args_job.is_set()
pilot.control.job.check_job_monitor_waiting_time(args, peeking_time, abort_override=False)
Check the waiting time in the job monitor. Set global graceful_stop if necessary.
Parameters
• args – args object.
• peeking_time – time when monitored_payloads queue was peeked into (int).
Returns
pilot.control.job.control(queues, traces, args)
Main function of job control.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
7.3. Pilot
147
PanDAWMS
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.job.create_data_payload(queues, traces, args)
Get a Job object from the “validated_jobs” queue.
If the job has defined input files, move the Job object to the “data_in” queue and put the internal pilot state to
“stagein”. In case there are no input files, place the Job object in the “finished_data_in” queue. For either case,
the thread also places the Job object in the “payloads” queue (another thread will retrieve it and wait for any
stage-in to finish).
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.job.create_job(dispatcher_response, queue)
Create a job object out of the dispatcher response.
Parameters
• dispatcher_response – raw job dictionary from the dispatcher.
• queue – queue name (string).
Returns job object
pilot.control.job.create_k8_link(job_dir)
Create a soft link to the payload workdir on Kubernetes if SHARED_DIR exists.
Parameters job_dir – payload workdir (string).
pilot.control.job.delayed_space_check(queues, traces, args, job)
Run the delayed space check if necessary.
Parameters
• queues – queues object.
• traces – traces object.
• args – args object.
• job – job object.
Returns
pilot.control.job.fail_monitored_job(job, exit_code, diagnostics, queues, traces)
Fail a monitored job.
Parameters
• job – job object
• exit_code – exit code from job_monitor_tasks (int).
• diagnostics – pilot error diagnostics (string).
• queues – queues object.
• traces – traces object.
Returns
148
Chapter 7. System Architecture
PanDAWMS
pilot.control.job.fast_job_monitor(queues, traces, args)
Fast monitoring of job parameters.
This function can be used for monitoring processes below the one minute threshold of the normal job_monitor
thread.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.job.fast_monitor_tasks(job)
Perform user specific fast monitoring tasks.
Parameters job – job object.
Returns exit code (int).
pilot.control.job.get_cpu_consumption_time(cpuconsumptiontime)
Get the CPU consumption time. The function makes sure that the value exists and is within allowed limits (<
10^9).
Parameters cpuconsumptiontime – CPU consumption time (int/None).
Returns properly set CPU consumption time (int/None).
pilot.control.job.get_data_structure(job, state, args, xml=None, metadata=None)
Build the data structure needed for getJob, updateJob.
Parameters
• job – job object.
• state – state of the job (string).
• args –
• xml – optional XML string.
• metadata – job report metadata read as a string.
Returns data structure (dictionary).
pilot.control.job.get_debug_command(cmd)
Identify and filter the given debug command.
Note: only a single command will be allowed from a predefined list: tail, ls, gdb, ps, du.
Parameters cmd – raw debug command from job definition (string).
Returns debug_mode (Boolean, True if command is deemed ok), debug_command (string).
pilot.control.job.get_debug_stdout(job)
Return the requested output from a given debug command.
Parameters job – job object.
Returns output (string).
pilot.control.job.get_dispatcher_dictionary(args)
Return a dictionary with required fields for the dispatcher getJob operation.
7.3. Pilot
149
PanDAWMS
The dictionary should contain the following fields: siteName, computingElement (queue name), prodSourceLa-
bel (e.g. user, test, ptest), diskSpace (available disk space for a job in MB), workingGroup, countryGroup, cpu
(float), mem (float) and node (worker node name).
workingGroup, countryGroup and allowOtherCountry we add a new pilot setting allowOtherCountry=True to be
used in conjunction with countryGroup=us for US pilots. With these settings, the Panda server will produce the
desired behaviour of dedicated X% of the resource exclusively (so long as jobs are available) to countryGroup=us
jobs. When allowOtherCountry=false this maintains the behavior relied on by current users of the countryGroup
mechanism – to NOT allow the resource to be used outside the privileged group under any circumstances.
Parameters args – arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns dictionary prepared for the dispatcher getJob operation.
pilot.control.job.get_fake_job(input=True)
Return a job definition for internal pilot testing. Note: this function is only used for testing purposes. The job
definitions below are ATLAS specific.
Parameters input – Boolean, set to False if no input files are wanted
Returns job definition (dictionary).
pilot.control.job.get_finished_or_failed_job(args, queues)
Check if the job has either finished or failed and if so return it. If failed, order a log transfer. If the job is in state
‘failed’ and abort_job is set, set job_aborted.
Parameters
• args – pilot args object.
• queues – pilot queues object.
Returns job object.
pilot.control.job.get_general_command_stdout(job)
Return the output from the requested debug command.
Parameters job – job object.
Returns output (string).
pilot.control.job.get_heartbeat_period(debug=False)
Return the proper heartbeat period, as determined by normal or debug mode. In normal mode, the heartbeat
period is 30*60 s, while in debug mode it is 5*60 s. Both values are defined in the config file.
Parameters debug – Boolean, True for debug mode. False otherwise.
Returns heartbeat period (int).
pilot.control.job.get_job_definition(args)
Get a job definition from a source (server or pre-placed local file).
Parameters args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns job definition dictionary.
pilot.control.job.get_job_definition_from_file(path, harvester)
Get a job definition from a pre-placed file. In Harvester mode, also remove any existing job request files since it
is no longer needed/wanted.
Parameters
• path – path to job definition file.
• harvester – True if Harvester is being used (determined from args.harvester), otherwise
False
150
Chapter 7. System Architecture
PanDAWMS
Returns job definition dictionary.
pilot.control.job.get_job_definition_from_server(args)
Get a job definition from a server.
Parameters args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns job definition dictionary.
pilot.control.job.get_job_from_queue(queues, state)
Check if the job has finished or failed and if so return it.
Parameters
• queues – pilot queues.
• state – job state (e.g. finished/failed) (string).
Returns job object.
pilot.control.job.get_job_label(args)
Return a proper job label. The function returns a job label that corresponds to the actual pilot version, ie if the
pilot is a development version (ptest or rc_test2) or production version (managed or user). Example: -i RC ->
job_label = rc_test2. NOTE: it should be enough to only use the job label, -j rc_test2 (and not specify -i RC at
all).
Parameters args – pilot args object.
Returns job_label (string).
pilot.control.job.get_job_retrieval_delay(harvester)
Return the proper delay between job retrieval attempts. In Harvester mode, the pilot will look once per second
for a job definition file.
Parameters harvester – True if Harvester is being used (determined from args.harvester), other-
wise False
Returns sleep (s)
pilot.control.job.get_job_status(job, key)
Wrapper function around job.get_status(). If key = ‘LOG_TRANSFER’ but job object is not defined, the function
will return value = LOG_TRANSFER_NOT_DONE.
Parameters
• job – job object.
• key – key name (string).
Returns value (string).
pilot.control.job.get_job_status_from_server(job_id, url, port)
Return the current status of job <jobId> from the dispatcher.
typical dispatcher response:
‘sta-
tus=finished&StatusCode=0’ StatusCode 0: succeeded
10: time-out 20: general error 30: failed
In the case of time-out, the dispatcher will be asked one more time after 10 s.
Parameters
• job_id – PanDA job id (int).
• url – PanDA server URL (string).
• port – PanDA server port (int).
7.3. Pilot
151
PanDAWMS
Returns status (string; e.g. holding), attempt_nr (int), status_code (int)
pilot.control.job.get_latest_log_tail(files)
Get the tail of the latest updated file from the given file list.
Parameters files – files (list).
pilot.control.job.get_ls(debug_command, workdir)
Return the requested ls debug command.
Parameters
• debug_command – full debug command (string).
• workdir – job work directory (string).
Returns output (string).
pilot.control.job.get_panda_server(url, port)
Get the URL for the PanDA server.
Parameters
• url – URL string, if set in pilot option (port not included).
• port – port number, if set in pilot option (int).
Returns full URL (either from pilot options or from config file)
pilot.control.job.get_payload_log_tail(workdir)
Return the tail of the payload stdout or its latest updated log file.
Parameters workdir – job work directory (string).
Returns tail of stdout (string).
pilot.control.job.get_proper_state(job, state)
Return a proper job state to send to server. This function should only return ‘starting’, ‘running’, ‘finished’,
‘holding’ or ‘failed’. If the internal job.serverstate is not yet set, it means it is the first server update, ie ‘starting’
should be sent.
Parameters
• job – job object.
• state – internal pilot state (string).
Returns valid server state (string).
pilot.control.job.get_requested_log_tail(debug_command, workdir)
Return the tail of the requested debug log.
Examples tail workdir/tmp.stdout* <- pilot finds the requested log file in the specified relative path tail
log.RAWtoALL <- pilot finds the requested log file
Parameters
• debug_command – full debug command (string).
• workdir – job work directory (string).
Returns output (string).
pilot.control.job.get_task_id()
Return the task id for the current job. Note: currently the implementation uses an environmental variable to store
this number (PanDA_TaskID).
152
Chapter 7. System Architecture
PanDAWMS
Returns task id (string). Returns empty string in case of error.
pilot.control.job.getjob_server_command(url, port)
Prepare the getJob server command.
Parameters
• url – PanDA server URL (string)
• port – PanDA server port
Returns full server command (URL string)
pilot.control.job.handle_backchannel_command(res, job, args, test_tobekilled=False)
Does the server update contain any backchannel information? if so, update the job object.
Parameters
• res – server response (dictionary).
• job – job object.
• args – pilot args object.
• test_tobekilled – emulate a tobekilled command (boolean).
Returns
pilot.control.job.has_job_completed(queues, args)
Has the current job completed (finished or failed)? Note: the job object was extracted from monitored_payloads
queue before this function was called.
Parameters queues – Pilot queues object.
Returns True is the payload has finished or failed
pilot.control.job.interceptor(queues, traces, args)
MOVE THIS TO INTERCEPTOR.PY; TEMPLATE FOR THREADS
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.job.is_queue_empty(queues, queue)
Check if the given queue is empty (without pulling).
Parameters
• queues – pilot queues object.
• queue – queue name (string).
Returns True if queue is empty, False otherwise
pilot.control.job.job_monitor(queues, traces, args)
Monitoring of job parameters. This function monitors certain job parameters, such as job looping, at various
time intervals. The main loop is executed once a minute, while individual verifications may be executed at any
time interval (>= 1 minute). E.g. looping jobs are checked once per ten minutes (default) and the heartbeat is
send once per 30 minutes. Memory usage is checked once a minute.
Parameters
7.3. Pilot
153
PanDAWMS
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.job.locate_job_definition(args)
Locate the job definition file among standard locations.
Parameters args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns path (string).
pilot.control.job.make_job_report(job)
Make a summary report for the given job. This function is called when the job has completed.
Parameters job – job object.
Returns
pilot.control.job.now()
Return the current epoch as a UTF-8 encoded string. :return: current time as encoded string
pilot.control.job.order_log_transfer(queues, job)
Order a log transfer for a failed job.
Parameters
• queues – pilot queues object.
• job – job object.
Returns
pilot.control.job.pause_queue_monitor(delay)
Pause the queue monitor to let log transfer complete. Note: this function should use globally available object.
Use sleep for now. :param delay: sleep time in seconds (int). :return:
pilot.control.job.print_node_info()
Print information about the local node to the log.
Returns
pilot.control.job.proceed_with_getjob(timefloor, starttime, jobnumber, getjob_requests,
max_getjob_requests, update_server, submitmode, harvester,
verify_proxy, traces)
Can we proceed with getJob? We may not proceed if we have run out of time (timefloor limit), if the proxy is
too short, if disk space is too small or if we have already proceed enough jobs.
Parameters
• timefloor – timefloor limit (s) (int).
• starttime – start time of retrieve() (s) (int).
• jobnumber – number of downloaded jobs (int).
• getjob_requests – number of getjob requests (int).
• update_server – should pilot update server? (Boolean).
• submitmode – Harvester submit mode, PULL or PUSH (string).
• harvester – True if Harvester is used, False otherwise. Affects the max number of getjob
reads (from file) (Boolean).
154
Chapter 7. System Architecture
PanDAWMS
• verify_proxy – True if the proxy should be verified. False otherwise (Boolean).
• traces – traces object (to be able to propagate a proxy error all the way back to the wrapper).
Returns True if pilot should proceed with getJob (Boolean).
pilot.control.job.process_debug_mode(job)
Handle debug mode - preprocess debug command, get the output and kill the payload in case of gdb.
Parameters job – job object.
Returns stdout from debug command (string).
pilot.control.job.publish_harvester_reports(state, args, data, job, final)
Publish all reports needed by Harvester.
Parameters
• state – job state (string).
• args – pilot args object.
• data – data structure for server update (dictionary).
• job – job object.
• final – is this the final update? (Boolean).
Returns True if successful, False otherwise (Boolean).
pilot.control.job.queue_monitor(queues, traces, args)
Monitoring of queues. This function monitors queue activity, specifically if a job has finished or failed and then
reports to the server.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.job.remove_pilot_logs_from_list(list_of_files)
Remove any pilot logs from the list of last updated files.
Parameters list_of_files – list of last updated files (list).
Returns list of files (list).
pilot.control.job.retrieve(queues, traces, args)
Retrieve all jobs from a source.
The job definition is a json dictionary that is either present in the launch directory (preplaced) or downloaded
from a server specified by args.url.
The function retrieves the job definition from the proper source and places it in the queues.jobs queue.
WARNING: this function is nearly too complex. Be careful with adding more lines as flake8 will fail it.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
7.3. Pilot
155
PanDAWMS
Raises PilotException – if create_job fails (e.g. because queuedata could not be downloaded).
Returns
pilot.control.job.send_heartbeat_if_time(job, args, update_time)
Send a heartbeat to the server if it is time to do so.
Parameters
• job – job object.
• args – args object.
• update_time – last update time (from time.time()).
Returns possibly updated update_time (from time.time()).
pilot.control.job.send_state(job, args, state, xml=None, metadata=None, test_tobekilled=False)
Update the server (send heartbeat message). Interpret and handle any server instructions arriving with the up-
dateJob back channel.
Parameters
• job – job object.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
• state – job state (string).
• xml – optional metadata xml (string).
• metadata – job report metadata read as a string.
• test_tobekilled – emulate a tobekilled command (boolean).
Returns boolean (True if successful, False otherwise).
pilot.control.job.store_jobid(jobid, init_dir)
Store the PanDA job id in a file that can be picked up by the wrapper for other reporting.
Parameters
• jobid – job id (int).
• init_dir – pilot init dir (string).
Returns
pilot.control.job.update_server(job, args)
Update the server (wrapper for send_state() that also prepares the metadata).
Parameters
• job – job object.
• args – pilot args object.
Returns
pilot.control.job.validate(queues, traces, args)
Perform validation of job.
Parameters
• queues – queues object.
• traces – traces object.
• args – args object.
156
Chapter 7. System Architecture
PanDAWMS
Returns
pilot.control.job.verify_ctypes(queues, job)
Verify ctypes and make sure all subprocess are parented.
Parameters
• queues – queues object.
• job – job object.
Returns
pilot.control.job.verify_error_code(job)
Make sure an error code is properly set. This makes sure that job.piloterrorcode is always set for a failed/holding
job, that not only job.piloterrorcodes are set but not job.piloterrorcode. This function also negates the sign of the
error code and sets job state ‘holding’ (instead of ‘failed’) if the error is found to be recoverable by a later job
(user jobs only).
Parameters job – job object.
Returns
pilot.control.job.wait_for_aborted_job_stageout(args, queues, job)
Wait for stage-out to finish for aborted job.
Parameters
• args – pilot args object.
• queues – pilot queues object.
• job – job object.
Returns
pilot.control.job.write_heartbeat_to_file(data)
Write heartbeat dictionary to file. This is only done when server updates are not wanted.
Parameters data – server data (dictionary).
Returns True if successful, False otherwise (Boolean).
monitor
pilot.control.monitor.control(queues, traces, args)
Main control function, run from the relevant workflow module.
Parameters
• queues –
• traces –
• args –
Returns
pilot.control.monitor.get_max_running_time(lifetime, queuedata)
Return the maximum allowed running time for the pilot. The max time is set either as a pilot option or via the
schedconfig.maxtime for the PQ in question.
Parameters
• lifetime – optional pilot option time in seconds (int).
7.3. Pilot
157
PanDAWMS
• queuedata – queuedata object
Returns max running time in seconds (int).
pilot.control.monitor.get_process_info(cmd, user=None, args='aufx', pid=None)
Return process info for given command. The function returns a list with format [cpu, mem, command, number
of commands] as returned by ‘ps -u user args’ for a given command (e.g. python pilot2/pilot.py).
Example get_processes_for_command(‘sshd:’)
nilspal 1362 0.0 0.0 183424 2528 ? S 12:39 0:00 sshd: nilspal@pts/28 nilspal 1363 0.0 0.0 136628 2640
pts/28 Ss 12:39 0:00 _ -tcsh nilspal 8603 0.0 0.0 34692 5072 pts/28 S+ 12:44 0:00 _ python monitor.py
nilspal 8604 0.0 0.0 62036 1776 pts/28 R+ 12:44 0:00 _ ps -u nilspal aufx –no-headers
-> [‘0.0’, ‘0.0’, ‘sshd: nilspal@pts/28’, 1]
Parameters
• cmd – command (string).
• user – user (string).
• args – ps arguments (string).
• pid – process id (int).
Returns list with process info (l[0]=cpu usage(%), l[1]=mem usage(%), l[2]=command(string)).
pilot.control.monitor.run_checks(queues, args)
Perform non-job related monitoring checks.
Parameters
• queues –
• args –
Returns
payload
pilot.control.payload._validate_payload(job)
Perform validation tests for the payload.
Parameters job – job object.
Returns boolean.
pilot.control.payload.control(queues, traces, args)
(add description)
Parameters
• queues –
• traces –
• args –
Returns
pilot.control.payload.execute_payloads(queues, traces, args)
Execute queued payloads.
158
Chapter 7. System Architecture
PanDAWMS
Extract a Job object from the “validated_payloads” queue and put it in the “monitored_jobs” queue. The payload
stdout/err streams are opened and the pilot state is changed to “starting”. A payload executor is selected (for
executing a normal job, an event service job or event service merge job). After the payload (or rather its executor)
is started, the thread will wait for it to finish and then check for any failures. A successfully completed job is
placed in the “finished_payloads” queue, and a failed job will be placed in the “failed_payloads” queue.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.payload.failed_post(queues, traces, args)
Get a Job object from the “failed_payloads” queue. Set the pilot state to “stakeout” and the stageout field to
“log”, and add the Job object to the “data_out” queue.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.payload.get_payload_executor(args, job, out, err, traces)
Get payload executor function for different payload.
Parameters
• args – args object.
• job – job object.
• out –
• err –
• traces – traces object.
Returns instance of a payload executor
pilot.control.payload.perform_initial_payload_error_analysis(job, exit_code)
Perform an initial analysis of the payload. Singularity errors are caught here.
Parameters
• job – job object.
• exit_code – exit code from payload execution.
Returns
pilot.control.payload.set_cpu_consumption_time(job)
Set the CPU consumption time. :param job: job object. :return:
pilot.control.payload.set_error_code_from_stderr(msg, fatal)
Identify specific errors in stderr and set the corresponding error code. The function returns 0 if no error is
recognized.
Parameters
• msg – stderr (string).
7.3. Pilot
159
PanDAWMS
• fatal – boolean flag if fatal error among warning messages in stderr.
Returns error code (int).
pilot.control.payload.validate_post(queues, traces, args)
Validate finished payloads. If payload finished correctly, add the job to the data_out queue. If it failed, add it to
the data_out queue as well but only for log stage-out (in failed_post() below).
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
pilot.control.payload.validate_pre(queues, traces, args)
Get a Job object from the “payloads” queue and validate it.
If the payload is successfully validated (user defined), the Job object is placed in the “validated_payloads” queue,
otherwise it is placed in the “failed_payloads” queue.
Parameters
• queues – internal queues for job handling.
• traces – tuple containing internal pilot states.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns
payloads components
eventservice
class pilot.control.payloads.eventservice.Executor(args, job, out, err, traces)
__init__(args, job, out, err, traces)
__module__ = 'pilot.control.payloads.eventservice'
get_executor_type()
Get the executor type. This is usually the ‘generic’ type, which means normal event service. It can also be
‘raythena’ if specified in the Pilot options.
Returns executor type dictionary.
run_payload(job, cmd, out, err)
(add description)
Parameters
• job – job object.
• cmd – (unused in ES mode)
• out – stdout file object.
• err – stderr file object.
Returns
160
Chapter 7. System Architecture
PanDAWMS
wait_graceful(args, proc)
(add description)
Parameters
• args –
• proc –
Returns
generic
class pilot.control.payloads.generic.Executor(args, job, out, err, traces)
__dict__ = mappingproxy({'__module__': 'pilot.control.payloads.generic', '__init__':
<function Executor.__init__>, 'get_job': <function Executor.get_job>, 'pre_setup':
<function Executor.pre_setup>, 'post_setup': <function Executor.post_setup>,
'utility_before_payload': <function Executor.utility_before_payload>,
'utility_with_payload': <function Executor.utility_with_payload>,
'get_utility_command': <function Executor.get_utility_command>,
'utility_after_payload_started': <function Executor.utility_after_payload_started>,
'utility_after_payload_started_new': <function
Executor.utility_after_payload_started_new>, 'utility_after_payload_finished':
<function Executor.utility_after_payload_finished>, 'execute_utility_command':
<function Executor.execute_utility_command>, 'write_utility_output': <function
Executor.write_utility_output>, 'pre_payload': <function Executor.pre_payload>,
'post_payload': <function Executor.post_payload>, 'run_command': <function
Executor.run_command>, 'run_payload': <function Executor.run_payload>,
'extract_setup': <function Executor.extract_setup>, 'wait_graceful': <function
Executor.wait_graceful>, 'get_payload_command': <function
Executor.get_payload_command>, 'run_preprocess': <function Executor.run_preprocess>,
'run': <function Executor.run>, 'run_utility_after_payload_finished': <function
Executor.run_utility_after_payload_finished>, 'stop_utilities': <function
Executor.stop_utilities>, 'rename_log_files': <function Executor.rename_log_files>,
'__dict__': <attribute '__dict__' of 'Executor' objects>, '__weakref__': <attribute
'__weakref__' of 'Executor' objects>, '__doc__': None, '__annotations__': {}})
__init__(args, job, out, err, traces)
__module__ = 'pilot.control.payloads.generic'
__weakref__
list of weak references to the object (if defined)
execute_utility_command(cmd, job, label)
Execute a utility command (e.g. pre/postprocess commands; label=preprocess etc).
Parameters
• cmd – full command to be executed (string).
• job – job object.
• label – command label (string).
Returns exit code (int).
7.3. Pilot
161
PanDAWMS
extract_setup(cmd)
Extract the setup from the payload command (cmd). E.g. extract the full setup from the payload command
will be prepended to the pre/postprocess command.
Parameters cmd – payload command (string).
Returns updated secondary command (string).
get_job()
Get the job object. :return: job object.
get_payload_command(job)
Return the payload command string.
Parameters job – job object.
Returns command (string).
get_utility_command(order=None)
Return the command for the requested utility command (will be downloaded if necessary). Note: the
utility itself is defined in the user common code and is defined according to the order, e.g.
UTIL-
ITY_AFTER_PAYLOAD_STARTED means a co-process (see ATLAS user code).
Parameters order – order constant (const).
Returns command to be executed (string).
post_payload(job)
Calls to functions to run after payload. E.g. write time stamps to timing file.
Parameters job – job object
post_setup(job)
Functions to run post setup :param job: job object
pre_payload(job)
Calls to functions to run before payload. E.g. write time stamps to timing file.
Parameters job – job object.
pre_setup(job)
Functions to run pre setup :param job: job object
rename_log_files(iteration)
Parameters iteration –
Returns
run()
Run all payload processes (including pre- and post-processes, and utilities). In the case of HPO jobs, this
function will loop over all processes until the preprocess returns a special exit code. :return:
run_command(cmd, label=None)
Execute the given command and return the process id.
Parameters cmd – command (string).
Returns process id (int).
run_payload(job, cmd, out, err)
Setup and execute the main payload process.
REFACTOR using run_command()
162
Chapter 7. System Architecture
PanDAWMS
Parameters
• job – job object.
• out – (currently not used; deprecated)
• err – (currently not used; deprecated)
Returns proc (subprocess returned by Popen())
run_preprocess(job)
Run any preprocess payloads.
Parameters job – job object.
Returns
run_utility_after_payload_finished(exit_code, state, order)
Run utility command after the main payload has finished. In horovod mode, select the corresponding post-
process. Otherwise, select different post-process (e.g. Xcache).
The
order
constant
can
be
UTILITY_AFTER_PAYLOAD_FINISHED,
UTIL-
ITY_AFTER_PAYLOAD_FINISHED2
Parameters
• exit_code – transform exit code (int).
• state – payload state; finished/failed (string).
• order – constant used for utility selection (constant).
Returns exit code (int).
stop_utilities()
Stop any running utilities.
Returns
utility_after_payload_finished(job, order)
Prepare commands/utilities to run after payload has finished.
This command will be executed later.
The
order
constant
can
be
UTILITY_AFTER_PAYLOAD_FINISHED,
UTIL-
ITY_AFTER_PAYLOAD_FINISHED2
Parameters
• job – job object.
• order – constant used for utility selection (constant).
Returns command (string), label (string).
utility_after_payload_started(job)
Functions to run after payload started :param job: job object
utility_after_payload_started_new(job)
Functions to run after payload started
REFACTOR
Parameters job – job object
7.3. Pilot
163
PanDAWMS
utility_before_payload(job)
Prepare commands/utilities to run before payload. These commands will be executed later (as eg the payload
command setup is unknown at this point, which is needed for the preprocessing. Preprocessing is prepared
here).
REFACTOR
Parameters job – job object.
utility_with_payload(job)
Functions to run with payload.
REFACTOR
Parameters job – job object.
wait_graceful(args, proc)
Wait for payload process to finish.
Parameters
• args – Pilot arguments object.
• proc – Process id (int).
Returns exit code (int).
write_utility_output(workdir, step, stdout, stderr)
Write the utility command output to stdout, stderr files to the job.workdir for the current step.
->
<step>_stdout.txt, <step>_stderr.txt Example of step: preprocess, postprocess.
Parameters
• workdir – job workdir (string).
• step – utility step (string).
• stdout – command stdout (string).
• stderr – command stderr (string).
Returns
copytool components
common
pilot.copytool.common.get_copysetup(copytools, copytool_name)
Return the copysetup for the given copytool.
Parameters
• copytools – copytools list from infosys.
• name (copytool) – name of copytool (string).
Returns copysetup (string).
pilot.copytool.common.get_error_info(rcode, state, error_msg)
Return an error info dictionary specific to transfer errors. Helper function to resolve_common_transfer_errors().
Parameters
• rcode – return code (int).
164
Chapter 7. System Architecture
PanDAWMS
• state – state string used in Rucio traces.
• error_msg – transfer command stdout (string).
Returns dictionary with format {‘rcode’: rcode, ‘state’: state, ‘error’: error_msg}.
pilot.copytool.common.get_timeout(filesize, add=0)
Get a proper time-out limit based on the file size.
Parameters
• filesize – file size (int).
• add – optional additional time to be added [s] (int)
Returns time-out in seconds (int).
pilot.copytool.common.merge_destinations(files)
Converts the file-with-destination dict to a destination-with-files dict
:param files Files to merge
:returns destination-with-files dictionary
pilot.copytool.common.output_line_scan(ret, output)
Do some reg exp on the transfer command output to search for special errors.
Helper function to re-
solve_common_transfer_errors().
Parameters
• ret – pre-filled error info dictionary with format {‘rcode’: rcode, ‘state’: state, ‘error’:
error_msg}
• output – transfer command stdout (string).
Returns updated error info dictionary.
pilot.copytool.common.resolve_common_transfer_errors(output, is_stagein=True)
Resolve any common transfer related errors.
Parameters
• output – stdout from transfer command (string).
• is_stagein – optional (boolean).
Returns dict {‘rcode’: rcode, ‘state’: state, ‘error’: error_msg}.
pilot.copytool.common.verify_catalog_checksum(fspec, path)
Verify that the local and remote (fspec) checksum values are the same. The function will update the fspec object.
Parameters
• fspec – FileSpec object for a given file.
• path – path to local file (string).
Returns state (string), diagnostics (string).
7.3. Pilot
165
PanDAWMS
gfal
pilot.copytool.gfal.check_for_gfal()
pilot.copytool.gfal.copy_in(files, **kwargs)
Download given files using gfal-copy command.
Parameters files – list of FileSpec objects
Raise PilotException in case of controlled error
pilot.copytool.gfal.copy_out(files, **kwargs)
Upload given files using gfal command.
Parameters files – Files to upload
Raises PilotException in case of errors
pilot.copytool.gfal.is_valid_for_copy_in(files)
pilot.copytool.gfal.is_valid_for_copy_out(files)
pilot.copytool.gfal.move(source, destination, recursive=False)
pilot.copytool.gfal.move_all_files_in(files, nretries=1)
Move all files.
Parameters
• files –
• nretries – number of retries; sometimes there can be a timeout copying, but the next at-
tempt may succeed
Returns exit_code, stdout, stderr
pilot.copytool.gfal.move_all_files_out(files, nretries=1)
Move all files.
Parameters files –
Returns exit_code, stdout, stderr
lsm
pilot.copytool.lsm.check_for_lsm(dst_in=True)
pilot.copytool.lsm.copy_in(files, **kwargs)
Download given files using the lsm-get command.
Parameters files – list of FileSpec objects.
Raise PilotException in case of controlled error.
Returns files FileSpec object.
pilot.copytool.lsm.copy_in_old(files)
Tries to download the given files using lsm-get directly.
Parameters files – Files to download
Raises PilotException – StageInFailure
pilot.copytool.lsm.copy_out(files, **kwargs)
Upload given files using lsm copytool.
166
Chapter 7. System Architecture
PanDAWMS
Parameters files – list of FileSpec objects.
Raise PilotException in case of controlled error.
pilot.copytool.lsm.copy_out_old(files)
Tries to upload the given files using lsm-put directly.
Parameters files – Files to upload
Raises PilotException – StageOutFailure
pilot.copytool.lsm.is_valid_for_copy_in(files)
pilot.copytool.lsm.is_valid_for_copy_out(files)
pilot.copytool.lsm.move(source, destination, dst_in=True, copysetup='', options=None)
Use lsm-get or lsm-put to transfer the file.
Parameters
• source – path to source (string).
• destination – path to destination (string).
• dst_in – True for stage-in, False for stage-out (boolean).
Returns exit code, stdout, stderr
pilot.copytool.lsm.move_all_files_in(files, nretries=1)
Move all files.
Parameters
• files –
• nretries – number of retries; sometimes there can be a timeout copying, but the next at-
tempt may succeed
Returns exit_code, stdout, stderr
pilot.copytool.lsm.move_all_files_out(files, nretries=1)
Move all files.
Parameters files –
Returns exit_code, stdout, stderr
mv
pilot.copytool.mv.copy(source, destination)
Tries to upload the given files using xrdcp directly.
Parameters
• source –
• destination –
Returns exit_code, stdout, stderr
pilot.copytool.mv.copy_in(files, copy_type='symlink', **kwargs)
Tries to download the given files using mv directly.
Parameters files – list of FileSpec objects
Raises PilotException – StageInFailure
7.3. Pilot
167
PanDAWMS
pilot.copytool.mv.copy_out(files, copy_type='mv', **kwargs)
Tries to upload the given files using mv directly.
Parameters files – list of FileSpec objects
Raises PilotException – StageOutFailure
pilot.copytool.mv.create_output_list(files, init_dir, ddmconf )
Add files to the output list which tells ARC CE which files to upload
pilot.copytool.mv.is_valid_for_copy_in(files)
pilot.copytool.mv.is_valid_for_copy_out(files)
pilot.copytool.mv.move(source, destination)
Tries to upload the given files using mv directly.
Parameters
• source –
• destination –
Returns exit_code, stdout, stderr
pilot.copytool.mv.move_all_files(files, copy_type, workdir)
Move all files.
Parameters files – list of FileSpec objects
Returns exit_code, stdout, stderr
pilot.copytool.mv.symlink(source, destination)
Tries to ln the given files.
Parameters
• source –
• destination –
Returns exit_code, stdout, stderr
rucio
pilot.copytool.rucio._get_trace(fspec, traces)
Traces returned by Rucio are not orderred the same as input files from pilot. This method finds the proper trace.
Param fspec: the file that is seeked
Param traces: all traces that are received by Rucio
Returns trace_candiates that correspond to the given file
pilot.copytool.rucio._stage_in_api(dst, fspec, trace_report, trace_report_out, transfer_timeout,
use_pcache)
pilot.copytool.rucio._stage_in_bulk(dst, files, trace_report_out=None, trace_common_fields=None)
Stage-in files in bulk using the Rucio API.
Parameters
• dst – destination (string).
• files – list of fspec objects.
168
Chapter 7. System Architecture
PanDAWMS
• trace_report –
• trace_report_out –
Returns
pilot.copytool.rucio._stage_out_api(fspec, summary_file_path, trace_report, trace_report_out,
transfer_timeout)
pilot.copytool.rucio.copy_in(files, **kwargs)
Download given files using rucio copytool.
Parameters
• files – list of FileSpec objects
• ignore_errors – boolean, if specified then transfer failures will be ignored
Raise PilotException in case of controlled error
pilot.copytool.rucio.copy_in_bulk(files, **kwargs)
Download given files using rucio copytool.
Parameters
• files – list of FileSpec objects
• ignore_errors – boolean, if specified then transfer failures will be ignored
Raise PilotException in case of controlled error
pilot.copytool.rucio.copy_out(files, **kwargs)
Upload given files using rucio copytool.
Parameters
• files – list of FileSpec objects
• ignore_errors – boolean, if specified then transfer failures will be ignored
Raise PilotException in case of controlled error
pilot.copytool.rucio.get_protocol(trace_report_out)
Extract the protocol used for the transfer from the dictionary returned by rucio.
Parameters trace_report_out – returned rucio transfer dictionary (dictionary).
Returns protocol (string).
pilot.copytool.rucio.handle_rucio_error(error_msg, trace_report, trace_report_out, fspec, stagein=True)
Parameters
• error_msg –
• trace_report –
• trace_report_out –
• fspec –
Returns
pilot.copytool.rucio.is_valid_for_copy_in(files)
pilot.copytool.rucio.is_valid_for_copy_out(files)
7.3. Pilot
169
PanDAWMS
pilot.copytool.rucio.verify_stage_out(fspec)
Checks that the uploaded file is physically at the destination. :param fspec: file specifications
xrdcp
pilot.copytool.xrdcp._resolve_checksum_option(setup, **kwargs)
pilot.copytool.xrdcp._stagefile(coption, source, destination, filesize, is_stagein, setup=None, **kwargs)
Stage the file (stagein or stageout) :return: destination file details (checksum, checksum_type) in case of success,
throw exception in case of failure :raise: PilotException in case of controlled error
pilot.copytool.xrdcp.copy_in(files, **kwargs)
Download given files using xrdcp command.
Parameters files – list of FileSpec objects
Raise PilotException in case of controlled error
pilot.copytool.xrdcp.copy_out(files, **kwargs)
Upload given files using xrdcp command.
Parameters files – list of FileSpec objects
Raise PilotException in case of controlled error
pilot.copytool.xrdcp.get_file_info_from_output(output)
Extract file size, checksum value from xrdcp –chksum command output
Returns (filesize [int/None], checksum, checksum_type) or (None, None, None) in case of failure
pilot.copytool.xrdcp.is_valid_for_copy_in(files)
pilot.copytool.xrdcp.is_valid_for_copy_out(files)
eventservice components
hooks components
acthook
harvesterhook
eshook
esmanager
esmessage
esprocess
info components
170
Chapter 7. System Architecture
PanDAWMS
basedata
The implementation of base data structure to host various settings collected from external source with built-in validation
and schema translation support.
The main reasons for such incapsulation are to
• apply in one place all data validation actions (for attributes and values)
• introduce internal information schema (names of attribues) to remove dependency
with data structrure, formats, names from external sources (e.g. AGIS/CRIC)
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.basedata.BaseData
High-level object to host structured data collected from external source It’s considered to be like a bridge (con-
nector) in order to remove direct dependency to external schema (format) implementation
__dict__ = mappingproxy({'__module__': 'pilot.info.basedata', '__doc__': "\n
High-level object to host structured data collected from external source\n It's
considered to be like a bridge (connector) in order to remove direct dependency to\n
external schema (format) implementation\n ", '_keys': {}, '_load_data': <function
BaseData._load_data>, 'clean': <function BaseData.clean>, 'clean_numeric': <function
BaseData.clean_numeric>, 'clean_string': <function BaseData.clean_string>,
'clean_boolean': <function BaseData.clean_boolean>, 'clean_dictdata': <function
BaseData.clean_dictdata>, 'clean_listdata': <function BaseData.clean_listdata>,
'__repr__': <function BaseData.__repr__>, '__dict__': <attribute '__dict__' of
'BaseData' objects>, '__weakref__': <attribute '__weakref__' of 'BaseData' objects>,
'__annotations__': {}})
__module__ = 'pilot.info.basedata'
__repr__()
Default representation of an object
__weakref__
list of weak references to the object (if defined)
_keys = {}
_load_data(data, kmap={}, validators=None)
Construct and initialize data from ext source.
Parameters
• data – input dictionary of raw data settings
• kmap – the translation map of data attributes from external format to internal schema
• validators – map of validation handlers to be applied
clean()
Validate and finally clean up required data values (required object properties) if need Executed once all
fields have already passed field-specific validation checks Could be customized by child object :return:
None
7.3. Pilot
171
PanDAWMS
clean_boolean(raw, ktype, kname=None, defval=None)
Clean and convert input value to requested boolean type :param raw: raw input data :param ktype: variable
type to which result should be casted :param defval: default value to be used in case of cast error
clean_dictdata(raw, ktype, kname=None, defval=None)
Clean and convert input value to requested dict type :param raw: raw input data :param ktype: variable
type to which result should be casted :param defval: default value to be used in case of cast error
clean_listdata(raw, ktype, kname=None, defval=None)
Clean and convert input value to requested list type :param raw: raw input data :param ktype: variable type
to which result should be casted :param defval: default value to be used in case of cast error
clean_numeric(raw, ktype, kname=None, defval=0)
Clean and convert input value to requested numeric type :param raw: raw input data :param ktype: variable
type to which result should be casted :param defval: default value to be used in case of cast error
clean_string(raw, ktype, kname=None, defval='')
Clean and convert input value to requested string type :param raw: raw input data :param ktype: variable
type to which result should be casted :param defval: default value to be used in case of cast error
configinfo
Pilot Config specific info provider mainly used to customize Queue, Site, etc data of Information Service with details
fetched directly from local Pilot instance configuration
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.configinfo.PilotConfigProvider(conf=None)
Info provider which is used to extract settings specific for local Pilot instance and overwrite general configuration
used by Information Service
__dict__ = mappingproxy({'__module__': 'pilot.info.configinfo', '__doc__': '\n Info
provider which is used to extract settings specific for local Pilot instance\n and
overwrite general configuration used by Information Service\n ', 'config': None,
'__init__': <function PilotConfigProvider.__init__>, 'resolve_schedconf_sources':
<function PilotConfigProvider.resolve_schedconf_sources>, 'resolve_queuedata':
<function PilotConfigProvider.resolve_queuedata>, '__dict__': <attribute '__dict__'
of 'PilotConfigProvider' objects>, '__weakref__': <attribute '__weakref__' of
'PilotConfigProvider' objects>, '__annotations__': {}})
__init__(conf=None)
__module__ = 'pilot.info.configinfo'
__weakref__
list of weak references to the object (if defined)
config = None
resolve_queuedata(pandaqueue, **kwargs)
Resolve queue data details
Parameters pandaqueue – name of PandaQueue
Returns dict of settings for given PandaQueue as a key
172
Chapter 7. System Architecture
PanDAWMS
resolve_schedconf_sources()
Resolve prioritized list of source names to be used for SchedConfig data load :return: prioritized list of
source names
dataloader
Base loader class to retrive data from Ext sources (file, url)
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.dataloader.DataLoader
Base data loader
__dict__ = mappingproxy({'__module__': 'pilot.info.dataloader', '__doc__': '\n Base
data loader\n ', 'is_file_expired': <classmethod object>,
'get_file_last_update_time': <classmethod object>, 'load_url_data': <classmethod
object>, 'load_data': <classmethod object>, '__dict__': <attribute '__dict__' of
'DataLoader' objects>, '__weakref__': <attribute '__weakref__' of 'DataLoader'
objects>, '__annotations__': {}})
__module__ = 'pilot.info.dataloader'
__weakref__
list of weak references to the object (if defined)
classmethod get_file_last_update_time(fname)
Return the last update time of the given file.
Parameters fname – File name.
Returns Last update time in seconds or None if file does not exist.
classmethod is_file_expired(fname, cache_time=0)
Check if file fname is older than cache_time seconds from its last_update_time.
Parameters
• fname – File name.
• cache_time – Cache time in seconds.
Returns Boolean.
classmethod load_data(sources, priority, cache_time=60, parser=None)
Download data from various sources (prioritized). Try to get data from sources according to priority values
passed
Expected format of source entry: sources = {‘NAME’:{‘url’:”source url”, ‘nretry’:int, ‘fname’:’cache file
(optional)’, ‘cache_time’:int (optional), ‘sleep_time’:opt}}
Parameters
• sources – Dict of source configuration
• priority – Ordered list of source names
• cache_time – Default cache time in seconds. Can be overwritten by cache_time value
passed in sources dict
7.3. Pilot
173
PanDAWMS
• parser – Callback function to interpret/validate data which takes read data from source as
input. Default is json.loads
Returns Data loaded and processed by parser callback
classmethod load_url_data(url, fname=None, cache_time=0, nretry=3, sleep_time=60)
Download data from url or file resource and optionally save it into cache file fname. The file will not be
(re-)loaded again if cache age from last file modification does not exceed cache_time seconds.
If url is None then data will be read from cache file fname (if any)
Parameters
• url – Source of data
• fname – Cache file name. If given then loaded data will be saved into it.
• cache_time – Cache time in seconds.
• nretry – Number of retries (default is 3).
• sleep_time – Sleep time (default is 60 s) between retry attempts.
Returns data loaded from the url or file content if url passed is a filename.
pilot.info.dataloader.merge_dict_data(d1, d2, keys=[], common=True, left=True, right=True, rec=False)
Recursively merge two dict objects Merge content of d2 dict into copy of d1 :param common: if True then do
merge keys exist in both dicts :param left: if True then preseve keys exist only in d1 :param right: if True then
preserve keys exist only in d2
extinfo
Information provider from external source(s) which is mainly used to retrive Queue, Site, etc data required for Infor-
mation Service
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.extinfo.ExtInfoProvider(cache_time=60)
Information provider to retrive data from external source(s) (e.g. AGIS, PanDA, CVMFS)
__init__(cache_time=60)
Parameters cache_time – Default cache time in seconds
__module__ = 'pilot.info.extinfo'
static get_cvmfs_path(url, fname)
Return a proper path for cvmfs.
Parameters
• url – URL (string).
• fname – file name for CRIC JSON (string).
Returns cvmfs path (string).
classmethod load_queuedata(pandaqueue, priority=[], cache_time=60)
Download the queuedata from various sources (prioritized). Try to get data from PanDA, CVMFS first,
then AGIS
174
Chapter 7. System Architecture
PanDAWMS
This function retrieves only min information of queuedata provided by PanDA cache for the moment.
Parameters
• pandaqueue – PandaQueue name
• cache_time – Default cache time in seconds.
Returns
classmethod load_schedconfig_data(pandaqueues=[], priority=[], cache_time=60)
Download the (AGIS-extended) data associated to PandaQueue from various sources (prioritized). Try to
get data from CVMFS first, then AGIS or from Panda JSON sources (not implemented).
For the moment PanDA source does not provide the full schedconfig description
Parameters
• pandaqueues – list of PandaQueues to be loaded
• cache_time – Default cache time in seconds.
Returns
classmethod load_storage_data(ddmendpoints=[], priority=[], cache_time=60)
Download DDM Storages details by given name (DDMEndpoint) from various sources (prioritized). Try
to get data from LOCAL first, then CVMFS and AGIS
Parameters
• pandaqueues – list of PandaQueues to be loaded
• cache_time – Default cache time in seconds.
Returns dict of DDMEndpoint settings by DDMendpoint name as a key
resolve_queuedata(pandaqueue, schedconf_priority=None)
Resolve final full queue data details (primary data provided by PanDA merged with overall queue details
from AGIS)
Parameters pandaqueue – name of PandaQueue
Returns dict of settings for given PandaQueue as a key
resolve_storage_data(ddmendpoints=[])
Resolve final DDM Storages details by given names (DDMEndpoint)
Parameters ddmendpoints – list of ddmendpoint names
Returns dict of settings for given DDMEndpoint as a key
filespec
infoservice
The implmemtation of high-level Info Service module, which includes a set of low-level information providers to
aggregate, prioritize (overwrite), hide dependency to external storages and expose (queue, site, storage, etc) details in
a unified structured way via provided high-level API
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
7.3. Pilot
175
PanDAWMS
class pilot.info.infoservice.InfoService
High-level Information Service
__dict__ = mappingproxy({'__module__': 'pilot.info.infoservice', '__doc__': '\n
High-level Information Service\n ', 'cache_time': 60, 'require_init': <function
InfoService.require_init>, '__init__': <function InfoService.__init__>, 'init':
<function InfoService.init>, 'whoami': <classmethod object>, '_resolve_data':
<classmethod object>, 'resolve_queuedata': <function
InfoService.require_init.<locals>.inner>, 'resolve_storage_data': <function
InfoService.resolve_storage_data>, 'resolve_schedconf_sources': <function
InfoService.require_init.<locals>.inner>, 'resolve_ddmendpoint_storageid': <function
InfoService.resolve_ddmendpoint_storageid>, 'get_storage_id': <function
InfoService.get_storage_id>, 'get_ddmendpoint': <function
InfoService.get_ddmendpoint>, '__dict__': <attribute '__dict__' of 'InfoService'
objects>, '__weakref__': <attribute '__weakref__' of 'InfoService' objects>,
'__annotations__': {}})
__init__()
__module__ = 'pilot.info.infoservice'
__weakref__
list of weak references to the object (if defined)
classmethod _resolve_data(fname, providers=[], args=[], kwargs={}, merge=False)
Resolve data by calling function fname of passed provider objects.
Iterate over providers, merge data from all providers if merge is True, (consider 1st success result from
prioritized list if merge mode is False) and resolve data by execution function fname with passed arguments
args and kwargs
Returns The result of first successfull execution will be returned
cache_time = 60
get_ddmendpoint(storage_id)
Return the ddmendpoint name from a storage id.
Parameters storage_id – storage_id as an int.
Returns ddmendpoint ddmendpoint name.
Raises NotDefined –
get_storage_id(ddmendpoint)
Return the storage_id of a ddmendpoint.
Parameters ddmendpoint – ddmendpoint name.
Returns storage_id storage_id of the ddmendpoint.
Raises NotDefined –
init(pandaqueue, confinfo=None, extinfo=None, jobinfo=None)
require_init()
Method decorator to check if object is initialized
resolve_ddmendpoint_storageid(ddmendpoint=[])
Resolve the map between ddmendpoint and storage_id
resolve_queuedata(*args, **kwargs)
resolve_schedconf_sources(*args, **kwargs)
176
Chapter 7. System Architecture
PanDAWMS
resolve_storage_data(ddmendpoints=[])
Returns dict of DDMEndpoint settings by DDMEndpoint name as a key
classmethod whoami()
Returns Current function name being executed
jobdata
The implementation of data structure to host Job definition.
The main reasons for such incapsulation are to
• apply in one place all data validation actions (for attributes and values)
• introduce internal information schema (names of attributes) to remove dependency
with data structure, formats, names from external source (PanDA)
author Alexey Anisenkov
contact anisyonk@cern.ch
date February 2018
class pilot.info.jobdata.JobData(data, use_kmap=True)
High-level object to host Job definition/settings
__contains__(key)
Temporary Integration function to keep dict-based access for old logic in compatible way TO BE RE-
MOVED ONCE all fields will be moved to Job object attributes
__getitem__(key)
Temporary Integration function to keep dict-based access for old logic in compatible way TO BE RE-
MOVED ONCE all fields will be moved to Job object attributes
__init__(data, use_kmap=True)
Parameters data – input dictionary of data settings
__module__ = 'pilot.info.jobdata'
__setitem__(key, val)
Temporary Integration function to keep dict-based access for old logic in compatible way TO BE RE-
MOVED ONCE all fields will be moved to Job object attributes
_get_all_output(ksources, kmap, log_lfn, data)
Create lists of FileSpecs for output + log files. Helper function for prepare_output().
Parameters
• ksources –
• kmap –
• log_lfn – log file name (string).
• data –
Returns ret_output (list of FileSpec), ret_log (list of FileSpec)
7.3. Pilot
177
PanDAWMS
_keys = {<class 'int'>: ['corecount', 'piloterrorcode', 'transexitcode', 'exitcode',
'cpuconversionfactor', 'exeerrorcode', 'attemptnr', 'nevents', 'neventsw', 'pid',
'cpuconsumptiontime', 'maxcpucount', 'actualcorecount'], <class 'str'>: ['jobid',
'taskid', 'jobparams', 'transformation', 'destinationdblock', 'exeerrordiagstate',
'serverstate', 'workdir', 'stageout', 'platform', 'piloterrordiag', 'exitmsg',
'produserid', 'jobdefinitionid', 'writetofile', 'cpuconsumptionunit', 'homepackage',
'jobsetid', 'payload', 'processingtype', 'swrelease', 'zipmap', 'imagename',
'imagename_jobdef', 'accessmode', 'transfertype', 'datasetin', 'infilesguids',
'memorymonitor', 'allownooutput'], <class 'list'>: ['piloterrorcodes',
'piloterrordiags', 'workdirsizes', 'zombies', 'corecounts'], <class 'dict'>:
['status', 'fileinfo', 'metadata', 'utilities', 'overwrite_queuedata', 'sizes',
'preprocess', 'postprocess', 'coprocess', 'containeroptions'], <class 'bool'>:
['is_eventservice', 'is_eventservicemerge', 'is_hpo', 'noexecstrcnv', 'debug',
'usecontainer', 'use_vp', 'looping_check']}
_rawdata = {}
accessmode = ''
actualcorecount = 0
add_size(size)
Add a size measurement to the sizes field at the current time stamp. A size measurement is in Bytes.
Parameters size – size of object in Bytes (int).
Returns
add_workdir_size(workdir_size)
Add a measured workdir size to the workdirsizes field. The function will deduce any input and output file
sizes from the workdir size.
Parameters workdir_size – workdir size (int).
Returns
allownooutput = ''
alrbuserplatform = ''
attemptnr = 0
clean()
Validate and finally clean up required data values (object properties) if need :return: None
clean__corecount(raw, value)
Verify and validate value for the corecount key (set to 1 if not set)
clean__jobparams(raw, value)
Verify and validate value for the jobparams key Extract value from jobparams not related to job options.
The function will in particular extract and remove –overwriteQueueData, ZIP_MAP and –containerimage.
It will remove the old Pilot 1 option –overwriteQueuedata which should be replaced with –overwriteQueue-
Data.
Parameters
• raw – (unused).
• value – job parameters (string).
Returns updated job parameters (string).
178
Chapter 7. System Architecture
PanDAWMS
clean__platform(raw, value)
Verify and validate value for the platform key.
Set the alrbuserplatform value if encoded in plat-
form/cmtconfig string.
Parameters
• raw – (unused).
• value – platform (string).
Returns updated platform (string).
collect_zombies(tn=None)
Collect zombie child processes, tn is the max number of loops, plus 1, to avoid infinite looping even if
some child processes really get wedged; tn=None means it will keep going until all child zombies have
been collected.
Parameters tn – max depth (int).
Returns
command = ''
containeroptions = {}
coprocess = {}
corecount = 1
corecounts = []
cpuconsumptiontime = -1
cpuconsumptionunit = 's'
cpuconversionfactor = 1
currentsize = 0
datasetin = ''
dbdata = None
dbtime = None
debug = False
debug_command = ''
destinationdblock = ''
exeerrorcode = 0
exeerrordiag = ''
exitcode = 0
exitmsg = ''
extract_container_image(jobparams)
Extract the container image from the job parameters if present, and remove it.
Parameters jobparams – job parameters (string).
Returns updated job parameters (string), extracted image name (string).
fileinfo = {}
7.3. Pilot
179
PanDAWMS
get(key, defval=None)
Temporary Integration function to keep dict-based access for old logic in compatible way TO BE RE-
MOVED ONCE all fields will be moved to Job object attributes
get_job_option_for_input_name(input_name)
Expecting something like –inputHitsFile=@input_name in jobparams.
Returns job_option such as –inputHitsFile
static get_kmap()
Return the kmap dictionary for server data to pilot conversions.
Returns kmap (dict).
get_lfns_and_guids()
Return ordered lists with the input file LFNs and GUIDs.
Returns list of input files, list of corresponding GUIDs.
get_max_workdir_size()
Return the maximum disk space used by the payload.
Returns workdir size (int).
static get_opts_pargs(data)
Get the opts and pargs variables.
Parameters data – input command line arguments (raw string)
Returns opts (dict), pargs (list)
static get_ret(options, opts)
Get the ret variable from the options.
Parameters
• options –
• opts –
Returns ret (dict).
get_size()
Determine the size (B) of the job object.
Returns size (int).
get_status(key)
Return
the
value
for
the
given
key
(e.g.
LOG_TRANSFER)
from
the
status
dictionary.
LOG_TRANSFER_NOT_DONE is returned if job object is not defined for key=’LOG_TRANSFER’. If
no key is found, None will be returned.
Parameters key – key name (string).
Returns corresponding key value in job.status dictionary (string).
has_remoteio()
Check status of input file transfers and determine either direct access mode will be used or not. :return:
True if at least one file should use direct access mode
homepackage = ''
imagename = ''
imagename_jobdef = ''
indata = []
180
Chapter 7. System Architecture
PanDAWMS
infilesguids = ''
init(infosys)
Parameters infosys – infosys object
is_analysis()
Determine whether the job is an analysis user job or not. :return: True in case of user analysis job
is_build_job()
Check if the job is a build job. (i.e. check if the job has an output file that is a lib file).
Returns boolean
is_eventservice = False
is_eventservicemerge = False
is_hpo = False
is_local()
Should the input files be accessed locally? Note: all input files will have storage_token set to local in that
case.
Returns boolean.
jobdefinitionid = ''
jobid = None
jobparams = ''
jobsetid = ''
load(data, use_kmap=True)
Construct and initialize data from ext source :param data: input dictionary of job data settings
logdata = []
looping_check = True
maxcpucount = 0
memorymonitor = ''
metadata = {}
nevents = 0
neventsw = 0
noexecstrcnv = None
only_copy_to_scratch()
Determine if the payload only has copy-to-scratch input. In this case, there should be no –usePFCTurl or
–directIn in the job parameters.
Returns True if only copy-to-scratch. False if at least one file should use direct access mode
outdata = []
overwrite_queuedata = {}
overwrite_storagedata = {}
7.3. Pilot
181
PanDAWMS
classmethod parse_args(data, options, remove=False)
Extract option/values from string containing command line options (arguments) :param data: input com-
mand line arguments (raw string) :param options: dict of option names to be considered: (name, type), type
is a cast function to be applied with result value :param remove: boolean, if True then exclude specified
options from returned raw string of command line arguments :return: tuple: (dict of extracted options, raw
string of final command line options)
payload = ''
pgrp = None
pid = None
piloterrorcode = 0
piloterrorcodes = []
piloterrordiag = ''
piloterrordiags = []
platform = ''
postprocess = {}
prepare_infiles(data)
Construct FileSpec objects for input files from raw dict data :return: list of validated FileSpec objects
prepare_outfiles(data)
Construct validated FileSpec objects for output and log files from raw dict data Note: final preparation for
output files can only be done after the payload has finished in case the payload has produced a job report
with e.g. output file guids. This is verified in pilot/control/payload/process_job_report().
Parameters data –
Returns (list of FileSpec for output, list of FileSpec for log)
preprocess = {}
process_writetofile()
Expecting
writetofile
from
the
job
definition.
The
format
is
‘input-
For_file1:lfn1,lfn2^inputFor_file2:lfn3,lfn4’
format writetofile_dictionary = {‘inputFor_file1’: [lfn1, lfn2], ‘inputFor_file2’: [lfn3, lfn4]}
processingtype = ''
produserid = ''
reset_errors()
Returns
resimevents = None
serverstate = ''
set_accessmode()
Set the accessmode field using jobparams.
Returns
setup = ''
static show_access_settings(access_keys)
Show access settings for the case job.infosys.queuedata is not initialized.
182
Chapter 7. System Architecture
PanDAWMS
Parameters access_keys – list of access keys (list).
Returns
sizes = {}
stageout = ''
state = ''
status = {'LOG_TRANSFER': 'NOT_DONE'}
swrelease = ''
t0 = None
taskid = None
to_json()
transexitcode = 0
transfertype = ''
transformation = ''
use_vp = False
usecontainer = False
utilities = {}
workdir = ''
workdirsizes = []
writetofile = ''
zipmap = ''
zombies = []
jobinfo
Job specific info provider mainly used to customize Queue, Site, etc data of Information Service with details fetched
directly from Job instance
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.jobinfo.JobInfoProvider(job)
Job info provider which is used to extract settings specific for given Job and overwrite general configuration used
by Information Service
__dict__ = mappingproxy({'__module__': 'pilot.info.jobinfo', '__doc__': '\n Job info
provider which is used to extract settings specific for given Job\n and overwrite
general configuration used by Information Service\n ', 'job': None, '__init__':
<function JobInfoProvider.__init__>, 'resolve_schedconf_sources': <function
JobInfoProvider.resolve_schedconf_sources>, 'resolve_queuedata': <function
JobInfoProvider.resolve_queuedata>, 'resolve_storage_data': <function
JobInfoProvider.resolve_storage_data>, '__dict__': <attribute '__dict__' of
'JobInfoProvider' objects>, '__weakref__': <attribute '__weakref__' of
'JobInfoProvider' objects>, '__annotations__': {}})
7.3. Pilot
183
PanDAWMS
__init__(job)
__module__ = 'pilot.info.jobinfo'
__weakref__
list of weak references to the object (if defined)
job = None
resolve_queuedata(pandaqueue, **kwargs)
Resolve Job specific settings for queue data (overwriteQueueData) :return: dict of settings for given Pan-
daQueue as a key
resolve_schedconf_sources()
Resolve Job specific prioritized list of source names to be used for SchedConfig data load :return: prioritized
list of source names
resolve_storage_data(ddmendpoints=[], **kwargs)
Resolve Job specific settings for storage data (including data passed via –overwriteStorageData) :return:
dict of settings for requested DDMEndpoints with ddmendpoin as a key
jobinfoservice
Job specific Info Service It could customize/overwrite settings provided by the main Info Service
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.jobinfoservice.JobInfoService(job)
Info service: Job specific Job could overwrite settings provided by Info Service
*** KEPT for a while in repo .. most probably will be deprecated and removed soon **
__init__(job)
__module__ = 'pilot.info.jobinfoservice'
queuedata
The implementation of data structure to host queuedata settings.
The main reasons for such incapsulation are to
• apply in one place all data validation actions (for attributes and values)
• introduce internal information schema (names of attribues) to remove dependency
with data structrure, formats, names from external sources (e.g. AGIS/CRIC)
This module should be standalone as much as possible and even does not depend on the configuration settings (for that
purposed PilotConfigProvider can be user to customize data)
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.queuedata.QueueData(data)
High-level object to host all queuedata settings associated to given PandaQueue
184
Chapter 7. System Architecture
PanDAWMS
__init__(data)
Parameters data – input dictionary of queue data settings
__module__ = 'pilot.info.queuedata'
_keys = {<class 'int'>: ['timefloor', 'maxwdir', 'pledgedcpu', 'es_stageout_gap',
'corecount', 'maxrss', 'maxtime', 'maxinputsize'], <class 'str'>: ['name', 'type',
'appdir', 'catchall', 'platform', 'container_options', 'container_type', 'resource',
'state', 'status', 'site'], <class 'dict'>: ['copytools', 'acopytools', 'astorages',
'aprotocols', 'acopytools_schemas'], <class 'bool'>: ['allow_lan', 'allow_wan',
'direct_access_lan', 'direct_access_wan', 'is_cvmfs', 'use_pcache']}
acopytools = None
acopytools_schemas = {}
allow_lan = True
allow_wan = False
appdir = ''
aprotocols = None
astorages = None
catchall = ''
clean()
Validate and finally clean up required data values (required object properties) if need :return: None
clean__container_options(raw, value)
Verify and validate value for the container_options key (remove bad values)
clean__container_type(raw, value)
Parse
and
prepare
value
for
the
container_type
key
Expected
raw
data
in
format
‘con-
tainer_name:user_name;’ E.g. container_type = ‘singularity:pilot;docker:wrapper’
Returns dict of container names by user as a key
clean__corecount(raw, value)
Verify and validate value for the corecount key (set to 1 if not set)
clean__timefloor(raw, value)
Verify and validate value for the timefloor key (convert to seconds)
container_options = ''
container_type = {}
copytools = None
corecount = 1
direct_access_lan = False
direct_access_wan = False
es_stageout_gap = 0
is_cvmfs = True
load(data)
Construct and initialize data from ext source :param data: input dictionary of queue data settings
7.3. Pilot
185
PanDAWMS
maxinputsize = 0
maxrss = 0
maxtime = 0
maxwdir = 0
name = ''
platform = ''
pledgedcpu = 0
resolve_allowed_schemas(activity, copytool=None)
Resolve list of allowed schemas for given activity and requested copytool based on acopytools_schemas
settings :param activity: str or ordered list of transfer activity names to resolve acopytools related data
:return: list of protocol schemes
resource = ''
site = None
state = None
status = ''
timefloor = 0
use_pcache = False
storagedata
The implementation of data structure to host storage data description.
The main reasons for such incapsulation are to
• apply in one place all data validation actions (for attributes and values)
• introduce internal information schema (names of attribues) to remove direct dependency
with data structrure, formats, names from external sources (e.g. AGIS/CRIC)
author Alexey Anisenkov
contact anisyonk@cern.ch
date January 2018
class pilot.info.storagedata.StorageData(data)
High-level object to host Storage details (available protocols, etc.)
__init__(data)
Parameters data – input dictionary of storage description by DDMEndpoint name as key
__module__ = 'pilot.info.storagedata'
_keys = {<class 'int'>: ['pk'], <class 'str'>: ['name', 'state', 'site', 'type',
'token'], <class 'dict'>: ['copytools', 'acopytools', 'astorages', 'arprotocols',
'rprotocols', 'resource'], <class 'bool'>: ['is_deterministic']}
arprotocols = {}
186
Chapter 7. System Architecture
PanDAWMS
get_security_key(secret_key, access_key)
Get security key pair from panda :param secret_key: secrect key name as string :param access_key: access
key name as string :return: setup as a string
get_special_setup(protocol_id=None)
Construct special setup for ddms such as objectstore :param protocol_id: protocol id. :return: setup as a
string
is_deterministic = None
load(data)
Construct and initialize data from ext source :param data: input dictionary of storage description by
DDMEndpoint name as key
name = ''
pk = 0
resource = None
rprotocols = {}
site = None
special_setup = {}
state = None
token = ''
type = ''
resource components
alcf
pilot.resource.alcf.get_setup(job=None)
Return the resource specific setup.
Parameters job – optional job object.
Returns setup commands (list).
bnl
pilot.resource.bnl.get_setup(job=None)
Return the resource specific setup.
Parameters job – optional job object.
Returns setup commands (list).
7.3. Pilot
187
PanDAWMS
generic
pilot.resource.generic.get_setup(job=None)
Return the resource specific setup.
Parameters job – optional job object.
Returns setup commands (list).
nersc
pilot.resource.nersc.get_setup(job=None)
Return the resource specific setup.
Parameters job – optional job object.
Returns setup commands (list).
summit
pilot.resource.summit.get_setup(job=None)
Return the resource specific setup.
Parameters job – optional job object.
Returns setup commands (list).
titan
pilot.resource.titan.command_fix(command, job_scratch_dir)
Modification of payload parameters, to be executed on Titan on RAM disk. Some cleanup.
Parameters
• command – payload command (string).
• job_scratch_dir – local path to input files (string).
Returns updated/fixed payload command (string).
pilot.resource.titan.get_job(harvesterpath)
Return job description in dictionary and MPI rank (if applicable)
Parameters harvesterpath – path to config.Harvester.jobs_list_file (string).
Returns job object, rank (int).
pilot.resource.titan.get_setup(job=None)
Return the resource specific setup.
Parameters job – optional job object.
Returns setup commands (list).
pilot.resource.titan.postprocess_workdir(workdir)
Post-processing of working directory. Unlink paths.
Parameters workdir – path to directory to be processed (string).
Raises FileHandlingFailure – in case of IOError.
188
Chapter 7. System Architecture
PanDAWMS
pilot.resource.titan.process_jobreport(payload_report_file, job_scratch_path,
job_communication_point)
Copy job report file to make it accessible by Harvester. Shrink job report file.
Parameters
• payload_report_file – name of job report (string).
• job_scratch_path – path to scratch directory (string).
• job_communication_point – path to updated job report accessible by Harvester (string).
Raises FileHandlingFailure – in case of IOError.
pilot.resource.titan.set_job_workdir(job, path)
Point pilot to job working directory (job id).
Parameters
• job – job object.
• path – local path to Harvester access point (string).
Returns job working directory (string).
pilot.resource.titan.set_scratch_workdir(job, work_dir, args)
Copy input files and some db files to RAM disk.
Parameters
• job – job object.
• work_dir – job working directory (permanent FS) (string).
• args – args dictionary to collect timing metrics.
Returns job working directory in scratch (string).
user components
atlas components
common
class pilot.user.atlas.common.DictQuery
Helper class for parsing job report.
__dict__ = mappingproxy({'__module__': 'pilot.user.atlas.common', '__doc__': '\n
Helper class for parsing job report.\n ', 'get': <function DictQuery.get>,
'__dict__': <attribute '__dict__' of 'DictQuery' objects>, '__weakref__': <attribute
'__weakref__' of 'DictQuery' objects>, '__annotations__': {}})
__module__ = 'pilot.user.atlas.common'
__weakref__
list of weak references to the object (if defined)
get(path, dst_dict, dst_key)
Return the value for key if key is in the dictionary, else default.
pilot.user.atlas.common.add_athena_proc_number(cmd)
Add the ATHENA_PROC_NUMBER and ATHENA_CORE_NUMBER to the payload command if necessary.
7.3. Pilot
189
PanDAWMS
Parameters cmd – payload execution command (string).
Returns updated payload execution command (string).
pilot.user.atlas.common.add_makeflags(job_core_count, cmd)
Correct for multi-core if necessary (especially important in case coreCount=1 to limit parallel make).
Parameters
• job_core_count – core count from the job definition (int).
• cmd – payload execution command (string).
Returns updated payload execution command (string).
pilot.user.atlas.common.allow_timefloor(submitmode)
Should the timefloor mechanism (multi-jobs) be allowed for the given submit mode?
Parameters submitmode – submit mode (string).
pilot.user.atlas.common.cleanup_broken_links(workdir)
Run a second pass to clean up any broken links prior to log file creation.
Parameters workdir – working directory (string)
Returns
pilot.user.atlas.common.cleanup_looping_payload(workdir)
Run a special cleanup for looping payloads. Remove any root and tmp files.
Parameters workdir – working directory (string)
Returns
pilot.user.atlas.common.cleanup_payload(workdir, outputfiles=None, removecores=True)
Cleanup of payload (specifically AthenaMP) sub directories prior to log file creation. Also remove core dumps.
Parameters
• workdir – working directory (string).
• outputfiles – list of output files.
• removecores – remove core files if True (Boolean).
Returns
pilot.user.atlas.common.discover_new_outdata(job)
Discover new outdata created by HPO job.
Parameters job – job object.
Returns new_outdata (list of FileSpec objects)
pilot.user.atlas.common.discover_new_output(name_pattern, workdir)
Discover new output created by HPO job in the given work dir.
name_pattern for known ‘filename’ is ‘filename_N’ (N = 0, 1, 2, ..). Example: name_pattern = 23578835.met-
rics.000001.tgz
should discover files with names 23578835.metrics.000001.tgz_N (N = 0, 1, ..)
new_output = { lfn: {‘path’: path, ‘size’: size, ‘checksum’: checksum}, .. }
Parameters
• name_pattern – assumed name pattern for file to discover (string).
• workdir – work directory (string).
190
Chapter 7. System Architecture
PanDAWMS
Returns new_output (dictionary).
pilot.user.atlas.common.download_command(process, workdir)
Download the pre/postprocess commands if necessary.
Process FORMAT: {‘command’: <command>, ‘args’: <args>, ‘label’: <some name>}
Parameters
• process – pre/postprocess dictionary.
• workdir – job workdir (string).
Returns updated pre/postprocess dictionary.
pilot.user.atlas.common.extract_output_file_guids(job)
Extract output file info from the job report and make sure all guids are assigned (use job report value if present,
otherwise generate the guid. Note: guid generation is done later, not in this function since this function might
not be called if metadata info is not found prior to the call).
Parameters job – job object.
Returns
pilot.user.atlas.common.extract_turls(indata)
Extract TURLs from indata for direct i/o files.
Parameters indata – list of FileSpec.
Returns comma-separated list of turls (string).
pilot.user.atlas.common.get_analysis_run_command(job, trf_name)
Return the proper run command for the user job.
Example output: export X509_USER_PROXY=<..>;./runAthena <job parameters> –usePFCTurl –directIn
Parameters
• job – job object.
• trf_name – name of the transform that will run the job (string).
Used when containers are not used. :return: command (string).
pilot.user.atlas.common.get_cpu_times(jobreport_dictionary)
Extract and add up the total CPU times from the job report. E.g. (‘s’, 5790L, 1.0).
Note: this function is used with Event Service jobs
Parameters jobreport_dictionary –
Returns cpu_conversion_unit (unit), total_cpu_time,
conversion_factor (output consistent with set_time_consumed())
pilot.user.atlas.common.get_db_info(jobreport_dictionary)
Extract and add up the DB info from the job report. This information is reported with the jobMetrics. Note: this
function adds up the different dbData and dbTime’s in the different executor steps. In modern job reports this
might have been done already by the transform and stored in dbDataTotal and dbTimeTotal.
Parameters jobreport_dictionary – job report dictionary.
Returns db_time (int), db_data (long)
pilot.user.atlas.common.get_db_info_str(db_time, db_data)
Convert db_time, db_data to strings. E.g. dbData=”105077960”, dbTime=”251.42”.
Parameters
7.3. Pilot
191
PanDAWMS
• db_time – time (s)
• db_data – long integer
Returns db_time_s, db_data_s (strings)
pilot.user.atlas.common.get_executor_dictionary(jobreport_dictionary)
Extract the ‘executor’ dictionary from with a job report.
Parameters jobreport_dictionary –
Returns executor_dictionary
pilot.user.atlas.common.get_exit_info(jobreport_dictionary)
Return the exit code (exitCode) and exit message (exitMsg). E.g. (0, ‘OK’).
Parameters jobreport_dictionary –
Returns exit_code, exit_message
pilot.user.atlas.common.get_file_open_command(script_path, turls, nthreads)
Parameters
• script_path – path to script (string).
• turls – comma-separated turls (string).
• nthreads – number of concurrent file open threads (int).
Returns comma-separated list of turls (string).
pilot.user.atlas.common.get_file_transfer_info(job)
Return information about desired file transfer.
Parameters job – job object
Returns use copy tool (boolean), use direct access (boolean),
use PFC Turl (boolean).
pilot.user.atlas.common.get_generic_payload_command(cmd, job, preparesetup, userjob)
Parameters
• cmd –
• job – job object.
• preparesetup –
• userjob – True for user analysis jobs, False otherwise (bool).
Returns generic job command (string).
pilot.user.atlas.common.get_guids_from_jobparams(jobparams, infiles, infilesguids)
Extract the correct guid from the input file list. The guids list is used for direct reading. 1. extract input file list
for direct reading from job parameters 2. for each input file in this list, find the corresponding guid from the input
file guid list. Since the job parameters string is entered by a human, the order of the input files might not be the
same.
Parameters
• jobparams – job parameters.
• infiles – input file list.
192
Chapter 7. System Architecture
PanDAWMS
• infilesguids – input file guids list.
Returns guids list.
pilot.user.atlas.common.get_metadata(workdir)
Return the metadata from file.
Parameters workdir – work directory (string)
Returns
pilot.user.atlas.common.get_normal_payload_command(cmd, job, preparesetup, userjob)
Return the payload command for a normal production/analysis job.
Parameters
• cmd – any preliminary command setup (string).
• job – job object.
• userjob – True for user analysis jobs, False otherwise (bool).
• preparesetup – True if the pilot should prepare the setup,
False if already in the job parameters. :return: normal payload command (string).
pilot.user.atlas.common.get_nthreads(catchall)
Extract number of concurrent file open threads from catchall. Return nthreads=1 if nopenfiles=.. is not present
in catchall.
Parameters catchall – queuedata catchall (string).
Returns number of threads (int).
pilot.user.atlas.common.get_outfiles_records(subfiles)
Extract file info from job report JSON subfiles entry.
Parameters subfiles – list of subfiles.
Returns file info dictionary with format { ‘guid’: .., ‘size’: .., ‘nentries’: .. (optional)}
pilot.user.atlas.common.get_payload_command(job)
Return the full command for executing the payload, including the sourcing of all setup files and setting of envi-
ronment variables.
Parameters job – job object.
Raises PilotException – TrfDownloadFailure.
Returns command (string).
pilot.user.atlas.common.get_precopostprocess_command(process, workdir, label)
Return the pre/co/post-process command dictionary.
Command FORMAT: {‘command’: <command>, ‘args’: <args>, ‘label’: <some name>}
The returned command has the structure: { ‘command’: <string>, } :param process: pre/co/post-process (dictio-
nary). :param workdir: working directory (string). :param label: label (string). :return: command (dictionary).
pilot.user.atlas.common.get_redundant_path()
Return the path to the file containing the redundant files and directories to be removed prior to log file creation.
Returns file path (string).
7.3. Pilot
193
PanDAWMS
pilot.user.atlas.common.get_redundants()
Get list of redundant files and directories (to be removed). The function will return the content of an external
file. It that can’t be read, then a list defined in this function will be returned instead. Any updates to the external
file must be propagated to this function.
Returns files and directories list
pilot.user.atlas.common.get_resimevents(jobreport_dictionary)
Extract and add up the resimevents from the job report. This information is reported with the jobMetrics.
Parameters jobreport_dictionary – job report dictionary.
Returns resimevents (int or None)
pilot.user.atlas.common.get_stageout_label(job)
Get a proper stage-out label.
Parameters job – job object.
Returns “all”/”log” depending on stage-out type (string).
pilot.user.atlas.common.get_utility_after_payload_started()
Return the command dictionary for the utility after the payload has started.
Command FORMAT: {‘command’: <command>, ‘args’: <args>, ‘label’: <some name>}
Returns command (dictionary).
pilot.user.atlas.common.get_utility_command_execution_order(name)
Should the given utility command be executed before or after the payload?
Parameters name – utility name (string).
Returns execution order constant.
pilot.user.atlas.common.get_utility_command_kill_signal(name)
Return the proper kill signal used to stop the utility command.
Parameters name – name of utility command (string).
Returns kill signal
pilot.user.atlas.common.get_utility_command_output_filename(name, selector=None)
Return the filename to the output of the utility command.
Parameters
• name – utility name (string).
• selector – optional special conditions flag (boolean).
Returns filename (string).
pilot.user.atlas.common.get_utility_command_setup(name, job, setup=None)
Return the proper setup for the given utility command. If a payload setup is specified, then the utility command
string should be prepended to it.
Parameters
• name – name of utility (string).
• job – job object.
• setup – optional payload setup string.
Returns utility command setup (string).
194
Chapter 7. System Architecture
PanDAWMS
pilot.user.atlas.common.get_utility_commands(order=None, job=None)
Return a dictionary of utility commands and arguments to be executed in parallel with the payload. This could e.g.
be memory and network monitor commands. A separate function can be used to determine the corresponding
command setups using the utility command name. If the optional order parameter is set, the function should
return the list of corresponding commands.
For example:
If order=UTILITY_BEFORE_PAYLOAD, the function should return all commands that are to be executed be-
fore the payload.
If order=UTILITY_WITH_PAYLOAD, the corresponding commands will be prepended to the payload execution
string.
If order=UTILITY_AFTER_PAYLOAD_STARTED, the commands that should be executed after the payload
has been started should be returned.
If order=UTILITY_WITH_STAGEIN, the commands that should be executed parallel with stage-in will be re-
turned.
FORMAT: {‘command’: <command>, ‘args’: <args>, ‘label’: <some name>, ‘ignore_failure’: <Boolean>}
Parameters
• order – optional sorting order (see pilot.util.constants).
• job – optional job object.
Returns dictionary of utilities to be executed in parallel with the payload.
pilot.user.atlas.common.get_xcache_command(catchall, workdir, jobid, label, xcache_function)
Return the proper xcache command for either activation or deactivation.
Command FORMAT: {‘command’: <command>, ‘args’: <args>, ‘label’: <some name>}
Parameters
• catchall – queuedata catchall field (string).
• workdir – job working directory (string).
• jobid – PanDA job id (string).
• label – label (string).
• xcache_function – activation/deactivation function name (function).
Returns command (dictionary).
pilot.user.atlas.common.list_work_dir(workdir)
Execute ls -lF for the given directory and dump to log.
Parameters workdir – directory name (string).
pilot.user.atlas.common.open_remote_files(indata, workdir, nthreads)
Verify that direct i/o files can be opened.
Parameters
• indata – list of FileSpec.
• workdir – working directory (string).
• nthreads – number of concurrent file open threads (int).
Returns exit code (int), diagnostics (string).
7.3. Pilot
195
PanDAWMS
pilot.user.atlas.common.parse_jobreport_data(job_report)
Parse a job report and extract relevant fields.
Parameters job_report –
Returns
pilot.user.atlas.common.post_prestagein_utility_command(**kwargs)
Execute any post pre-stage-in utility commands.
Parameters kwargs – kwargs (dictionary).
Returns
pilot.user.atlas.common.post_utility_command_action(name, job)
Perform post action for given utility command.
Parameters
• name – name of utility command (string).
• job – job object.
Returns
pilot.user.atlas.common.preprocess_debug_command(job)
Pre-process the debug command in debug mode.
Parameters job – Job object.
Returns
pilot.user.atlas.common.process_debug_command(debug_command, pandaid)
In debug mode, the server can send a special debug command to the piloti via the updateJob backchannel. This
function can be used to process that command, i.e. to identify a proper pid to debug (which is unknown to the
server).
For gdb, the server might send a command with gdb option –pid %. The pilot need to replace the % with the
proper pid. The default (hardcoded) process will be that of athena.py. The pilot will find the corresponding pid.
Parameters
• debug_command – debug command (string).
• pandaid – PanDA id (string).
Returns updated debug command (string).
pilot.user.atlas.common.process_remote_file_traces(path, job, not_opened_turls)
Report traces for remote files. The function reads back the base trace report (common part of all traces) and
updates it per file before reporting it to the Rucio server.
Parameters
• path – path to base trace report (string).
• job – job object.
• not_opened_turls – list of turls that could not be opened (list).
Returns
pilot.user.atlas.common.remove_archives(workdir)
Explicitly remove any soft linked archives (.a files) since they will be dereferenced by the tar command (–deref-
erence option).
Parameters workdir – working directory (string)
196
Chapter 7. System Architecture
PanDAWMS
Returns
pilot.user.atlas.common.remove_from_stageout(lfn, job)
From the given lfn from the stage-out list.
Parameters
• lfn – local file name (string).
• job – job object
Returns [updated job object]
pilot.user.atlas.common.remove_no_output_files(job)
Remove files from output file list if they are listed in allowNoOutput and do not exist.
Parameters job – job object.
Returns
pilot.user.atlas.common.remove_redundant_files(workdir, outputfiles=None, islooping=False,
debugmode=False)
Remove redundant files and directories prior to creating the log file.
Note: in debug mode, any core files should not be removed before creating the log.
Parameters
• workdir – working directory (string).
• outputfiles – list of protected output files (list).
• islooping – looping job variable to make sure workDir is not removed in case of looping
(Boolean).
• debugmode – True if debug mode has been switched on (Boolean).
Returns
pilot.user.atlas.common.remove_special_files(workdir, dir_list, outputfiles)
Remove list of special files from the workdir.
Parameters
• workdir – work directory (string).
• dir_list – list of special files (list).
• outputfiles – output files (list).
Returns
pilot.user.atlas.common.sanity_check()
Perform an initial sanity check before doing anything else in a given workflow. This function can be used to
verify importing of modules that are otherwise used much later, but it is better to abort the pilot if a problem is
discovered early.
Returns exit code (0 if all is ok, otherwise non-zero exit code).
pilot.user.atlas.common.set_xcache_var(line, name='', pattern='')
Extract the value of a given environmental variable from a given stdout line.
Parameters
• line – line from stdout to be investigated (string).
• name – name of env var (string).
7.3. Pilot
197
PanDAWMS
• pattern – regex pattern (string).
Returns
pilot.user.atlas.common.should_update_logstash(frequency=10)
Should logstash be updated with prmon dictionary?
Parameters frequency –
Returns return True once per ‘frequency’ times.
pilot.user.atlas.common.update_forced_accessmode(log, cmd, transfertype, jobparams, trf_name)
Update the payload command for forced accessmode. accessmode is an option that comes from HammerCloud
and is used to force a certain input file access mode; i.e. copy-to-scratch or direct access.
Parameters
• log – logging object.
• cmd – payload command.
• transfertype – transfer type (.e.g ‘direct’) from the job
definition with priority over accessmode (string). :param jobparams: job parameters (string). :param trf_name:
transformation name (string). :return: updated payload command string.
pilot.user.atlas.common.update_job_data(job)
This function can be used to update/add data to the job object. E.g. user specific information can be extracted
from other job object fields. In the case of ATLAS, information is extracted from the metadata field and added
to other job object fields.
Parameters job – job object
Returns
pilot.user.atlas.common.update_output_for_hpo(job)
Update the output (outdata) for HPO jobs.
Parameters job – job object.
Returns
pilot.user.atlas.common.update_server(job)
Perform any user specific server actions.
E.g. this can be used to send special information to a logstash.
Parameters job – job object.
Returns
pilot.user.atlas.common.update_stagein(job)
Skip DBRelease files during stage-in.
Parameters job – job object.
Returns
pilot.user.atlas.common.validate(job)
Perform user specific payload/job validation. This function will produce a local DBRelease file if necessary (old
releases).
Parameters job – job object.
Returns Boolean (True if validation is successful).
198
Chapter 7. System Architecture
PanDAWMS
pilot.user.atlas.common.verify_extracted_output_files(output, lfns_jobdef, job)
Make sure all output files extracted from the job report are listed. Grab the number of events if possible.
Parameters
• output – list of FileSpecs (list).
• lfns_jobdef – list of lfns strings from job definition (list).
• job – job object.
Returns True if successful|False if failed, number of events (Boolean, int)
pilot.user.atlas.common.verify_job(job)
Verify job parameters for specific errors. Note:
in case of problem,
the function should set the corresponding pilot error code using:
job.piloterrorcodes, job.piloterrordiags = errors.add_error_code(error.get_error_code())
Parameters job – job object
Returns Boolean.
pilot.user.atlas.common.verify_lfn_length(outdata)
Make sure that the LFNs are all within the allowed length.
Parameters outdata – FileSpec object.
Returns error code (int), diagnostics (string).
pilot.user.atlas.common.verify_ncores(corecount)
Verify that nCores settings are correct
Parameters corecount – number of cores (int).
Returns
pilot.user.atlas.common.verify_output_files(job)
Make sure that the known output files from the job definition are listed in the job report and number of processed
events is greater than zero. If the output file is not listed in the job report, then if the file is listed in allowNoOutput
remove it from stage-out, otherwise fail the job.
Note from Rod: fail scenario: The output file is not in output:[] or is there with zero events. Then if allownooutput
is not set - fail the job. If it is set, then do not store the output, and finish ok.
Parameters job – job object.
Returns Boolean (and potentially updated job.outdata list)
pilot.user.atlas.common.verify_release_string(release)
Verify that the release (or homepackage) string is set.
Parameters release – release or homepackage string that might or might not be set.
Returns release (set string).
pilot.user.atlas.common.xcache_activation_command(workdir='', jobid='')
Return the xcache service activation command.
Note: the workdir is not used here, but the function prototype needs it in the called (xcache_activation_command
needs it).
Parameters
• workdir – unused work directory - do not remove (string).
7.3. Pilot
199
PanDAWMS
• jobid – PanDA job id to guarantee that xcache process is unique (int).
Returns xcache command (string).
pilot.user.atlas.common.xcache_deactivation_command(workdir='', jobid='')
Return the xcache service deactivation command. This service should be stopped after the payload has finished.
Copy the messages log before shutting down.
Note: the job id is not used here, but the function prototype needs it in the called (xcache_activation_command
needs it).
Parameters
• workdir – payload work directory (string).
• jobid – unused job id - do not remove (string).
Returns xcache command (string).
pilot.user.atlas.common.xcache_proxy(output)
Extract env vars from xcache stdout and set them.
Parameters output – command output (string).
Returns
container
pilot.user.atlas.container.add_asetup(job, alrb_setup, is_cvmfs, release_setup, container_script,
container_options)
Add atlasLocalSetup and options to form the final payload command.
Parameters
• job – job object.
• alrb_setup – ALRB setup (string).
• is_cvmfs – True for cvmfs sites (Boolean).
• release_setup – release setup (string).
• container_script – container script name (string).
• container_options – container options (string).
Returns final payload command (string).
pilot.user.atlas.container.alrb_wrapper(cmd, workdir, job=None)
Wrap the given command with the special ALRB setup for containers E.g. cmd = /bin/bash hello_world.sh -
> export thePlatform=”x86_64-slc6-gcc48-opt” export ALRB_CONT_RUNPAYLOAD=”cmd’ setupATLAS -c
$thePlatform
Parameters
• (string) (cmd) – command to be executed in a container.
• workdir – (not used)
• job – job object.
Returns prepended command with singularity execution command (string).
200
Chapter 7. System Architecture
PanDAWMS
pilot.user.atlas.container.create_middleware_container_command(workdir, cmd, container_options,
label='stagein', proxy=True)
Create the stage-in/out container command.
The function takes the isolated stage-in/out command, adds bits and pieces needed for the containerisation and
stores it in a stage[in|out].sh script file. It then generates the actual command that will execute the stage-in/out
script in a container.
new cmd: lsetup rucio davis xrootd old cmd exit $?
write new cmd to stage[in|out].sh script create container command and return it
Parameters
• workdir – working directory where script will be stored (string).
• cmd – isolated stage-in/out command (string).
• container_options – container options from queuedata (string).
• label – ‘stage-[in|out]’ (string).
Returns container command to be executed (string).
pilot.user.atlas.container.create_release_setup(cmd, atlas_setup, full_atlas_setup, release,
imagename, workdir, is_cvmfs)
Get the proper release setup script name, and create the script if necessary.
This function also updates the cmd string (removes full asetup from payload command).
Note: for stand-alone containers, the function will return /release_setup.sh and assume that this script exists in
the container. The pilot will only create a my_release_setup.sh script for OS containers.
In case the release setup is not present in an unpacked container, the function will reset the cmd string.
Parameters
• cmd – Payload execution command (string).
• atlas_setup – asetup command (string).
• full_atlas_setup – full asetup command (string).
• release – software release, needed to determine Athena environment (string).
• imagename – container image name (string).
• workdir – job workdir (string).
• is_cvmfs – does the queue have cvmfs? (Boolean).
Returns proper release setup name (string), updated cmd (string).
pilot.user.atlas.container.create_release_setup_old(cmd, atlas_setup, full_atlas_setup, release,
imagename, workdir, is_cvmfs)
Get the proper release setup script name, and create the script if necessary.
This function also updates the cmd string (removes full asetup from payload command).
Note: for stand-alone containers, the function will return /release_setup.sh and assume that this script exists in
the container. The pilot will only create a my_release_setup.sh script for OS containers.
In case the release setup is not present in an unpacked container, the function will reset the cmd string.
Parameters
• cmd – Payload execution command (string).
7.3. Pilot
201
PanDAWMS
• atlas_setup – asetup command (string).
• full_atlas_setup – full asetup command (string).
• release – software release, needed to determine Athena environment (string).
• imagename – container image name (string).
• workdir – job workdir (string).
• is_cvmfs – does the queue have cvmfs? (Boolean).
Returns proper release setup name (string), updated cmd (string).
pilot.user.atlas.container.create_root_container_command(workdir, cmd)
Parameters
• workdir –
• cmd –
Returns
pilot.user.atlas.container.do_use_container(**kwargs)
Decide whether to use a container or not.
Parameters kwargs – dictionary of key-word arguments.
Returns True if function has decided that a container should be used, False otherwise (boolean).
pilot.user.atlas.container.extract_atlas_setup(asetup, swrelease)
Extract the asetup command from the full setup command for jobs that have a defined release. export AT-
LAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase;
source
${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh
–quiet;source
$Atlas-
Setup/scripts/asetup.sh
-> $AtlasSetup/scripts/asetup.sh, export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase
${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh –quiet;
Parameters
• asetup – full asetup command (string).
• swrelease – ATLAS release (string).
Returns extracted asetup command, cleaned up full asetup command without asetup.sh (string).
pilot.user.atlas.container.extract_full_atlas_setup(cmd, atlas_setup)
Extract the full asetup (including options) from the payload setup command. atlas_setup is typically ‘$Atlas-
Setup/scripts/asetup.sh’.
Parameters
• cmd – full payload setup command (string).
• atlas_setup – asetup command (string).
Returns extracted full asetup command, updated full payload setup command without asetup part
(string).
pilot.user.atlas.container.extract_platform_and_os(platform)
Extract the platform and OS substring from platform
202
Chapter 7. System Architecture
PanDAWMS
Parameters (string) (platform) – E.g. “x86_64-slc6-gcc48-opt”
Returns extracted platform specifics (string). E.g. “x86_64-slc6”. In case of failure, return the full
platform
pilot.user.atlas.container.get_and_verify_payload_proxy_from_server(x509)
Download a payload proxy from the server and verify it.
Parameters x509 – X509_USER_PROXY (string).
Returns exit code (int), diagnostics (string), updated X509_USER_PROXY (string).
pilot.user.atlas.container.get_container_options(container_options)
Get the container options from AGIS for the container execution command. For Raythena ES jobs, replace the
-C with “” (otherwise IPC does not work, needed by yampl).
Parameters container_options – container options from AGIS (string).
Returns updated container command (string).
pilot.user.atlas.container.get_full_asetup(cmd, atlas_setup)
Extract the full asetup command from the payload execution command. (Easier that generating it again). We
need to remove this command for stand-alone containers. Alternatively: do not include it in the first place
(but this seems to trigger the need for further changes). atlas_setup is “source $AtlasSetup/scripts/asetup.sh”,
which is extracted in a previous step. The function typically returns: “source $AtlasSetup/scripts/asetup.sh
21.0,Athena,2020-05-19T2148,notest –makeflags=’$MAKEFLAGS’;”.
Parameters
• cmd – payload execution command (string).
• atlas_setup – extracted atlas setup (string).
Returns full atlas setup (string).
pilot.user.atlas.container.get_grid_image_for_singularity(platform)
Return the full path to the singularity grid image
Parameters platform – E.g. “x86_64-slc6” (string).
Returns full path to grid image (string).
pilot.user.atlas.container.get_middleware_container(label=None)
Return the middleware container.
Parameters label – label (string).
Returns path (string).
pilot.user.atlas.container.get_middleware_container_script(middleware_container, cmd,
asetup=False, label='')
Return the content of the middleware container script. If asetup is True, atlasLocalSetup will be added to the
command.
Parameters
• middleware_container – container image (string).
• cmd – isolated stage-in/out command (string).
• asetup – optional True/False (boolean).
Returns script content (string).
7.3. Pilot
203
PanDAWMS
pilot.user.atlas.container.get_middleware_type()
Return
the
middleware
type
from
the
container
type.
E.g.
container_type
=
‘singular-
ity:pilot;docker:wrapper;container:middleware’ get_middleware_type() -> ‘container’, meaning that middleware
should be taken from the container. The default is otherwise ‘workernode’, i.e. middleware is assumed to be
present on the worker node.
Returns middleware_type (string)
pilot.user.atlas.container.get_payload_proxy(proxy_outfile_name, voms_role='atlas')
Parameters
• proxy_outfile_name – specify the file to store proxy
• voms_role – what proxy (role) to request. It should exist on Panda node
Returns True on success
pilot.user.atlas.container.get_release_setup_name(release, imagename)
Return the file name for the release setup script.
NOTE: the /srv path will only be added later, in the case of OS containers.
For OS containers, return config.Container.release_setup (my_release_setup.sh); for stand-alone containers (user
defined containers, ie when –containerImage or job.imagename was used/set), return ‘/release_setup.sh’. re-
lease_setup.sh will NOT be created for stand-alone containers. The pilot will specify /release_setup.sh only
when jobs use the Athena environment (ie has a set job.swrelease).
Parameters
• release – software release (string).
• imagename – container image name (string).
Returns release setup file name (string).
pilot.user.atlas.container.get_root_container_script(cmd)
Return the content of the root container script.
Parameters cmd – root command (string).
Returns script content (string).
pilot.user.atlas.container.is_release_setup(script, imagename)
Does the release_setup.sh file exist? This check can only be made for unpacked containers. These must have the
release setup file present, or setup will fail. For non-unpacked containers, the function will return True and the
pilot will assume that the container has the setup file.
Parameters
• script – release setup script (string).
• imagename – container/image name (string).
Returns Boolean.
pilot.user.atlas.container.remove_container_string(job_params)
Retrieve the container string from the job parameters
pilot.user.atlas.container.replace_last_command(cmd, replacement)
Replace the last command in cmd with given replacement.
Parameters
• cmd – command (string).
204
Chapter 7. System Architecture
PanDAWMS
• replacement – replacement (string).
Returns updated command (string).
pilot.user.atlas.container.set_platform(job, alrb_setup)
Set thePlatform variable and add it to the sub container command.
Parameters
• job – job object.
• alrb_setup – ALRB setup (string).
Returns updated ALRB setup (string).
pilot.user.atlas.container.singularity_wrapper(cmd, workdir, job=None)
Prepend the given command with the singularity execution command E.g. cmd = /bin/bash hello_world.sh -> sin-
gularity_command = singularity exec -B <bindmountsfromcatchall> <img> /bin/bash hello_world.sh singular-
ity exec -B <bindmountsfromcatchall> /cvmfs/atlas.cern.ch/repo/images/singularity/x86_64-slc6.img <script>
Note: if the job object is not set, then it is assumed that the middleware container is to be used.
Parameters
• cmd – command to be prepended (string).
• workdir – explicit work directory where the command should be executed (needs to be set
for Singularity) (string).
• job – job object.
Returns prepended command with singularity execution command (string).
pilot.user.atlas.container.update_alrb_setup(cmd, use_release_setup)
Update the ALRB setup command. Add the ALRB_CONT_SETUPFILE in case the release setup file was
created earlier (required available cvmfs).
Parameters
• cmd – full ALRB setup command (string).
• use_release_setup – should the release setup file be added to the setup command?
(Boolean).
Returns updated ALRB setup command (string).
pilot.user.atlas.container.update_for_user_proxy(_cmd, cmd, is_analysis=False)
Add the X509 user proxy to the container sub command string if set, and remove it from the main container
command. Try to receive payload proxy and update X509_USER_PROXY in container setup command In case
payload proxy from server is required, this function will also download and verify this proxy.
Parameters
• _cmd – container setup command (string).
• cmd – command the container will execute (string).
• is_analysis – True for user job (Boolean).
Returns exit_code (int), diagnostics (string), updated _cmd (string), updated cmd (string).
pilot.user.atlas.container.wrapper(executable, **kwargs)
Wrapper function for any container specific usage. This function will be called by pilot.util.container.execute()
and prepends the executable with a container command.
Parameters
• executable – command to be executed (string).
7.3. Pilot
205
PanDAWMS
• kwargs – dictionary of key-word arguments.
Returns executable wrapped with container command (string).
jobmetrics
pilot.user.atlas.jobmetrics.add_analytics_data(job_metrics, workdir, state)
Add the memory leak+chi2 analytics data to the job metrics.
Parameters
• job_metrics – job metrics (string).
• workdir – work directory (string).
• state – job state (string).
Returns updated job metrics (string).
pilot.user.atlas.jobmetrics.add_event_number(job_metrics, workdir)
Extract event number from file and add to job metrics if it exists
Parameters
• job_metrics – job metrics (string).
• workdir – work directory (string).
Returns updated job metrics (string).
pilot.user.atlas.jobmetrics.get_job_metrics(job)
Return a properly formatted job metrics string. The format of the job metrics string is defined by the server. It
will be reported to the server during updateJob.
Example of job metrics: Number of events read | Number of events written | vmPeak maximum | vmPeak aver-
age | RSS average | .. Format: nEvents=<int> nEventsW=<int> vmPeakMax=<int> vmPeakMean=<int> RSS-
Mean=<int> hs06=<float> shutdownTime=<int>
cpuFactor=<float> cpuLimit=<float> diskLimit=<float> jobStart=<int> memLimit=<int> run-
Limit=<float>
Parameters job – job object.
Returns job metrics (string).
pilot.user.atlas.jobmetrics.get_job_metrics_string(job)
Get the job metrics string.
Parameters job – job object.
Returns job metrics (string).
pilot.user.atlas.jobmetrics.get_number_in_string(line, pattern='\\ done\\ processing\\ event\\
\\#(\\d+)\\,')
Extract a number from the given string.
E.g. file eventLoopHeartBeat.txt contains done processing event #20166959, run #276689 22807 events read
so far <<<===
This function will return 20166959 as in int.
Parameters
• line – line from a file (string).
206
Chapter 7. System Architecture
PanDAWMS
• pattern – reg ex pattern (raw string).
Returns extracted number (int).
loopingjob_definitions
pilot.user.atlas.loopingjob_definitions.allow_loopingjob_detection()
Should the looping job detection algorithm be allowed? The looping job detection algorithm finds recently
touched files within the job’s workdir. If a found file has not been touched during the allowed time limit (see
looping job section in util/default.cfg), the algorithm will kill the job/payload process.
Returns boolean.
pilot.user.atlas.loopingjob_definitions.remove_unwanted_files(workdir, files)
Remove files from the list that are to be ignored by the looping job algorithm.
Parameters workdir – working directory (string). Needed in case the find command includes the
workdir in the list of
recently touched files. :param files: list of recently touched files (file names). :return: filtered files list.
memory
pilot.user.atlas.memory.allow_memory_usage_verifications()
Should memory usage verifications be performed?
Returns boolean.
pilot.user.atlas.memory.get_ucore_scale_factor(job)
Get the correction/scale factor for SCORE/4CORE/nCORE jobs on UCORE queues/
Parameters job – job object.
Returns scale factor (int).
pilot.user.atlas.memory.memory_usage(job)
Perform memory usage verification.
Parameters job – job object
Returns exit code (int), diagnostics (string).
nordugrid
class pilot.user.atlas.nordugrid.XMLDictionary(rootname='outfiles')
This is a helper class that is used to create the dictionary which is converted to the special XML files for Nordugrid
pilots. Example dictionary:
dictionary = { “outfiles”: [ { “file”: { “surl”: “some_surl”, “size”: “123”, “ad32”: “aaaaaaa”,
“guid”: “ababa22”, “lfn”: “some_lfn”, “dataset”: “some_dataset”, “date”:
“11/11/11” } },
{}, {}, ..
]
}
7.3. Pilot
207
PanDAWMS
Usage: xmldic = XMLDictionary() xmldic.add_to_list({“surl”:
“some_surl1”, “size”:
“123”, “ad32”:
“aaaaaaa”, “guid”: “ababa22”, “lfn”: “some_lfn”,
“dataset”: “some_dataset”, “date”: “11/11/11”})
dictionary = xmldic.get_dictionary()
__dict__ = mappingproxy({'__module__': 'pilot.user.atlas.nordugrid', '__doc__': '\n
This is a helper class that is used to create the dictionary which is converted to
the special XML files for\n Nordugrid pilots.\n Example dictionary:\n dictionary = {
"outfiles": [ { "file": { "surl": "some_surl", "size": "123", "ad32": "aaaaaaa",\n
"guid": "ababa22", "lfn": "some_lfn", "dataset": "some_dataset",\n "date":
"11/11/11" } },\n {}, {}, ..\n ]\n }\n\n Usage:\n xmldic = XMLDictionary()\n
xmldic.add_to_list({"surl": "some_surl1", "size": "123", "ad32": "aaaaaaa", "guid":
"ababa22", "lfn": "some_lfn",\n "dataset": "some_dataset", "date": "11/11/11"})\n
dictionary = xmldic.get_dictionary()\n ', '_dictionary': None, '__init__': <function
XMLDictionary.__init__>, 'add_to_list': <function XMLDictionary.add_to_list>,
'get_dictionary': <function XMLDictionary.get_dictionary>, '__dict__': <attribute
'__dict__' of 'XMLDictionary' objects>, '__weakref__': <attribute '__weakref__' of
'XMLDictionary' objects>, '__annotations__': {}})
__init__(rootname='outfiles')
Standard init function. :param rootname: name of the root key. There is only one root key in the Nordugrid
XML file (‘outfiles’).
__module__ = 'pilot.user.atlas.nordugrid'
__weakref__
list of weak references to the object (if defined)
_dictionary = None
add_to_list(dictionary, rootname='outfiles', itemname='file')
Add dictionary to itemname key. See example in class header. :param dictionary: dictionary to add to
itemname key. :param rootname: name of the root key. There is only one root key in the Nordugrid XML
file (‘outfiles’). :param itemname: name of the item key. In the Nordugrid XML it should be called ‘file’.
:return:
get_dictionary()
Return the dictionary to be converted to XML. It should be populated with the dictionary added to it in
add_to_list(). :return: dictionary
pilot.user.atlas.nordugrid.convert_to_prettyprint(xmlstr)
Convert XML to pretty print for older python versions (< 2.7). :param xmlstr: input XML string :return: XML
string (pretty printed)
pilot.user.atlas.nordugrid.convert_to_xml(dictionary)
Convert a dictionary to XML. The dictionary is expected to follow the Nordugrid format. See the XMLDictionary
helper class.
Example of XML (OutputFiles.xml):
<?xml version=”1.0” ?> <outfiles> <file>
<ad32>aaaaaaa</ad32>
<surl>some_surl1</surl>
<lfn>some_lfn</lfn>
<dataset>some_dataset</dataset> <date>11/11/11</date> <guid>ababa22</guid> <size>123</size>
</file> </outfiles>
Parameters dictionary – dictionary created with XMLDictionary.
208
Chapter 7. System Architecture
PanDAWMS
Returns xml (pretty printed for python >= 2.7 - for older python, use the convert_to_prettyprint()
function).
proxy
pilot.user.atlas.proxy.extract_time_left(stdout)
Extract the time left from the proxy command. Some processing on the stdout is done.
Parameters stdout – stdout (string).
Returns validity_end, stdout (int, string))
pilot.user.atlas.proxy.interpret_proxy_info(ec, stdout, stderr, limit)
Interpret the output from arcproxy or voms-proxy-info.
Parameters
• ec – exit code from proxy command (int).
• stdout – stdout from proxy command (string).
• stderr – stderr from proxy command (string).
• limit – time limit in hours (int).
Returns exit code (int), diagnostics (string). validity end in seconds if detected, None if not de-
tected(int)
pilot.user.atlas.proxy.verify_arcproxy(envsetup, limit, proxy_id='pilot', test=False)
Verify the proxy using arcproxy.
Parameters
• envsetup – general setup string for proxy commands (string).
• limit – time limit in hours (int).
• proxy_id – proxy unique id name. The verification result will be cached for this id. If None
the result will not be cached (string)
Returns exit code (int), error diagnostics (string).
pilot.user.atlas.proxy.verify_gridproxy(envsetup, limit)
Verify proxy using grid-proxy-info command.
Parameters
• envsetup – general setup string for proxy commands (string).
• limit – time limit in hours (int).
Returns exit code (int), error diagnostics (string).
pilot.user.atlas.proxy.verify_proxy(limit=None, x509=None, proxy_id='pilot', test=False)
Check for a valid voms/grid proxy longer than N hours. Use limit to set required time limit.
Parameters
• limit – time limit in hours (int).
• x509 – points to the proxy file. If not set (=None) - get proxy file from X509_USER_PROXY
environment
Returns exit code (NOPROXY or NOVOMSPROXY), diagnostics (error diagnostics string).
7.3. Pilot
209
PanDAWMS
pilot.user.atlas.proxy.verify_vomsproxy(envsetup, limit)
Verify proxy using voms-proxy-info command.
Parameters
• envsetup – general setup string for proxy commands (string).
• limit – time limit in hours (int).
Returns exit code (int), error diagnostics (string).
setup
pilot.user.atlas.setup.download_transform(url, transform_name, workdir)
Download the transform from the given url :param url: download URL with path to transform (string). :param
transform_name: trf name (string). :param workdir: work directory (string). :return:
pilot.user.atlas.setup.get_alrb_export(add_if=False)
Return the export command for the ALRB path if it exists. If the path does not exist, return empty string.
Parameters add_if – Boolean. True means that an if statement will be placed around the export.
Returns export command
pilot.user.atlas.setup.get_analysis_trf(transform, workdir)
Prepare to download the user analysis transform with curl. The function will verify the download location from
a known list of hosts.
Parameters
• transform – full trf path (url) (string).
• workdir – work directory (string).
Returns exit code (int), diagnostics (string), transform_name (string)
pilot.user.atlas.setup.get_asetup(asetup=True, alrb=False, add_if=False)
Define the setup for asetup, i.e. including full path to asetup and setting of ATLAS_LOCAL_ROOT_BASE Only
include the actual asetup script if asetup=True. This is not needed if the jobPars contain the payload command
but the pilot still needs to add the exports and the atlasLocalSetup.
Parameters
• asetup – Boolean. True value means that the pilot should include the asetup command.
• alrb – Boolean. True value means that the function should return special setup used with
ALRB and containers.
• add_if – Boolean. True means that an if statement will be placed around the export.
Raises NoSoftwareDir if appdir does not exist.
Returns source <path>/asetup.sh (string).
pilot.user.atlas.setup.get_asetup_options(release, homepackage)
Determine the proper asetup options. :param release: ATLAS release string. :param homepackage: ATLAS
homePackage string. :return: asetup options (string).
pilot.user.atlas.setup.get_file_system_root_path()
Return the root path of the local file system. The function returns “/cvmfs” or “/(some path)/cvmfs” in case the
expected file system root path is not where it usually is (e.g. on an HPC). A site can set the base path by exporting
ATLAS_SW_BASE.
Returns path (string)
210
Chapter 7. System Architecture
PanDAWMS
pilot.user.atlas.setup.get_payload_environment_variables(cmd, job_id, task_id, attempt_nr,
processing_type, site_name, analysis_job)
Return an array with enviroment variables needed by the payload.
Parameters
• cmd – payload execution command (string).
• job_id – PanDA job id (string).
• task_id – PanDA task id (string).
• attempt_nr – PanDA job attempt number (int).
• processing_type – processing type (string).
• site_name – site name (string).
• analysis_job – True for user analysis jobs, False otherwise (boolean).
Returns list of environment variables needed by the payload.
pilot.user.atlas.setup.get_valid_base_urls(order=None)
Return a list of valid base URLs from where the user analysis transform may be downloaded from. If order
is defined, return given item first. E.g. order=http://atlpan.web.cern.ch/atlpan -> [‘http://atlpan.web.cern.ch/
atlpan’, ...] NOTE: the URL list may be out of date.
Parameters order – order (string).
Returns valid base URLs (list).
pilot.user.atlas.setup.get_writetoinput_filenames(writetofile)
Extract the writeToFile file name(s). writeToFile=’tmpin_mc16_13TeV.blah:AOD.15760866._000002.pool.root.1’
-> return ‘tmpin_mc16_13TeV.blah’
Parameters writetofile – string containing file name information.
Returns list of file names
pilot.user.atlas.setup.is_standard_atlas_job(release)
Is it a standard ATLAS job? A job is a standard ATLAS job if the release string begins with ‘Atlas-‘.
Parameters release – Release value (string).
Returns Boolean. Returns True if standard ATLAS job.
pilot.user.atlas.setup.replace_lfns_with_turls(cmd, workdir, filename, infiles, writetofile='')
Replace all LFNs with full TURLs in the payload execution command.
This function is used with direct access in production jobs. Athena requires a full TURL instead of LFN.
Parameters
• cmd – payload execution command (string).
• workdir – location of metadata file (string).
• filename – metadata file name (string).
• infiles – list of input files.
• writetofile –
Returns updated cmd (string).
pilot.user.atlas.setup.set_inds(dataset)
Set the INDS environmental variable used by runAthena.
7.3. Pilot
211
PanDAWMS
Parameters dataset – dataset for input files (realDatasetsIn) (string).
Returns
pilot.user.atlas.setup.should_pilot_prepare_setup(noexecstrcnv, jobpars, imagename=None)
Determine whether the pilot should add the setup to the payload command or not. The pilot will not add asetup
if jobPars already contain the information (i.e. it was set by the payload creator). If noExecStrCnv is set, then
jobPars is expected to contain asetup.sh + options If a stand-alone container / user defined container is used, pilot
should not prepare asetup.
Parameters
• noexecstrcnv – boolean.
• jobpars – job parameters (string).
• imagename – container image (string).
Returns boolean.
utilities
pilot.user.atlas.utilities.convert_text_file_to_dictionary(path)
Convert row-column text file to dictionary. User first row identifiers as dictionary keys. Note: file must follow
the convention:
NAME1 NAME2 .. value1 value2 .. .. .. ..
Parameters path – path to file (string).
Returns dictionary.
pilot.user.atlas.utilities.convert_unicode_string(unicode_string)
Convert a unicode string into str.
Parameters unicode_string –
Returns string.
pilot.user.atlas.utilities.get_average_summary_dictionary(path)
Loop over the memory monitor output file and create the averaged summary dictionary.
Parameters path – path to memory monitor txt output file (string).
Returns summary dictionary.
pilot.user.atlas.utilities.get_average_summary_dictionary_prmon(path)
Loop over the memory monitor output file and create the averaged summary dictionary.
prmon keys: ‘Time’, ‘nprocs’, ‘nthreads’, ‘pss’, ‘rchar’, ‘read_bytes’, ‘rss’, ‘rx_bytes’, ‘rx_packets’, ‘stime’,
‘swap’, ‘tx_bytes’, ‘tx_packets’, ‘utime’, ‘vmem’, ‘wchar’, ‘write_bytes’, ‘wtime’
The function uses the first line in the output file to define the dictionary keys used later in the function. This
means that any change in the format such as new columns will be handled automatically.
Parameters path – path to memory monitor txt output file (string).
Returns summary dictionary.
pilot.user.atlas.utilities.get_benchmark_setup(job)
Return the proper setup for the benchmark command.
Parameters job – job object.
212
Chapter 7. System Architecture
PanDAWMS
Returns setup string for the benchmark command.
pilot.user.atlas.utilities.get_last_value(value_list)
pilot.user.atlas.utilities.get_max_memory_monitor_value(value, maxvalue, totalvalue)
Return the max and total value (used by memory monitoring). Return an error code, 1, in case of value error.
Parameters
• value – value to be tested (integer).
• maxvalue – current maximum value (integer).
• totalvalue – total value (integer).
Returns exit code, maximum and total value (tuple of integers).
pilot.user.atlas.utilities.get_memory_monitor_info(workdir, allowtxtfile=False, name='')
Add the utility info to the node structure if available.
Parameters
• workdir – relevant work directory (string).
• allowtxtfile – boolean attribute to allow for reading the raw memory monitor output.
• name – name of memory monitor (string).
Returns node structure (dictionary).
pilot.user.atlas.utilities.get_memory_monitor_info_path(workdir, allowtxtfile=False)
Find the proper path to the utility info file Priority order:
1. JSON summary file from workdir
2. JSON summary file from pilot initdir
3. Text output file from workdir (if allowtxtfile is True)
Parameters
• workdir – relevant work directory (string).
• allowtxtfile – boolean attribute to allow for reading the raw memory monitor output.
Returns path (string).
pilot.user.atlas.utilities.get_memory_monitor_output_filename(suffix='txt')
Return the filename of the memory monitor text output file.
Returns File name (string).
pilot.user.atlas.utilities.get_memory_monitor_setup(pid, pgrp, jobid, workdir, command, setup='',
use_container=True, transformation='',
outdata=None, dump_ps=False)
Return the proper setup for the memory monitor. If the payload release is provided, the memory monitor can be
setup with the same release. Until early 2018, the memory monitor was still located in the release area. After
many problems with the memory monitor, it was decided to use a fixed version for the setup. Currently, release
21.0.22 is used.
Parameters
• pid – job process id (int).
• pgrp – process group id (int).
• jobid – job id (int).
7.3. Pilot
213
PanDAWMS
• workdir – job work directory (string).
• command – payload command (string).
• setup – optional setup in case asetup can not be used, which uses infosys (string).
• use_container – optional boolean.
• transformation – optional name of transformation, e.g. Sim_tf.py (string).
• outdata – optional list of output fspec objects (list).
• dump_ps – should ps output be dumped when identifying prmon process? (Boolean).
Returns job work directory (string), pid for process inside container (int).
pilot.user.atlas.utilities.get_memory_monitor_setup_old(pid, pgrp, jobid, workdir, command,
setup='', use_container=True,
transformation='', outdata=None,
dump_ps=False)
Return the proper setup for the memory monitor. If the payload release is provided, the memory monitor can be
setup with the same release. Until early 2018, the memory monitor was still located in the release area. After
many problems with the memory monitor, it was decided to use a fixed version for the setup. Currently, release
21.0.22 is used.
Parameters
• pid – job process id (int).
• pgrp – process group id (int).
• jobid – job id (int).
• workdir – job work directory (string).
• command – payload command (string).
• setup – optional setup in case asetup can not be used, which uses infosys (string).
• use_container – optional boolean.
• transformation – optional name of transformation, e.g. Sim_tf.py (string).
• outdata – optional list of output fspec objects (list).
• dump_ps – should ps output be dumped when identifying prmon process? (Boolean).
Returns job work directory (string), pid for process inside container (int).
pilot.user.atlas.utilities.get_memory_monitor_summary_filename(selector=None)
Return the name for the memory monitor summary file.
Parameters selector – special conditions flag (boolean).
Returns File name (string).
pilot.user.atlas.utilities.get_memory_values(workdir, name='')
Find the values in the memory monitor output file.
In case the summary JSON file has not yet been produced, create a summary dictionary with the same format
using the output text file (produced by the memory monitor and which is updated once per minute).
FORMAT:
{“Max”:{“maxVMEM”:40058624,”maxPSS”:10340177,”maxRSS”:16342012,”maxSwap”:16235568},
“Avg”:{“avgVMEM”:19384236,”avgPSS”:5023500,”avgRSS”:6501489,”avgSwap”:5964997},
“Other”:{“rchar”:NN,”wchar”:NN,”rbytes”:NN,”wbytes”:NN}}
214
Chapter 7. System Architecture
PanDAWMS
Parameters
• workdir – relevant work directory (string).
• name – name of memory monitor (string).
Returns memory values dictionary.
pilot.user.atlas.utilities.get_metadata_dict_from_txt(path, storejson=False, jobid=None)
Convert memory monitor text output to json, store it, and return a selection as a dictionary.
Parameters
• path –
• storejson – store dictionary on disk if True (boolean).
• jobid – job id (string).
Returns prmon metadata (dictionary).
pilot.user.atlas.utilities.get_network_monitor_setup(setup, job)
Return the proper setup for the network monitor. The network monitor is currently setup together with the payload
and is start before it. The payload setup should therefore be provided. The network monitor setup is prepended
to it.
Parameters
• setup – payload setup string.
• job – job object.
Returns network monitor setup string.
pilot.user.atlas.utilities.get_pid_for_command(ps, command='python pilot2/pilot.py')
Return the process id for the given command and user. The function returns 0 in case pid could not be found. If
no command is specified, the function looks for the “python pilot2/pilot.py” command in the ps output.
Parameters
• ps – ps command output (string).
• command – command string expected to be in ps output (string).
Returns pid (int) or None if no such process.
pilot.user.atlas.utilities.get_pid_for_jobid(ps, jobid)
Return the process id for the ps entry that contains the job id.
Parameters
• ps – ps command output (string).
• jobid – PanDA job id (int).
Returns pid (int) or None if no such process.
pilot.user.atlas.utilities.get_pid_for_trf(ps, transformation, outdata)
Return the process id for the given command and user. Note: function returns 0 in case pid could not be found.
Parameters
• ps – ps command output (string).
• transformation – transformation name, e.g. Sim_tf.py (String).
• outdata – fspec objects (list).
7.3. Pilot
215
PanDAWMS
Returns pid (int) or None if no such process.
pilot.user.atlas.utilities.get_prefetcher_setup(job)
Return the proper setup for the Prefetcher. Prefetcher is a tool used with the Event Streaming Service.
Parameters job – job object.
Returns setup string for the Prefetcher command.
pilot.user.atlas.utilities.get_proper_pid(pid, pgrp, jobid, command='', transformation='', outdata='',
use_container=True, dump_ps=False)
Return a pid from the proper source to be used with the memory monitor. The given pid comes from Popen(),
but in the case containers are used, the pid should instead come from a ps aux lookup. If the main process has
finished before the proper pid has been identified (it will take time if the payload is running inside a container),
then this function will abort and return -1. The called should handle this and not launch the memory monitor as
it is not needed any longer.
Parameters
• pid – process id (int).
• pgrp – process group id (int).
• jobid – job id (int).
• command – payload command (string).
• transformation – optional name of transformation, e.g. Sim_tf.py (string).
• outdata – list of output fspec object (list).
• use_container – optional boolean.
Returns pid (int).
pilot.user.atlas.utilities.get_ps_info(pgrp, whoami=None, options='axfo pid,user,args')
Return ps info for the given user.
Parameters
• pgrp – process group id (int).
• whoami – user name (string).
Returns ps aux for given user (string).
pilot.user.atlas.utilities.get_trf_command(command, transformation='')
Return the last command in the full payload command string. Note: this function returns the last command in
job.command which is only set for containers.
Parameters
• command – full payload command (string).
• transformation – optional name of transformation, e.g. Sim_tf.py (string).
Returns trf command (string).
pilot.user.atlas.utilities.post_memory_monitor_action(job)
Perform post action items for memory monitor.
Parameters job – job object.
Returns
pilot.user.atlas.utilities.precleanup()
Pre-cleanup at the beginning of the job to remove any pre-existing files from previous jobs in the main work dir.
216
Chapter 7. System Architecture
PanDAWMS
Returns
generic components
common
pilot.user.generic.common.allow_timefloor(submitmode)
Should the timefloor mechanism (multi-jobs) be allowed for the given submit mode?
Parameters submitmode – submit mode (string).
pilot.user.generic.common.get_analysis_run_command(job, trf_name)
Return the proper run command for the user job.
Example output: export X509_USER_PROXY=<..>;./runAthena <job parameters> –usePFCTurl –directIn
Parameters
• job – job object.
• trf_name – name of the transform that will run the job (string). Used when containers are
not used.
Returns command (string).
pilot.user.generic.common.get_metadata(workdir)
Return the metadata from file.
Parameters workdir – work directory (string)
Returns
pilot.user.generic.common.get_payload_command(job)
Return the full command for executing the payload, including the sourcing of all setup files and setting of envi-
ronment variables.
By default, the full payload command is assumed to be in the job.jobparams.
Parameters job – job object
Returns command (string)
pilot.user.generic.common.get_utility_command_execution_order(name)
Should the given utility command be executed before or after the payload?
Parameters name – utility name (string).
Returns execution
order
constant
(UTILITY_BEFORE_PAYLOAD
or
UTIL-
ITY_AFTER_PAYLOAD_STARTED)
pilot.user.generic.common.get_utility_command_kill_signal(name)
Return the proper kill signal used to stop the utility command.
Parameters name –
Returns kill signal
pilot.user.generic.common.get_utility_command_output_filename(name, selector=None)
Return the filename to the output of the utility command.
Parameters
• name – utility name (string).
• selector – optional special conditions flag (boolean).
7.3. Pilot
217
PanDAWMS
Returns filename (string).
pilot.user.generic.common.get_utility_command_setup(name, setup=None)
Return the proper setup for the given utility command. If a payload setup is specified :param name: :param setup:
:return:
pilot.user.generic.common.get_utility_commands(order=None, job=None)
Return a dictionary of utility commands and arguments to be executed in parallel with the payload.
This could e.g.
be memory and network monitor commands.
A separate function can be used to
determine the corresponding command setups using the utility command name.
If the optional or-
der parameter is set, the function should return the list of corresponding commands.
E.g.
if or-
der=UTILITY_BEFORE_PAYLOAD, the function should return all commands that are to be executed before the
payload. If order=UTILITY_WITH_PAYLOAD, the corresponding commands will be prepended to the payload
execution string. If order=UTILITY_AFTER_PAYLOAD_STARTED, the commands that should be executed
after the payload has been started should be returned.
FORMAT: {‘command’: <command>, ‘args’: <args>}
Parameters
• order – optional sorting order (see pilot.util.constants)
• job – optional job object.
Returns dictionary of utilities to be executed in parallel with the payload.
pilot.user.generic.common.post_prestagein_utility_command(**kwargs)
Execute any post pre-stage-in utility commands.
Parameters kwargs – kwargs (dictionary).
Returns
pilot.user.generic.common.post_utility_command_action(name, job)
Perform post action for given utility command.
Parameters
• name – name of utility command (string).
• job – job object.
Returns
pilot.user.generic.common.process_debug_command(debug_command, pandaid)
In debug mode, the server can send a special debug command to the pilot via the updateJob backchannel. This
function can be used to process that command, i.e. to identify a proper pid to debug (which is unknown to the
server).
Parameters
• debug_command – debug command (string), payload pid (int).
• pandaid – PanDA id (string).
Returns updated debug command (string)
pilot.user.generic.common.remove_redundant_files(workdir, outputfiles=[], islooping=False,
debugmode=False)
Remove redundant files and directories prior to creating the log file.
Parameters
• workdir – working directory (string).
• outputfiles – list of output files.
218
Chapter 7. System Architecture
PanDAWMS
• islooping – looping job variable to make sure workDir is not removed in case of looping
(boolean).
Returns
pilot.user.generic.common.sanity_check()
Perform an initial sanity check before doing anything else in a given workflow. This function can be used to
verify importing of modules that are otherwise used much later, but it is better to abort the pilot if a problem is
discovered early.
Returns exit code (0 if all is ok, otherwise non-zero exit code).
pilot.user.generic.common.update_job_data(job)
This function can be used to update/add data to the job object. E.g. user specific information can be extracted
from other job object fields. In the case of ATLAS, information is extracted from the metaData field and added
to other job object fields.
Parameters job – job object
Returns
pilot.user.generic.common.update_server(job)
Perform any user specific server actions.
E.g. this can be used to send special information to a logstash.
Parameters job – job object.
Returns
pilot.user.generic.common.update_stagein(job)
In case special files need to be skipped during stage-in, the job.indata list can be updated here. See ATLAS code
for an example.
Parameters job – job object.
Returns
pilot.user.generic.common.validate(job)
Perform user specific payload/job validation.
Parameters job – job object.
Returns Boolean (True if validation is successful).
pilot.user.generic.common.verify_job(job)
Verify job parameters for specific errors. Note:
in case of problem,
the function should set the corresponding pilot error code using
job.piloterrorcodes, job.piloterrordiags = errors.add_error_code(error.get_error_code())
Parameters job – job object
Returns Boolean.
7.3. Pilot
219
PanDAWMS
container
pilot.user.generic.container.create_stagein_container_command(workdir, cmd)
Create the stage-in container command.
The function takes the isolated stage-in command, adds bits and pieces needed for the containerisation and stores
it in a stagein.sh script file. It then generates the actual command that will execute the stage-in script in a container.
Parameters
• workdir – working directory where script will be stored (string).
• cmd – isolated stage-in command (string).
Returns container command to be executed (string).
pilot.user.generic.container.do_use_container(**kwargs)
Decide whether to use a container or not.
Parameters kwargs – dictionary of key-word arguments.
Returns True is function has decided that a container should be used, False otherwise (boolean).
pilot.user.generic.container.wrapper(executable, **kwargs)
Wrapper function for any container specific usage. This function will be called by pilot.util.container.execute()
and prepends the executable with a container command.
Parameters
• executable – command to be executed (string).
• kwargs – dictionary of key-word arguments.
Returns executable wrapped with container command (string).
generic
pilot.user.generic.jobmetrics.get_job_metrics(job)
Return a properly formatted job metrics string. The format of the job metrics string is defined by the server. It
will be reported to the server during updateJob.
Example of job metrics: Number of events read | Number of events written | vmPeak maximum | vmPeak aver-
age | RSS average | .. Format: nEvents=<int> nEventsW=<int> vmPeakMax=<int> vmPeakMean=<int> RSS-
Mean=<int> hs06=<float> shutdownTime=<int>
cpuFactor=<float> cpuLimit=<float> diskLimit=<float> jobStart=<int> memLimit=<int> run-
Limit=<float>
Parameters job – job object.
Returns job metrics (string).
220
Chapter 7. System Architecture
PanDAWMS
loopingjob_definitions
pilot.user.generic.loopingjob_definitions.allow_loopingjob_detection()
Should the looping job detection algorithm be allowed? The looping job detection algorithm finds recently
touched files within the job’s workdir. If a found file has not been touched during the allowed time limit (see
looping job section in util/default.cfg), the algorithm will kill the job/payload process.
Returns boolean.
pilot.user.generic.loopingjob_definitions.remove_unwanted_files(workdir, files)
Remove files from the list that are to be ignored by the looping job algorithm.
Parameters workdir – working directory (string). Needed in case the find command includes the
workdir in the list of
recently touched files. :param files: list of recently touched files (file names). :return: filtered files list.
memory
pilot.user.generic.memory.allow_memory_usage_verifications()
Should memory usage verifications be performed?
Returns boolean.
pilot.user.generic.memory.memory_usage(job)
Perform memory usage verification.
Parameters job – job object
Returns exit code (int), diagnostics (string).
proxy
pilot.user.generic.proxy.verify_proxy(limit=None)
Check for a valid voms/grid proxy longer than N hours. Use limit to set required time limit.
Parameters limit – time limit in hours (int).
Returns exit code (NOPROXY or NOVOMSPROXY), diagnostics (error diagnostics string).
generic
pilot.user.generic.setup.download_transform(url, transform_name, workdir)
Download the transform from the given url :param url: download URL with path to transform (string). :param
transform_name: trf name (string). :param workdir: work directory (string). :return:
pilot.user.generic.setup.get_analysis_trf(transform, workdir)
Prepare to download the user analysis transform with curl. The function will verify the download location from
a known list of hosts.
Parameters
• transform – full trf path (url) (string).
• workdir – work directory (string).
Returns exit code (int), diagnostics (string), transform_name (string)
7.3. Pilot
221
PanDAWMS
pilot.user.generic.setup.get_valid_base_urls(order=None)
Return a list of valid base URLs from where the user analysis transform may be downloaded from. If order
is defined, return given item first. E.g. order=http://atlpan.web.cern.ch/atlpan -> [‘http://atlpan.web.cern.ch/
atlpan’, ...] NOTE: the URL list may be out of date.
Parameters order – order (string).
Returns valid base URLs (list).
util components
auxiliary
pilot.util.auxiliary.check_for_final_server_update(update_server)
Do not set graceful stop if pilot has not finished sending the final job update i.e. wait until SERVER_UPDATE
is DONE_FINAL. This function sleeps for a maximum of 20*30 s until SERVER_UPDATE env variable has
been set to SERVER_UPDATE_FINAL.
Parameters update_server – args.update_server boolean.
Returns
pilot.util.auxiliary.convert_to_pilot_error_code(exit_code)
This conversion function is used to revert a batch system exit code back to a pilot error code. Note: the function
is used by Harvester.
Parameters exit_code – batch system exit code (int).
Returns pilot error code (int).
pilot.util.auxiliary.cut_output(txt, cutat=1024, separator='\n[...]\n')
Cut the given string if longer that 2*cutat value.
Parameters
• txt – text to be cut at position cutat (string).
• cutat – max length of uncut text (int).
• separator – separator text (string).
Returns cut text (string).
pilot.util.auxiliary.display_architecture_info()
Display OS/architecture information. The function attempts to use the lsb_release -a command if available. If
that is not available, it will dump the contents of
Returns
pilot.util.auxiliary.extract_memory_usage_value(output)
Extract the memory usage value from the ps output (in kB).
# USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND # usatlas1 13917 1.5 0.0
1324968 152832 ? Sl 09:33 2:55 /bin/python2 .. # -> 152832 (kB)
Parameters output – ps output (string).
Returns memory value in kB (int).
pilot.util.auxiliary.get_batchsystem_jobid()
Identify and return the batch system job id (will be reported to the server)
Returns batch system job id
222
Chapter 7. System Architecture
PanDAWMS
pilot.util.auxiliary.get_display_info()
Extract the product and vendor from the lshw command. E.g.
product: GD 5446 [1013:B8] vendor: Cirrus Logic [1013]
-> GD 5446, Cirrus Logic
Returns product (string), vendor (string).
pilot.util.auxiliary.get_error_code_translation_dictionary()
Define the error code translation dictionary.
Returns populated error code translation dictionary.
pilot.util.auxiliary.get_job_scheduler_id()
Get the job scheduler id from the environment variable PANDA_JSID
Returns job scheduler id (string)
pilot.util.auxiliary.get_key_value(catchall, key='SOMEKEY')
Return the value corresponding to key in catchall. :param catchall: catchall free string. :param key: key name
(string). :return: value (string).
pilot.util.auxiliary.get_logger(job_id, log=None)
Return the logger object. Use this function to get the proper logger object. It relies on a python 2.7 function,
getChild(), but if the queue is only using Python 2.6, the standard logger object will be returned instead.
WARNING: it seems using this function can lead to severe memory leaks (multiple GB) in some jobs. Do not
use. Keep this definition for possible later investigation.
Parameters jod_id – PanDA job id (string).
Returns logger object.
pilot.util.auxiliary.get_memory_usage(pid)
Return the memory usage string (ps auxf <pid>) for the given process.
Parameters pid – process id (int).
Returns ps exit code (int), stderr (strint), stdout (string).
pilot.util.auxiliary.get_object_size(obj, seen=None)
Recursively find the size of any objects
Parameters obj – object.
pilot.util.auxiliary.get_pid_from_command(cmd, pattern='gdb --pid (\\d+)')
Identify an explicit process id in the given command.
Example: cmd = ‘gdb –pid 19114 -ex ‘generate-core-file” -> pid = 19114
Parameters
• cmd – command containing a pid (string).
• pattern – regex pattern (raw string).
Returns pid (int).
pilot.util.auxiliary.get_pilot_id()
Get the pilot id from the environment variable GTAG
Returns pilot id (string)
7.3. Pilot
223
PanDAWMS
pilot.util.auxiliary.get_pilot_state(job=None)
Return the current pilot (job) state.
If the job object does not exist, the environmental variable PI-
LOT_JOB_STATE will be queried instead.
Parameters job –
Returns pilot (job) state (string).
pilot.util.auxiliary.get_resource_name()
Return the name of the resource (only set for HPC resources; e.g. Cori, otherwise return ‘grid’).
Returns resource_name (string).
pilot.util.auxiliary.get_size(obj_0)
Recursively iterate to sum size of object & members. Note: for size measurement to work, the object must have
set the data members in the __init__().
Parameters obj_0 – object to be measured.
Returns size in Bytes (int).
pilot.util.auxiliary.has_instruction_set(instruction_set)
Determine whether a given CPU instruction set is available. The function will use grep to search in /proc/cpuinfo
(both in upper and lower case).
Parameters instruction_set – instruction set (e.g. AVX2) (string).
Returns Boolean
pilot.util.auxiliary.has_instruction_sets(instruction_sets)
Determine whether a given list of CPU instruction sets is available. The function will use grep to search in
/proc/cpuinfo (both in upper and lower case). Example: instruction_sets = [‘AVX’, ‘AVX2’, ‘SSE4_2’, ‘XXX’]
-> “AVX|AVX2|SSE4_2” :param instruction_sets: instruction set (e.g. AVX2) (string). :return: Boolean
pilot.util.auxiliary.is_python3()
Check if we are running on Python 3.
Returns boolean.
pilot.util.auxiliary.is_string(obj)
Determine if the passed object is a string or not.
Parameters obj – object (object type).
Returns True if obj is a string (Boolean).
pilot.util.auxiliary.is_virtual_machine()
Are we running in a virtual machine? If we are running inside a VM, then linux will put ‘hypervisor’ in cpuinfo.
This function looks for the presence of that.
Returns boolean.
pilot.util.auxiliary.list_hardware()
Execute lshw to list local hardware.
Returns lshw output (string).
pilot.util.auxiliary.locate_core_file(cmd=None, pid=None)
Locate the core file produced by gdb.
Parameters
• cmd – optional command containing pid corresponding to core file (string).
• pid – optional pid to use with core file (core.pid) (int).
224
Chapter 7. System Architecture
PanDAWMS
Returns path to core file (string).
pilot.util.auxiliary.pilot_version_banner()
Print a pilot version banner.
Returns
pilot.util.auxiliary.set_pilot_state(job=None, state='')
Set the internal pilot state. Note: this function should update the global/singleton object but currently uses
an environmental variable (PILOT_JOB_STATE). The function does not update job.state if it is already set to
finished or failed. The environmental variable PILOT_JOB_STATE will always be set, in case the job object
does not exist.
Parameters
• job – optional job object.
• state – internal pilot state (string).
Returns
pilot.util.auxiliary.shell_exit_code(exit_code)
Translate the pilot exit code to a proper exit code for the shell (wrapper). Any error code that is to be converted
by this function, should be added to the traces object like:
traces.pilot[‘error_code’] = errors.<ERRORCODE>
The traces object will be checked by the pilot module.
Parameters exit_code – pilot error code (int).
Returns standard shell exit code (int).
pilot.util.auxiliary.show_memory_usage()
Display the current memory usage by the pilot process.
Returns
pilot.util.auxiliary.whoami()
Return the name of the pilot user.
Returns whoami output (string).
config
class pilot.util.config._ConfigurationSection
Keep the settings for a section of the configuration file
__dict__ = mappingproxy({'__module__': 'pilot.util.config', '__doc__': '\n Keep the
settings for a section of the configuration file\n ', '__getitem__': <function
_ConfigurationSection.__getitem__>, '__repr__': <function
_ConfigurationSection.__repr__>, '__getattr__': <function
_ConfigurationSection.__getattr__>, '__dict__': <attribute '__dict__' of
'_ConfigurationSection' objects>, '__weakref__': <attribute '__weakref__' of
'_ConfigurationSection' objects>, '__annotations__': {}})
__getattr__(attr)
__getitem__(item)
__module__ = 'pilot.util.config'
7.3. Pilot
225
PanDAWMS
__repr__()
Return repr(self).
__weakref__
list of weak references to the object (if defined)
pilot.util.config.read(config_file)
Read the settings from file and return a dot notation object
constants
container
pilot.util.container.containerise_executable(executable, **kwargs)
Wrap the containerisation command around the executable.
Parameters
• executable – command to be wrapper (string).
• kwargs – kwargs dictionary.
Returns containerised executable (list).
pilot.util.container.execute(executable, **kwargs)
Execute the command and its options in the provided executable list. The function also determines whether the
command should be executed within a container.
Parameters
• executable – command to be executed (string or list).
• returnproc) (kwargs (timeout, usecontainer,) –
Returns exit code, stdout and stderr (or process if requested via returnproc argument)
pilot.util.container.is_python3()
Check if we are running on Python 3.
Returns boolean.
disk
pilot.util.disk._ntuple_diskusage
alias of pilot.util.disk.usage
pilot.util.disk.disk_usage(path)
Return disk usage statistics about the given path as a (total, used, free) namedtuple. Values are expressed in
bytes.
226
Chapter 7. System Architecture
PanDAWMS
filehandling
pilot.util.filehandling._define_tabledict_keys(header, fields, separator)
Define the keys for the tabledict dictionary. Note: this function is only used by parse_table_from_file().
Parameters
• header – header string.
• fields – header content string.
• separator – separator character (char).
Returns tabledict (dictionary), keylist (ordered list with dictionary key names).
pilot.util.filehandling.add_to_total_size(path, total_size)
Add the size of file in the given path to the total size of all in/output files.
Parameters
• path – path to file (string).
• total_size – prior total size of all input/output files (long).
Returns total size of all input/output files (long).
pilot.util.filehandling.calculate_adler32_checksum(filename)
Calculate the adler32 checksum for the given file. The file is assumed to exist.
Parameters filename – file name (string).
Returns checksum value (string).
pilot.util.filehandling.calculate_checksum(filename, algorithm='adler32')
Calculate the checksum value for the given file. The default algorithm is adler32. Md5 is also be supported.
Valid algorithms are 1) adler32/adler/ad32/ad, 2) md5/md5sum/md.
Parameters
• filename – file name (string).
• algorithm – optional algorithm string.
Raises FileHandlingFailure, NotImplementedError – exception raised when file does not
exist or for unknown algorithm.
Returns checksum value (string).
pilot.util.filehandling.calculate_md5_checksum(filename)
Calculate the md5 checksum for the given file. The file is assumed to exist.
Parameters filename – file name (string).
Returns checksum value (string).
pilot.util.filehandling.convert(data)
Convert unicode data to utf-8.
Usage examples: 1. Dictionary:
data = {u’Max’: {u’maxRSS’: 3664, u’maxSwap’: 0, u’maxVMEM’: 142260, u’maxPSS’: 1288}, u’Avg’:
{u’avgVMEM’: 94840, u’avgPSS’: 850, u’avgRSS’: 2430, u’avgSwap’: 0}}
convert(data)
{‘Max’: {‘maxRSS’: 3664, ‘maxSwap’: 0, ‘maxVMEM’: 142260, ‘maxPSS’: 1288}, ‘Avg’: {‘avgVMEM’: 94840,
‘avgPSS’: 850, ‘avgRSS’: 2430, ‘avgSwap’: 0}}
7.3. Pilot
227
PanDAWMS
2. String:
data = u’hello’
convert(data) ‘hello’
3. List:
data = [u’1’,u’2’,’3’]
convert(data) [‘1’, ‘2’, ‘3’]
Parameters data – unicode object to be converted to utf-8
Returns converted data to utf-8
pilot.util.filehandling.copy(path1, path2)
Copy path1 to path2.
Parameters
• path1 – file path (string).
• path2 – file path (string).
Raises PilotException – FileHandlingFailure, NoSuchFile
Returns
pilot.util.filehandling.copy_pilot_source(workdir)
Copy the pilot source into the work directory.
Parameters workdir – working directory (string).
Returns diagnostics (string).
pilot.util.filehandling.create_symlink(from_path='', to_path='')
Create a symlink from/to the given paths.
Parameters
• from_path – from path (string).
• to_path – to path (string).
pilot.util.filehandling.dump(path, cmd='cat')
Dump the content of the file in the given path to the log.
Parameters
• path – file path (string).
• cmd – optional command (string).
Returns cat (string).
pilot.util.filehandling.establish_logging(debug=True, nopilotlog=False, filename='pilotlog.txt',
loglevel=0)
Setup and establish logging.
Option loglevel can be used to decide which (predetermined) logging format to use. Example:
228
Chapter 7. System Architecture
PanDAWMS
loglevel=0:
‘%(asctime)s | %(levelname)-8s | %(name)-32s | %(funcName)-25s | %(mes-
sage)s’ loglevel=1: ‘ts=%(asctime)s level=%(levelname)-8s event=%(name)-32s.%(funcName)-25s
msg=”%(message)s”’
Parameters
• debug – debug mode (Boolean),
• nopilotlog – True when pilot log is not known (Boolean).
• filename – name of log file (string).
• loglevel – selector for logging level (int).
Returns
pilot.util.filehandling.find_executable(name)
Is the command ‘name’ available locally?
Parameters name – command name (string).
Returns full path to command if it exists, otherwise empty string.
pilot.util.filehandling.find_last_line(filename)
Find the last line in a (not too large) file.
Parameters filename – file name, full path (string).
Returns last line (string).
pilot.util.filehandling.find_latest_modified_file(list_of_files)
Find the most recently modified file among the list of given files. In case int conversion of getmtime() fails,
int(time.time()) will be returned instead.
Parameters list_of_files – list of files with full paths.
Returns most recently updated file (string), modification time (int).
pilot.util.filehandling.find_text_files()
Find all non-binary files.
Returns list of files.
pilot.util.filehandling.get_checksum_type(checksum)
Return the checksum type (ad32 or md5). The given checksum can be either be a standard ad32 or md5 value,
or a dictionary with the format { checksum_type: value } as defined in the FileSpec class. In case the checksum
type cannot be identified, the function returns ‘unknown’.
Parameters checksum – checksum string or dictionary.
Returns checksum type (string).
pilot.util.filehandling.get_checksum_value(checksum)
Return the checksum value. The given checksum might either be a standard ad32 or md5 string, or a dictionary
with the format { checksum_type: value } as defined in the FileSpec class. This function extracts the checksum
value from this dictionary (or immediately returns the checksum value if the given value is a string).
Parameters checksum – checksum object (string or dictionary).
Returns checksum. checksum string.
pilot.util.filehandling.get_disk_usage(start_path='.')
Calculate the disk usage of the given directory (including any sub-directories).
Parameters start_path – directory (string).
7.3. Pilot
229
PanDAWMS
Returns disk usage in bytes (int).
pilot.util.filehandling.get_files(pattern='*.log')
Find all files whose names follow the given pattern.
Parameters pattern – file name pattern (string).
Returns list of files.
pilot.util.filehandling.get_guid()
Generate a GUID using the uuid library. E.g. guid = ‘92008FAF-BE4C-49CF-9C5C-E12BC74ACD19’
Returns a random GUID (string)
pilot.util.filehandling.get_local_file_size(filename)
Get the file size of a local file.
Parameters filename – file name (string).
Returns file size (int).
pilot.util.filehandling.get_nonexistant_path(fname_path)
Get the path to a filename which does not exist by incrementing path.
Parameters fname_path – file name path (string).
Returns file name path (string).
pilot.util.filehandling.get_pilot_work_dir(workdir)
Return the full path to the main PanDA Pilot work directory. Called once at the beginning of the batch job.
Parameters workdir – The full path to where the main work directory should be created
Returns The name of main work directory
pilot.util.filehandling.get_table_from_file(filename, header=None, separator='\t',
convert_to_float=True)
Extract a table of data from a txt file. E.g. header=”Time VMEM PSS RSS Swap rchar wchar rbytes wbytes” or
the first line in the file is Time VMEM PSS RSS Swap rchar wchar rbytes wbytes each of which will become keys
in the dictionary, whose corresponding values are stored in lists, with the entries corresponding to the values in
the rows of the input file.
The output dictionary will have the format {‘Time’: [ .. data from first row .. ], ‘VMEM’: [.. data from second
row], ..}
Parameters
• filename – name of input text file, full path (string).
• header – header string.
• separator – separator character (char).
• convert_to_float – boolean, if True, all values will be converted to floats.
Returns dictionary.
pilot.util.filehandling.get_valid_path_from_list(paths)
Return the first valid path from the given list.
Parameters paths – list of file paths.
Returns first valid path from list (string).
pilot.util.filehandling.grep(patterns, file_name)
Search for the patterns in the given list in a file.
230
Chapter 7. System Architecture
PanDAWMS
Example: grep([“St9bad_alloc”, “FATAL”], “athena_stdout.txt”) -> [list containing the lines below]
CaloTrkMuIdAlg2.sysExecute() ERROR St9bad_alloc AthAlgSeq.sysExecute() FATAL Standard
std::exception is caught
Parameters
• patterns – list of regexp patterns.
• file_name – file name (string).
Returns list of matched lines in file.
pilot.util.filehandling.is_json(input_file)
Check if the file is in JSON format. The function reads the first character of the file, and if it is “{” then returns
True.
Parameters input_file – file name (string)
Returns Boolean.
pilot.util.filehandling.locate_file(pattern)
Locate a file defined by the pattern.
Example: pattern = os.path.join(os.getcwd(), ‘**/core.123’) -> /Users/Paul/Development/python/tt/core.123
Parameters pattern – pattern name (string).
Returns path (string).
pilot.util.filehandling.mkdirs(workdir, chmod=504)
Create a directory. Perform a chmod if set.
Parameters
• workdir – Full path to the directory to be created
• chmod – chmod code (default 0770) (octal).
Raises PilotException – MKDirFailure.
Returns
pilot.util.filehandling.move(path1, path2)
Move a file from path1 to path2.
Parameters
• path1 – source path (string).
• path2 – destination path2 (string).
pilot.util.filehandling.open_file(filename, mode)
Open and return a file pointer for the given mode. Note: the caller needs to close the file.
Parameters
• filename – file name (string).
• mode – file mode (character).
Raises PilotException – FileHandlingFailure.
Returns file pointer.
7.3. Pilot
231
PanDAWMS
pilot.util.filehandling.read_file(filename, mode='r')
Open, read and close a file. :param filename: file name (string). :param mode: :return: file contents (string).
pilot.util.filehandling.read_json(filename)
Read a dictionary with unicode to utf-8 conversion
Parameters filename –
Raises PilotException – FileHandlingFailure, ConversionFailure
Returns json dictionary
pilot.util.filehandling.read_list(filename)
Read a list from a JSON file.
Parameters filename – file name (string).
Returns list.
pilot.util.filehandling.remove(path)
Remove file. :param path: path to file (string). :return: 0 if successful, -1 if failed (int)
pilot.util.filehandling.remove_core_dumps(workdir, pid=None)
Remove any remaining core dumps so they do not end up in the log tarball
A core dump from the payload process should not be deleted if in debug mode (checked by the called). Also, a
found core dump from a non-payload process, should be removed but should result in function returning False.
Parameters
• workdir – working directory for payload (string).
• pid – payload pid (integer).
Returns Boolean (True if a payload core dump is found)
pilot.util.filehandling.remove_dir_tree(path)
Remove directory tree. :param path: path to directory (string). :return: 0 if successful, -1 if failed (int)
pilot.util.filehandling.remove_empty_directories(src_dir)
Removal of empty directories in the given src_dir tree. Only empty directories will be removed.
Parameters src_dir – directory to be purged of empty directories.
Returns
pilot.util.filehandling.remove_files(workdir, files)
Remove all given files from workdir.
Parameters
• workdir – working directory (string).
• files – file list.
Returns exit code (0 if all went well, -1 otherwise)
pilot.util.filehandling.rmdirs(path)
Remove directory in path.
Parameters path – path to directory to be removed (string).
Returns Boolean (True if success).
pilot.util.filehandling.scan_file(path, error_messages, warning_message=None)
Scan file for known error messages.
Parameters
232
Chapter 7. System Architecture
PanDAWMS
• path – path to file (string).
• error_messages – list of error messages.
• warning_message – optional warning message to printed with any of the error_messages
have been found (string).
Returns Boolean. (note: True means the error was found)
pilot.util.filehandling.tail(filename, nlines=10)
Return the last n lines of a file. Note: the function uses the posix tail function.
Parameters
• filename – name of file to do the tail on (string).
• nlines – number of lines (int).
Returns file tail (str).
pilot.util.filehandling.tar_files(wkdir, excludedfiles, logfile_name, attempt=0)
Tarring of files in given directory.
Parameters
• wkdir – work directory (string)
• excludedfiles – list of files to be excluded from tar operation (list)
• logfile_name – file name (string)
• attempt – attempt number (integer)
Returns 0 if successful, 1 in case of error (int)
pilot.util.filehandling.touch(path)
Touch a file and update mtime in case the file exists. Default to use execute() if case of python problem with
appending to non-existant path.
Parameters path – full path to file to be touched (string).
Returns
pilot.util.filehandling.update_extension(path='', extension='')
Update the file name extension to the given extension.
Parameters
• path – file path (string).
• extension – new extension (string).
Returns file path with new extension (string).
pilot.util.filehandling.verify_file_list(list_of_files)
Make sure that the files in the given list exist, return the list of files that does exist.
Parameters list_of_files – file list.
Returns list of existing files.
pilot.util.filehandling.write_file(path, contents, mute=True, mode='w', unique=False)
Write the given contents to a file. If unique=True, then if the file already exists, an index will be added (e.g.
‘out.txt’ -> ‘out-1.txt’) :param path: full path for file (string). :param contents: file contents (object). :param
mute: boolean to control stdout info message. :param mode: file mode (e.g. ‘w’, ‘r’, ‘a’, ‘wb’, ‘rb’) (string).
:param unique: file must be unique (Boolean). :raises PilotException: FileHandlingFailure. :return: True if
successful, otherwise False.
7.3. Pilot
233
PanDAWMS
pilot.util.filehandling.write_json(filename, data, sort_keys=True, indent=4, separators=(',', ': '))
Write the dictionary to a JSON file.
param filename file name (string).
param data object to be written to file (dictionary or list).
param sort_keys should entries be sorted? (boolean).
param indent indentation level, default 4 (int).
param separators field separators (default (‘,’, ‘: ‘) for dictionaries, use e.g. (‘,
‘) for lists) (tuple)
raises PilotException FileHandlingFailure.
return status (boolean).
harvester
pilot.util.harvester.dump(obj)
function for debugging - dumps object to sysout
pilot.util.harvester.findfile(path, name)
find the first instance of file in the directory tree
Parameters
• path – directory tree to search
• name – name of the file to search
Returns the path to the first instance of the file
pilot.util.harvester.get_event_status_file(args)
Return the name of the event_status.dump file as defined in the pilot config file and from the pilot arguments.
Parameters args – Pilot arguments object.
Returns event staus file name.
pilot.util.harvester.get_initial_work_report()
Prepare the work report dictionary.
Note:
the work_report should also contain all fields defined in
parse_jobreport_data().
Returns work report dictionary.
pilot.util.harvester.get_job_request_file_name()
Return the name of the job request file as defined in the pilot config file.
Returns job request file name.
pilot.util.harvester.get_worker_attributes_file(args)
Return the name of the worker attributes file as defined in the pilot config file and from the pilot arguments.
Parameters args – Pilot arguments object.
Returns worker attributes file name.
pilot.util.harvester.is_harvester_mode(args)
Determine if the pilot is running in Harvester mode. :param args: Pilot arguments object. :return: Boolean.
234
Chapter 7. System Architecture
PanDAWMS
pilot.util.harvester.kill_worker()
Create (touch) a kill_worker file in the pilot launch directory. This file will let Harverster know that the pilot has
finished.
Returns
pilot.util.harvester.parse_job_definition_file(filename)
This function parses the Harvester job definition file and re-packages the job definition dictionaries. The format
of the Harvester job definition dictionary is: dict = { job_id: { key: value, .. }, .. } The function returns a list of
these dictionaries each re-packaged as dict = { key: value } (where the job_id is now one of the key-value pairs:
‘jobid’: job_id)
Parameters filename – file name (string).
Returns list of job definition dictionaries.
pilot.util.harvester.publish_job_report(job, args, job_report_file='jobReport.json')
Copy job report file to make it accessible by Harvester. Shrink job report file.
Parameters
• job – job object.
• args – Pilot arguments object.
• job_report_file – name of job report (string).
Raises FileHandlingFailure – in case of IOError.
:return True or False
pilot.util.harvester.publish_stageout_files(job, event_status_file)
Publishing of work report to file.
The work report dictionary should contain the fields defined in
get_initial_work_report().
Parameters
• args – Pilot arguments object.
• job – job object.
• name (event status file) –
Returns Boolean. status of writing the file information to a json
pilot.util.harvester.publish_work_report(work_report=None,
worker_attributes_file='worker_attributes.json')
Publishing of work report to file.
The work report dictionary should contain the fields defined in
get_initial_work_report().
Parameters
• work_report – work report dictionary.
• worker_attributes_file –
Raises FileHandlingFailure – in case of IOError.
Returns True or False
pilot.util.harvester.remove_job_request_file()
Remove an old job request file when it is no longer needed.
Returns
7.3. Pilot
235
PanDAWMS
pilot.util.harvester.request_new_jobs(njobs=1)
Inform Harvester that the pilot is ready to process new jobs by creating a job request file with the desired number
of jobs.
Parameters njobs – Number of jobs. Default is 1 since on grids and clouds the pilot does not know
how many jobs it can
process before it runs out of time. :return:
https
class pilot.util.https._ctx(ssl_context, user_agent, capath, cacert)
__getnewargs__()
Return self as a plain tuple. Used by copy and pickle.
__module__ = 'pilot.util.https'
static __new__(_cls, ssl_context, user_agent, capath, cacert)
Create new instance of _ctx(ssl_context, user_agent, capath, cacert)
__repr__()
Return a nicely formatted representation string
__slots__ = ()
_asdict()
Return a new OrderedDict which maps field names to their values.
_field_defaults = {}
_fields = ('ssl_context', 'user_agent', 'capath', 'cacert')
_fields_defaults = {}
classmethod _make(iterable)
Make a new _ctx object from a sequence or iterable
_replace(**kwds)
Return a new _ctx object replacing specified fields with new values
property cacert
Alias for field number 3
property capath
Alias for field number 2
property ssl_context
Alias for field number 0
property user_agent
Alias for field number 1
pilot.util.https._tester(func, *args)
Tests function func on arguments and returns first positive.
>>> _tester(lambda x: x%3 == 0, 1, 2, 3, 4, 5, 6)
3
>>> _tester(lambda x: x%3 == 0, 1, 2)
None
236
Chapter 7. System Architecture
PanDAWMS
Parameters
• func – function(arg)->boolean
• args – other arguments
Returns something or none
pilot.util.https.cacert(args=None)
Tries to get CA (Certification Authority) certificate or X509 one. Testifies it to be a regular file. Tries next
locations:
1. --cacert from arguments
2. X509_USER_PROXY from env
3. Path /tmp/x509up_uXXX, where XXX refers to UID
Parameters args – arguments, parsed by argparse
Returns str – certificate file path, or None
pilot.util.https.cacert_default_location()
Tries to get current user ID through os.getuid, and get the posix path for x509 certificate. :returns: str – posix
default x509 path, or None
pilot.util.https.capath(args=None)
Tries to get CA path with certificates. Testifies it to be a directory. Tries next locations:
1. --capath from arguments
2. X509_CERT_DIR from env
3. Path /etc/grid-security/certificates
Parameters args – arguments, parsed by argparse
Returns str – directory path, or None
class pilot.util.https.ctx
__dict__ = mappingproxy({'ssl_context': None, 'user_agent': 'Pilot2 client',
'capath': None, 'cacert': None, '__module__': 'pilot.util.https', '__dict__':
<attribute '__dict__' of 'ctx' objects>, '__weakref__': <attribute '__weakref__' of
'ctx' objects>, '__doc__': None, '__annotations__': {}})
__module__ = 'pilot.util.https'
__weakref__
list of weak references to the object (if defined)
cacert = None
capath = None
ssl_context = None
user_agent = 'Pilot2 client'
pilot.util.https.execute_request(req)
Execute the curl request.
Parameters req – curl request command (string).
7.3. Pilot
237
PanDAWMS
Returns status (int), output (string).
pilot.util.https.execute_urllib(url, data, plain, secure)
Execute the request using urllib.
Parameters
• url – URL (string).
• data – data structure
Returns urllib request structure.
pilot.util.https.get_curl_command(plain, dat)
Get the curl command.
Parameters
• plain –
• dat – curl config option (string).
Returns curl command (string).
pilot.util.https.get_curl_config_option(writestatus, url, data, filename)
Get the curl config option.
Parameters
• writestatus – status of write_file call (Boolean).
• url – URL (string).
• data – data structure (dictionary).
• filename – file name of config file (string).
Returns config option (string).
pilot.util.https.get_urlopen2_output(req, context)
Get the output from the urlopen2 request.
Parameters
• req –
• context –
Returns ec (int), output (string).
pilot.util.https.get_urlopen_output(req, context)
Get the output from the urlopen request.
Parameters
• req –
• context –
Returns ec (int), output (string).
pilot.util.https.get_vars(url, data)
Get the filename and strdata for the curl config file.
Parameters
• url – URL (string).
• data – data to be written to file (dictionary).
238
Chapter 7. System Architecture
PanDAWMS
Returns filename (string), strdata (string).
pilot.util.https.https_setup(args=None, version=None)
Sets up the context for future HTTPS requests:
1. Selects the certificate paths
2. Sets up User-Agent
3. Tries to create ssl.SSLContext for future use (falls back to curl if fails)
Parameters
• args – arguments, parsed by argparse
• version (str) – pilot version string (for User-Agent)
pilot.util.https.request(url, data=None, plain=False, secure=True)
This function sends a request using HTTPS. Sends User-Agent and certificates previously being set up by
https_setup. If ssl.SSLContext is available, uses urllib2 as a request processor. Otherwise uses curl.
If data is provided, encodes it as a URL form data and sends it to the server.
Treats the request as JSON unless a parameter plain is True.
If JSON is expected, sends Accept:
application/json header.
Parameters
• url (string) – the URL of the resource
• data (dict) – data to send
• plain (boolean) – if true, treats the response as a plain text.
• secure – Boolean (default: True, ie use certificates)
Usage:
https_setup(args, PILOT_VERSION)
# sets up ssl and other stuff
response = request('https://some.url', {'some':'data'})
Returns:
• dict – if everything went OK
• str – if plain parameter is True
• None – if something went wrong
information
jobmetrics
pilot.util.jobmetrics.get_job_metrics(job)
Return a properly formatted job metrics string. Job metrics are highly user specific, so this function merely calls
a corresponding get_job_metrics() in the user code. The format of the job metrics string is defined by the server.
It will be reported to the server during updateJob.
Example of job metrics: Number of events read | Number of events written | vmPeak maximum | vmPeak aver-
age | RSS average | .. Format: nEvents=<int> nEventsW=<int> vmPeakMax=<int> vmPeakMean=<int> RSS-
Mean=<int> hs06=<float> shutdownTime=<int>
7.3. Pilot
239
PanDAWMS
cpuFactor=<float> cpuLimit=<float> diskLimit=<float> jobStart=<int> memLimit=<int> run-
Limit=<float>
Parameters job – job object.
Returns job metrics (string).
pilot.util.jobmetrics.get_job_metrics_entry(name, value)
Get a formatted job metrics entry. Return a a job metrics substring with the format ‘name=value ‘ (return empty
entry if value is not set).
Parameters
• name – job metrics parameter name (string).
• value – job metrics parameter value (string).
Returns job metrics entry (string).
loopingjob
pilot.util.loopingjob.create_core_dump(pid=None, workdir=None)
Create core dump and copy it to work directory
pilot.util.loopingjob.get_looping_job_limit()
Get the time limit for looping job detection.
Returns looping job time limit in seconds (int).
pilot.util.loopingjob.get_time_for_last_touch(job, mt, looping_limit)
Return the time when the files in the workdir were last touched. in case no file was touched since the last check,
the returned value will be the same as the previous time.
Parameters
• job – job object.
• mt – MonitoringTime object.
• looping_limit – looping limit in seconds.
Returns time in seconds since epoch (int) (or None in case of failure).
pilot.util.loopingjob.kill_looping_job(job)
Kill the looping process. TODO: add allow_looping_job() exp. spec?
Parameters job – job object.
Returns (updated job object.)
pilot.util.loopingjob.looping_job(job, mt)
Looping job detection algorithm. Identify hanging tasks/processes. Did the stage-in/out finish within allowed
time limit, or did the payload update any files recently? The files must have been touched within the given
looping_limit, or the process will be terminated.
Parameters
• job – job object.
• mt – MonitoringTime object.
Returns exit code (int), diagnostics (string).
240
Chapter 7. System Architecture
PanDAWMS
math
pilot.util.math.add_lists(list1, list2)
Add list1 and list2 and remove any duplicates. Example: list1=[1,2,3,4] list2=[3,4,5,6] add_lists(list1, list2) =
[1, 2, 3, 4, 5, 6]
Parameters
• list1 – input list 1
• list2 – input list 2
Returns added lists with removed duplicates
pilot.util.math.bytes2human(n, _format='%(value).1f %(symbol)s', symbols='customary')
Convert n bytes into a human readable string based on format. symbols can be either “customary”, “custom-
ary_ext”, “iec” or “iec_ext”, see: http://goo.gl/kTQMs
>>> bytes2human(0)
'0.0 B'
>>> bytes2human(0.9)
'0.0 B'
>>> bytes2human(1)
'1.0 B'
>>> bytes2human(1.9)
'1.0 B'
>>> bytes2human(1024)
'1.0 K'
>>> bytes2human(1048576)
'1.0 M'
>>> bytes2human(1099511627776127398123789121)
'909.5 Y'
>>> bytes2human(9856, symbols="customary")
'9.6 K'
>>> bytes2human(9856, symbols="customary_ext")
'9.6 kilo'
>>> bytes2human(9856, symbols="iec")
'9.6 Ki'
>>> bytes2human(9856, symbols="iec_ext")
'9.6 kibi'
>>> bytes2human(10000, "%(value).1f %(symbol)s/sec")
'9.8 K/sec'
>>> # precision can be adjusted by playing with %f operator
>>> bytes2human(10000, _format="%(value).5f %(symbol)s")
'9.76562 K'
pilot.util.math.chi2(observed, expected)
Return the chi2 sum of the provided observed and expected values.
Parameters
• observed – list of floats.
• expected – list of floats.
7.3. Pilot
241
PanDAWMS
Returns chi2 (float).
pilot.util.math.convert_mb_to_b(size)
Convert value from MB to B for the given size variable. If the size is a float, the function will convert it to int.
Parameters size – size in MB (float or int).
Returns size in B (int).
Raises ValueError for conversion error.
pilot.util.math.diff_lists(list_a, list_b)
Return the difference between list_a and list_b.
Parameters
• list_a – input list a.
• list_b – input list b.
Returns difference (list).
pilot.util.math.float_to_rounded_string(num, precision=3)
Convert float to a string with a desired number of digits (the precision). E.g. num=3.1415, precision=2 -> ‘3.14’.
Parameters
• num – number to be converted (float).
• precision – number of desired digits (int)
Raises NotDefined – for undefined precisions and float conversions to Decimal.
Returns rounded string.
pilot.util.math.human2bytes(s, divider=None)
Attempts to guess the string format based on default symbols set and return the corresponding bytes as an integer.
When unable to recognize the format ValueError is raised.
If no digit passed, only a letter, it is interpreted as a one of a kind. Eg “KB” = “1 KB”. If no letter passed, it is
assumed to be in bytes. Eg “512” = “512 B”
The second argument is used to convert to another magnitude (eg return not bytes but KB). It can be interpreted
as a cluster size. Eg “512 B”, or “0.2 K”.
>>> human2bytes('0 B')
0
>>> human2bytes('3')
3
>>> human2bytes('K')
1024
>>> human2bytes('1 K')
1024
>>> human2bytes('1 M')
1048576
>>> human2bytes('1 Gi')
1073741824
>>> human2bytes('1 tera')
1099511627776
>>> human2bytes('0.5kilo')
512
(continues on next page)
242
Chapter 7. System Architecture
PanDAWMS
(continued from previous page)
>>> human2bytes('0.1
byte')
0
>>> human2bytes('1 k')
# k is an alias for K
1024
>>> human2bytes('12 foo')
Traceback (most recent call last):
...
ValueError: can't interpret '12 foo'
>>> human2bytes('1 M', 'K')
1024
>>> human2bytes('2 G', 'M')
2048
>>> human2bytes('G', '2M')
512
pilot.util.math.is_greater_or_equal(a, b)
Is the numbered string a >= b? “1.2.3” > “1.2” – more digits “1.2.3” > “1.2.2” – rank based comparison “1.3.2”
> “1.2.3” – rank based comparison “1.2.N” > “1.2.2” – nightlies checker, always greater
Parameters
• a – numbered string.
• b – numbered string.
Returns boolean.
pilot.util.math.mean(data)
Return the sample arithmetic mean of data.
Parameters data – list of floats or ints.
Returns mean value (float).
pilot.util.math.split_version(s)
Split version string into parts and convert the parts into integers when possible. Any encountered strings are left as
they are. The function is used with release strings. split_version(“1.2.3”) = (1,2,3) split_version(“1.2.Nightly”)
= (1,2,”Nightly”)
The function can also be used for sorting: > names = [‘YT4.11’, ‘4.3’, ‘YT4.2’, ‘4.10’, ‘PT2.19’, ‘PT2.9’] >
sorted(names, key=splittedname) [‘4.3’, ‘4.10’, ‘PT2.9’, ‘PT2.19’, ‘YT4.2’, ‘YT4.11’]
Parameters s – release string.
Returns converted release tuple.
pilot.util.math.sum_dev(x, y)
Return sum of deviations of sequence data. Sum (x - x_mean)**(y - y_mean)
Parameters
• x – list of ints or floats.
• y – list of ints or floats.
Returns sum of deviations (float).
pilot.util.math.sum_square_dev(data)
Return sum of square deviations of sequence data. Sum (x - x_mean)**2
7.3. Pilot
243
PanDAWMS
Parameters data – list of floats or ints.
Returns sum of squares (float).
pilot.util.math.tryint(x)
Used by numbered string comparison (to protect against unexpected letters in version number).
Parameters x – possible int.
Returns converted int or original value in case of ValueError.
monitoring
pilot.util.monitoring.check_local_space(initial=True)
Do we have enough local disk space left to run the job? For the initial local space check, the Pilot will require 2
GB of free space, but during running this can be lowered to 1 GB.
Parameters initial – True means a 2 GB limit, False means a 1 GB limit (optional Boolean)
Returns pilot error code (0 if success, NOLOCALSPACE if failure)
pilot.util.monitoring.check_output_file_sizes(job)
Are the output files within the allowed size limits?
Parameters job – job object.
Returns exit code (int), error diagnostics (string)
pilot.util.monitoring.check_payload_stdout(job)
Check the size of the payload stdout.
Parameters job – job object.
Returns exit code (int), diagnostics (string).
pilot.util.monitoring.check_work_dir(job)
Check the size of the work directory. The function also updates the workdirsizes list in the job object.
Parameters job – job object.
Returns exit code (int), error diagnostics (string)
pilot.util.monitoring.display_oom_info(payload_pid)
Display OOM process info.
Parameters payload_pid – payload pid (int).
pilot.util.monitoring.get_exception_error_code(diagnostics)
Identify a suitable error code to a given exception.
Parameters diagnostics – exception diagnostics (string).
Returns exit_code
pilot.util.monitoring.get_local_size_limit_stdout(bytes=True)
Return a proper value for the local size limit for payload stdout (from config file).
Parameters bytes – boolean (if True, convert kB to Bytes).
Returns size limit (int).
pilot.util.monitoring.get_max_allowed_work_dir_size(queuedata)
Return the maximum allowed size of the work directory.
Parameters queuedata – job.infosys.queuedata object.
244
Chapter 7. System Architecture
PanDAWMS
Returns max allowed work dir size in Bytes (int).
pilot.util.monitoring.get_max_input_size(queuedata, megabyte=False)
Return a proper maxinputsize value.
Parameters
• queuedata – job.infosys.queuedata object.
• megabyte – return results in MB (Boolean).
Returns max input size (int).
pilot.util.monitoring.get_score(pid)
Get the OOM process score.
Parameters pid – process id (int).
Returns score (string).
pilot.util.monitoring.job_monitor_tasks(job, mt, args)
Perform the tasks for the job monitoring. The function is called once a minute. Individual checks will be per-
formed at any desired time interval (>= 1 minute).
Parameters
• job – job object.
• mt – MonitoringTime object.
• args – Pilot arguments (e.g. containing queue name, queuedata dictionary, etc).
Returns exit code (int), diagnostics (string).
pilot.util.monitoring.set_number_used_cores(job)
Set the number of cores used by the payload. The number of actual used cores is reported with job metrics (if
set).
Parameters job – job object.
Returns
pilot.util.monitoring.should_abort_payload(current_time, mt)
Should the pilot abort the payload? In the case of Raythena, the Driver is monitoring the time to end jobs and
may decide that the pilot should abort the payload. Internally, this is achieved by letting the Actors know it’s time
to end, and they in turn contacts the pilot by placing a ‘pilot_kill_payload’ file in the run directory.
Parameters
• current_time – current time at the start of the monitoring loop (int).
• mt – measured time object.
Returns exit code (int), error diagnostics (string).
pilot.util.monitoring.utility_monitor(job)
Make sure that any utility commands are still running. In case a utility tool has crashed, this function may restart
the process. The function is used by the job monitor thread.
Parameters job – job object.
Returns
pilot.util.monitoring.verify_disk_usage(current_time, mt, job)
Verify the disk usage. The function checks 1) payload stdout size, 2) local space, 3) work directory size, 4) output
file sizes.
7.3. Pilot
245
PanDAWMS
Parameters
• current_time – current time at the start of the monitoring loop (int).
• mt – measured time object.
• job – job object.
Returns exit code (int), error diagnostics (string).
pilot.util.monitoring.verify_looping_job(current_time, mt, job)
Verify that the job is not looping.
Parameters
• current_time – current time at the start of the monitoring loop (int).
• mt – measured time object.
• job – job object.
Returns exit code (int), error diagnostics (string).
pilot.util.monitoring.verify_memory_usage(current_time, mt, job)
Verify the memory usage (optional). Note: this function relies on a stand-alone memory monitor tool that may
be executed by the Pilot.
Parameters
• current_time – current time at the start of the monitoring loop (int).
• mt – measured time object.
• job – job object.
Returns exit code (int), error diagnostics (string).
pilot.util.monitoring.verify_running_processes(current_time, mt, pid)
Verify the number of running processes. The function sets the environmental variable PILOT_MAXNPROC to
the maximum number of found (child) processes corresponding to the main payload process id. The function
does not return an error code (always returns exit code 0).
Parameters
• current_time – current time at the start of the monitoring loop (int).
• mt – measured time object.
• pid – payload process id (int).
Returns exit code (int), error diagnostics (string).
pilot.util.monitoring.verify_user_proxy(current_time, mt)
Verify the user proxy. This function is called by the job_monitor_tasks() function.
Parameters
• current_time – current time at the start of the monitoring loop (int).
• mt – measured time object.
Returns exit code (int), error diagnostics (string).
246
Chapter 7. System Architecture
PanDAWMS
monitoringtime
class pilot.util.monitoringtime.MonitoringTime
A simple class to store the various monitoring task times. Different monitoring tasks should be executed at
different intervals. An object of this class is used to store the time when a specific monitoring task was last
executed. The actual time interval for a given monitoring tasks is stored in the util/default.cfg file.
__dict__ = mappingproxy({'__module__': 'pilot.util.monitoringtime', '__doc__': '\n A
simple class to store the various monitoring task times.\n Different monitoring
tasks should be executed at different intervals. An object of this class is used to
store\n the time when a specific monitoring task was last executed. The actual time
interval for a given monitoring tasks\n is stored in the util/default.cfg file.\n ',
'__init__': <function MonitoringTime.__init__>, 'update': <function
MonitoringTime.update>, 'get': <function MonitoringTime.get>, '__dict__': <attribute
'__dict__' of 'MonitoringTime' objects>, '__weakref__': <attribute '__weakref__' of
'MonitoringTime' objects>, '__annotations__': {}})
__init__()
Return the initial MonitoringTime object with the current time as start values.
__module__ = 'pilot.util.monitoringtime'
__weakref__
list of weak references to the object (if defined)
get(key)
Return the value for the given key. Usage: mt=MonitoringTime()
mt.get(‘ct_proxy’)
The method throws an AttributeError in case of no such key.
Parameters key – name of key (string).
Returns key value (int).
update(key, modtime=None)
Update a given key with the current time or given time. Usage: mt=MonitoringTime()
mt.update(‘ct_proxy’)
Parameters
• key – name of key (string).
• modtime – modification time (int).
Returns
node
parameters
pilot.util.parameters.convert_to_int(parameter, default=None)
Try to convert a given parameter to an integer value. The default parameter can be used to force the function to
always return a given value in case the integer conversion, int(parameter), fails.
Parameters
• parameter – parameter (any type).
7.3. Pilot
247
PanDAWMS
• default – None by default (if set, always return an integer; the given value will be returned
if
conversion to integer fails). :return: converted integer.
pilot.util.parameters.get_maximum_input_sizes()
This function returns the maximum allowed size for all input files. The sum of all input file sizes should not
exceed this value.
Returns maxinputsizes (integer value in MB).
processes
pilot.util.processes.cleanup(job, args)
Cleanup called after completion of job.
Parameters job – job object
Returns
pilot.util.processes.convert_ps_to_dict(output, pattern='(\\d+) (\\d+) (\\d+) (.+)')
Convert output from a ps command to a dictionary.
Example: ps axo pid,ppid,pgid,cmd PID PPID PGID COMMAND 22091 6672 22091 bash 32581 22091
32581 ps something;sdfsdfds/athena.py ddfg -> dictionary = { ‘PID’: [22091, 32581], ‘PPID’: [22091,
6672], .. , ‘COMMAND’: [‘ps ..’, ‘bash’]}
Parameters
• output – ps stdout (string).
• pattern – regex pattern matching the ps output (raw string).
Returns dictionary.
pilot.util.processes.dump_stack_trace(pid)
Execute the stack trace command (pstack <pid>).
Parameters pid – process id (int).
Returns
pilot.util.processes.find_cmd_pids(cmd, ps_dictionary)
Find all pids for the given command. Example. cmd = ‘athena.py’ -> pids = [1234, 2267] (in case there are two
pilots running on the WN).
Parameters
• cmd – command (string).
• ps_dictionary – converted ps output (dictionary).
pilot.util.processes.find_pid(pandaid, ps_dictionary)
Find the process id for the command that contains ‘export PandaID=%d’.
Parameters pandaid – PanDA ID (string).
:param ps_dictionaryL ps output dictionary. :return: pid (int).
pilot.util.processes.find_processes_in_group(cpids, pid)
Find all processes that belong to the same group. Recursively search for the children processes belonging to pid
and return their pid’s. pid is the parent pid and cpids is a list that has to be initialized before calling this function
and it contains the pids of the children AND the parent.
248
Chapter 7. System Architecture
PanDAWMS
Parameters
• cpids – list of pid’s for all child processes to the parent pid, as well as the parent pid itself
(int).
• pid – parent process id (int).
Returns (updated cpids input parameter list).
pilot.util.processes.get_cgroups_base_path()
Return the base path for CGROUPS.
Returns base path for CGROUPS (string).
pilot.util.processes.get_cpu_consumption_time(t0)
Return the CPU consumption time for child processes measured by system+user time from os.times(). Note: the
os.times() tuple is user time, system time, s user time, s system time, and elapsed real time since a fixed point in
the past.
Parameters t0 – initial os.times() tuple prior to measurement.
Returns system+user time for child processes (float).
pilot.util.processes.get_current_cpu_consumption_time(pid)
Get the current CPU consumption time (system+user time) for a given process, by looping over all child pro-
cesses.
Parameters pid – process id (int).
Returns system+user time for a given pid (float).
pilot.util.processes.get_instant_cpu_consumption_time(pid)
Return the CPU consumption time (system+user time) for a given process, by parsing /prod/pid/stat. Note 1: the
function returns 0.0 if the pid is not set. Note 2: the function must sum up all the user+system times for both the
main process (pid) and the child processes, since the main process is most likely spawning new processes.
Parameters pid – process id (int).
Returns system+user time for a given pid (float).
pilot.util.processes.get_max_memory_usage_from_cgroups()
Read the max_memory from CGROUPS file memory.max_usage_in_bytes.
Returns max_memory (int).
pilot.util.processes.get_number_of_child_processes(pid)
Get the number of child processes for a given parent process.
Parameters pid – parent process id (int).
Returns number of child processes (int).
pilot.util.processes.get_pilot_pid_from_processes(_processes, pattern)
Identify the pilot pid from the list of processes.
Parameters
• _processes – ps output (string).
• pattern – regex pattern (compiled regex string).
Returns pilot pid (int or None).
pilot.util.processes.get_process_commands(euid, pids)
Return a list of process commands corresponding to a pid list for user euid.
Parameters
7.3. Pilot
249
PanDAWMS
• euid – user id (int).
• pids – list of process id’s.
Returns list of process commands.
pilot.util.processes.get_trimmed_dictionary(keys, dictionary)
Return a sub-dictionary with only the given keys.
Parameters
• keys – keys to keep (list).
• dictionary – full dictionary.
Returns trimmed dictionary.
pilot.util.processes.is_child(pid, pandaid_pid, dictionary)
Is the given pid a child process of the pandaid_pid? Proceed recursively until the parent pandaid_pid has been
found, or return False if it fails to find it.
pilot.util.processes.is_process_running(process_id)
Check whether process is still running.
Parameters process_id – process id (int).
Returns Boolean.
pilot.util.processes.is_zombie(pid)
Is the given process a zombie? :param pid: process id (int). :return: boolean.
pilot.util.processes.kill(pid, sig)
Kill the given process with the given signal.
Parameters
• pid – process id (int).
• sig – signal (int).
Return status True when successful (Boolean).
pilot.util.processes.kill_child_processes(pid)
Kill child processes.
Parameters pid – process id (int).
Returns
pilot.util.processes.kill_orphans()
Find and kill all orphan processes belonging to current pilot user.
Returns
pilot.util.processes.kill_process(pid)
Kill process.
Parameters pid – process id (int).
Returns boolean (True if successful SIGKILL)
pilot.util.processes.kill_process_group(pgrp)
Kill the process group.
Parameters pgrp – process group id (int).
Returns boolean (True if SIGMTERM followed by SIGKILL signalling was successful)
250
Chapter 7. System Architecture
PanDAWMS
pilot.util.processes.kill_processes(pid)
Kill process beloging to given process group.
Parameters pid – process id (int).
Returns
pilot.util.processes.killpg(pid, sig, args)
Kill given process group with given signal.
Parameters
• pid – process group id (int).
• sig – signal (int).
Returns
pilot.util.processes.threads_aborted(abort_at=2)
Have the threads been aborted?
Parameters abort_at – 1 for workflow finish, 2 for thread finish (since check is done just before
thread finishes) (int).
Returns Boolean.
proxy
pilot.util.proxy.get_distinguished_name()
Get the user DN. Note: the DN is also sent by the server to the pilot in the job description (produserid).
Returns User DN (string).
timer
Standalone implementation of time-out check on function call. Timer stops execution of wrapped function if it reaches
the limit of provided time. Supports decorator feature.
author Alexey Anisenkov
contact anisyonk@cern.ch
date March 2018
class pilot.util.timer.TimedProcess(timeout)
Process-based Timer implementation (multiprocessing module). Uses shared Queue to keep result. (completely
isolated memory space) In default python implementation multiprocessing considers (c)pickle as serialization
backend which is not able properly (requires a hack) to pickle local and decorated functions (affects Windows
only) Traceback data is printed to stderr
__dict__ = mappingproxy({'__module__': 'pilot.util.timer', '__doc__': '\n
Process-based Timer implementation (`multiprocessing` module). Uses shared Queue to
keep result.\n (completely isolated memory space)\n In default python implementation
multiprocessing considers (c)pickle as serialization backend\n which is not able
properly (requires a hack) to pickle local and decorated functions (affects Windows
only)\n Traceback data is printed to stderr\n ', '__init__': <function
TimedProcess.__init__>, 'run': <function TimedProcess.run>, '__dict__': <attribute
'__dict__' of 'TimedProcess' objects>, '__weakref__': <attribute '__weakref__' of
'TimedProcess' objects>, '__annotations__': {}})
7.3. Pilot
251
PanDAWMS
__init__(timeout)
Parameters timeout – timeout value for operation in seconds.
__module__ = 'pilot.util.timer'
__weakref__
list of weak references to the object (if defined)
run(func, args, kwargs, timeout=None)
class pilot.util.timer.TimedThread(timeout)
Thread-based Timer implementation (threading module) (shared memory space, GIL limitations, no way to kill
thread, Windows compatible)
__dict__ = mappingproxy({'__module__': 'pilot.util.timer', '__doc__': '\n
Thread-based Timer implementation (`threading` module)\n (shared memory space, GIL
limitations, no way to kill thread, Windows compatible)\n ', '__init__': <function
TimedThread.__init__>, 'execute': <function TimedThread.execute>, 'run': <function
TimedThread.run>, '__dict__': <attribute '__dict__' of 'TimedThread' objects>,
'__weakref__': <attribute '__weakref__' of 'TimedThread' objects>,
'__annotations__': {}})
__init__(timeout)
Parameters timeout – timeout value for operation in seconds.
__module__ = 'pilot.util.timer'
__weakref__
list of weak references to the object (if defined)
execute(func, args, kwargs)
run(func, args, kwargs, timeout=None)
Raise TimeoutException if timeout value is reached before function finished
exception pilot.util.timer.TimeoutException(message, timeout=None, *args)
__init__(message, timeout=None, *args)
__module__ = 'pilot.util.timer'
__str__()
Return str(self).
__weakref__
list of weak references to the object (if defined)
pilot.util.timer.Timer
alias of pilot.util.timer.TimedProcess
pilot.util.timer.timeout(seconds, timer=None)
Decorator for a function which causes it to timeout (stop execution) once passed given number of seconds. :param
timer: timer class (by default is Timer) :raise: TimeoutException in case of timeout interrupt
252
Chapter 7. System Architecture
PanDAWMS
timing
pilot.util.timing.add_to_pilot_timing(job_id, timing_constant, time_measurement, args, store=False)
Add the given timing contant and measurement got job_id to the pilot timing dictionary.
Parameters
• job_id – PanDA job id (string).
• timing_constant – timing constant (string).
• time_measurement – time measurement (float).
• args – pilot arguments.
• store – if True, write timing dictionary to file. False by default.
Returns
pilot.util.timing.get_elapsed_real_time(t0=None)
Return a time stamp corresponding to the elapsed real time (since t0 if requested). The function uses os.times()
to get the current time stamp. If t0 is provided, the returned time stamp is relative to t0. t0 is assumed to be an
os.times() tuple.
Parameters t0 – os.times() tuple for the t0 time stamp.
Returns time stamp (int).
pilot.util.timing.get_final_update_time(job_id, args)
High level function that returns the time for execution the final update for the given job_id.
Parameters
• job_id – PanDA job id (string).
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_getjob_time(job_id, args)
High level function that returns the time for the getjob operation for the given job_id.
Parameters
• job_id – PanDA job id (string).
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_initial_setup_time(job_id, args)
High level function that returns the time for the initial setup. The initial setup time is measured from PI-
LOT_START_TIME to PILOT_PRE_GETJOB.
Parameters
• job_id – PanDA job id (string).
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_payload_execution_time(job_id, args)
High level function that returns the time for the payload execution for the given job_id.
Parameters
• job_id – PanDA job id (string).
7.3. Pilot
253
PanDAWMS
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_postgetjob_time(job_id, args)
Return the post getjob time.
Parameters
• job_id – job object.
• args – pilot arguments.
Returns post getjob time measurement (int). In case of failure, return None.
pilot.util.timing.get_setup_time(job_id, args)
High level function that returns the time for the setup operation for the given job_id.
Parameters
• job_id – PanDA job id (string).
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_stagein_time(job_id, args)
High level function that returns the time for the stage-in operation for the given job_id.
Parameters
• job_id – PanDA job id (string).
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_stageout_time(job_id, args)
High level function that returns the time for the stage-out operation for the given job_id.
Parameters
• job_id – PanDA job id (string).
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_time_difference(job_id, timing_constant_1, timing_constant_2, args)
Return the positive time difference between the given constants. The order is not important and a positive differ-
ence is always returned. The function collects the time measurements corresponding to the given timing constants
from the pilot timing file. The job_id is used internally as a dictionary key. The given timing constants and their
timing measurements, belong to the given job_id. Structure of pilot timing dictionary:
{ job_id: { <timing_constant_1>: <time measurement in seconds since epoch>, .. }
job_id = 0 means timing information from wrapper. Timing constants are defined in pilot.util.constants. Time
measurement are time.time() values. The float value will be converted to an int as a last step.
Parameters
• job_id – PanDA job id (string).
• timing_constant_1 –
• timing_constant_2 –
• args – pilot arguments.
254
Chapter 7. System Architecture
PanDAWMS
Returns time difference in seconds (int).
pilot.util.timing.get_time_measurement(timing_constant, time_measurement_dictionary,
timing_dictionary, job_id)
Return a requested time measurement from the time measurement dictionary, read from the pilot timing file.
Parameters
• timing_constant – timing constant (e.g. PILOT_MULTIJOB_START_TIME)
• time_measurement_dictionary – time measurement dictionary, extracted from pilot tim-
ing dictionary.
• timing_dictionary – full timing dictionary from pilot timing file.
• job_id – PanDA job id (string).
Returns time measurement (float).
pilot.util.timing.get_time_since(job_id, timing_constant, args)
Return the amount of time that has passed since the time measurement of timing_constant.
Parameters
• job_id – PanDA job id (string).
• timing_constant –
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_time_since_multijob_start(args)
Return the amount of time that has passed since the last multi job was launched.
Parameters args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_time_since_start(args)
Return the amount of time that has passed since the pilot was launched.
Parameters args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.get_total_pilot_time(job_id, args)
High level function that returns the end time for the given job_id. This means the wall time that has passed from
the start of the pilot until after the last job update.
Parameters
• job_id – PanDA job id (string).
• args – pilot arguments.
Returns time in seconds (int).
pilot.util.timing.read_pilot_timing()
Read the pilot timing dictionary from file.
Returns pilot timing dictionary (json dictionary).
pilot.util.timing.time_stamp()
Return ISO-8601 compliant date/time format
Returns time information
7.3. Pilot
255
PanDAWMS
pilot.util.timing.timing_report(job_id, args)
Write a timing report to the job log and return relevant timing measurements.
Parameters
• job_id – job id (string).
• args – pilot arguments.
Returns time_getjob, time_stagein, time_payload, time_stageout, time_total_setup (integer strings).
pilot.util.timing.write_pilot_timing(pilot_timing_dictionary)
Write the given pilot timing dictionary to file.
Parameters pilot_timing_dictionary –
Returns
workernode
pilot.util.workernode.check_hz()
Try to read the SC_CLK_TCK and write it to the log.
Returns
pilot.util.workernode.collect_workernode_info(path=None)
Collect node information (cpu, memory and disk space). The disk space (in MB) is return for the disk in the
given path.
Parameters path – path to disk (string).
Returns memory (float), cpu (float), disk space (float).
pilot.util.workernode.get_condor_node_name(nodename)
On a condor system, add the SlotID to the nodename
Parameters nodename –
Returns
pilot.util.workernode.get_cpu_model()
Get cpu model and cache size from /proc/cpuinfo.
Example. model name : Intel(R) Xeon(TM) CPU 2.40GHz cache size : 512 KB
gives the return string “Intel(R) Xeon(TM) CPU 2.40GHz 512 KB”.
Returns cpu model (string).
pilot.util.workernode.get_cpuinfo()
Return the CPU frequency (in MHz).
Returns cpu (float).
pilot.util.workernode.get_disk_space(queuedata)
Get the disk space from the queuedata that should be available for running the job; either what is actually locally
available or the allowed size determined by the site (value from queuedata). This value is only to be used internally
by the job dispatcher.
Parameters queuedata – infosys object.
Returns disk space that should be available for running the job (int).
pilot.util.workernode.get_local_disk_space(path)
Return remaning disk space for the disk in the given path. Unit is MB.
256
Chapter 7. System Architecture
PanDAWMS
Parameters path – path to disk (string). Can be None, if call to collect_workernode_info() doesn’t
specify it.
Returns disk space (float).
pilot.util.workernode.get_meminfo()
Return the total memory (in MB).
Returns memory (float).
pilot.util.workernode.get_node_name()
Return the local node name.
Returns node name (string)
workflow components
generic
pilot.workflow.generic.interrupt(args, signum, frame)
Interrupt function on the receiving end of kill signals. This function is forwarded any incoming signals (SIGINT,
SIGTERM, etc) and will set abort_job which instructs the threads to abort the job.
Parameters
• args – pilot arguments.
• signum – signal.
• frame – stack/execution frame pointing to the frame that was interrupted by the signal.
Returns
pilot.workflow.generic.register_signals(signals, args)
Register kill signals for intercept function.
Parameters
• signals – list of signals.
• args – pilot args.
Returns
pilot.workflow.generic.run(args)
Main execution function for the generic workflow.
The function sets up the internal queues which handle the flow of jobs.
Parameters args – pilot arguments.
Returns traces.
7.3. Pilot
257
PanDAWMS
7.4 PanDA monitor
Under construction.
7.5 Harvester
Harvester provisions the Pilot on computing resources using the relevant communication protocol for each resource
provider, and communicates with PanDA server on behalf of Pilot if it runs on the limited environment where outbound
network connectivity is missing.
Harvester is a common project that extends beyond PanDA. The details of Harvester are described on an external page.
7.6 Identity and Access Management
PanDA has an Identity and Access Management (IAM) scheme fully compliant with OIDC/OAuth2.0 capable of iden-
tity federation among scientific and academic identity providers. Although legacy x509 is also supported, it is recom-
mended to avoid it since it is being outdated.
PanDA IAM is consist of
• Indigo IAM
• CILogon
• Identity providers
Indigo IAM is an account and group membership management service to define virtual organizations (VOs) and groups,
to add/remove users to/from VOs and groups, and issue ID tokens once users are authenticated. CILogon is a federated
ID broker to delegate authentication to ID providers such as CERN, BNL IT/SDCC, KIT, Google, ...
The figure above shows the procedure of user authentication and authorization, where the device code flow is used to
allow users to run command-line tools. First, the user invokes a command-line tool which checks if a valid ID token
is locally available. If not, the command-line tool sends an authentication request to Indigo IAM on behalf of the user
and retrieves a verification URL. Then the user opens a web browser to go to the verification URL, and is eventually
redirected to his/her own ID provider through CILogon. Once the user successfully logs on, a couple of tokens are
exchanged between CILogon and Indigo IAM, and an ID token is issued. The command-line tool gets the ID token
and put it to the HTTP request header when accessing the PanDA server. The PanDA server decodes the token and
authorizes the user based on OIDC claims such as name, username, and groups.
7.6.1 DOMA PanDA IAM
There is an multipurpose IAM instance at DOMA PanDA IAM which can define any VO or group to play with PanDA.
258
Chapter 7. System Architecture
PanDAWMS
7.6. Identity and Access Management
259
PanDAWMS
7.6.2 Client setup
panda-client needs to set the following environment variables to enable OIDC/OAuth2.0 based Auth.
$ export PANDA_AUTH=oidc
$ export PANDA_AUTH_VO=<name of virtual organization>
$ export PANDA_VERIFY_HOST=off
where <name of virtual organization> should be replaced with the actual VO name.
7.6.3 Adding a new VO to the PanDA server
Each VO can be defined as a group in PanDA IAM, so that VOs share the same OIDC client attributes to skip the
registration step in CILogon. In other words, if the VO wants to use a new OIDC client it needs to be registered in
CILogon at https://cilogon.org/oauth2/register.
There are three parameters in panda_server.cfg.
# set to oidc to enable OpenID Connect
token_authType = oidc
# directory where OIDC authentication config files are placed
auth_config = /opt/panda/etc/panda/auth/
# json filename where OIDC authorization policies are described
auth_policies = /opt/panda/etc/panda/auth_policies.json
token_authType needs to be oidc to enable the OIDC/OAuth2.0 based Auth. The OIDC authentication configuration
file are placed under the directory specified by the auth_config parameter. The filename should be <name of virtual
organization>_auth_config.json. The configuration file contains
• “audience”
• “client_id”
• “client_secret”
• “oidc_config_url”
• and “vo”
The first three are attributes of the OIDC client defined in PanDA IAM, “oidc_config_url” is the well-known openid-
configuration URL of PanDA IAM, and “vo” is the VO name. Those configuration files must be reachable through
Web interface of the PanDA server, so that make sure that the directory needs to be exposed in httpd.conf like
Alias /auth/ "/opt/panda/etc/panda/auth/"
A json file, specified by auth_policies, defines authorization policies of VOs. The format is
{
"VO name": [
[
"string in OIDC group attribute",
{
"group": "group name in the VO",
"role": "user"
}
(continues on next page)
260
Chapter 7. System Architecture
PanDAWMS
(continued from previous page)
],
...
],
"another vo name": [
...
]
],
...
}
PanDA IAM gives all group names in the OIDC group attribute. This means that each group name must be unique.
The authorization policy file describes mapping between OIDC groups and actual groups in VOs. The “role” defines
the permission level of users in the group.
7.6. Identity and Access Management
261
PanDAWMS
262
Chapter 7. System Architecture
CHAPTER
EIGHT
DATABASE
All operations in PanDA are handled and checkpointed through a central database that is shared by all the PanDA and
JEDI servers. ATLAS operation at scale (O(1M) jobs per day) is performed using an Oracle backend and several opti-
mizations have been build over the years. There is also a more limited experience and support for MySQL/MariaDB. A
few PanDA instances are operated by different experiments, but the scale is several orders of magnitude lower requiring
far less optimization. Also not the full functionalities of PanDA and JEDI are used. Currently PanDA SQL code is
written and tested for the ATLAS Oracle case and is translated to MySQL dialect through a in-house function. I.e.
there is no usage of ORM layers as SQL Alchemy.
You can find more information in the following sections.
8.1 Client configuration
8.1.1 DB clients
The connection to the DBs is done through cx_Oracle or mysqlclient packages. There is a PanDA in-house library for
connection pooling.
8.1.2 Configuration
Below is an annotated excerpt of the PanDA configuration file that describes the DB settings. General configuration is
explained further in the installation and administration sections.
##########################
#
# Database parameters
#
# PanDA server connection pooling
# The number of connections to the DB **for each server process**
# If you are using FastCGI or WSGI, only the second value will be used
nDBConnection = 2
nDBConForFastCGIWSGI = 1
# JEDI workers
nWorkers = 8
# Timeout configuration
usedbtimeout = True
(continues on next page)
263
PanDAWMS
(continued from previous page)
dbtimeout = 300
# Logging configuration, can generate heavy logs if enabled
dbbridgeverbose = False
dump_sql = False
# Schemas
schemaPANDA = ATLAS_PANDA
schemaMETA = ATLAS_PANDAMETA
schemaGRISLI = ATLAS_GRISLI
schemaPANDAARCH = ATLAS_PANDAARCH
schemaJEDI = ATLAS_PANDA
schemaDEFT = ATLAS_DEFT
##########################
#
# Oracle specific configuration
#
dbhost = <DB HOST>
dbuser = <DB USER>>
dbpasswd = <PASSWORD>
dbname = <DB NAME>>
##########################
#
# MySQL specific configuration
#
# activate MySQL option
backend = mysql
# server configuration
dbhostmysql = <DB HOST>
dbportmysql = <PORT>
dbnamemysql = <DB NAME>
dbusermysql = <DB USER>
dbpasswdmysql = <PASSWORD>
8.2 Database server requirements
8.2.1 DB node specifications
The database server needs to be hosted on a server that is powerful enough to sustain the scale of the experiment’s
operations. One important metric that will dictate the specifications of the DB node is the number of daily jobs that
an experiment is running. Each job requires one entry in the jobs table and will also have entries in multiple auxiliary
tables (files, metadata).
Below are the specifications for the DB node used in the ATLAS experiment.
264
Chapter 8. Database
PanDAWMS
CPU and memory
The current database server has 512 GB memory and 2 x Intel Xeon E5-2630 3.1 GHz (each 10 Intel cores). It
will be migrated during 2021 to a server with 768 GB memory and 2 x Intel Xeon Gold 5222 3.9 GHz (each 4
Intel cores). The large memory is important so that the active tables are kept in memory and I/O is avoided.
This setup is preferred to a RAC environment with multiple smaller nodes with less memory. It is also important not
to share the node with other applications to avoid them overwriting each other’s memory.
Storage
The active database is currently occupying 40 TB of storage. Only a fraction of tables (job and file) are archived. For
other tables the active database contains the full history. This storage should be performant for fast data access. The
archive database is currently occupying 30 TB archive and is storing 10 years of jobs. These tables are rarely/never
accessed, but kept in case there would be an exceptional case to investigate.
8.2.2 Database service
In the case of ATLAS the database service is provided by the CERN IT Oracle DB team and they ensure high availability
of the service: - The node is part of a RAC cluster. During normal operation, the applications are split across the nodes.
I.e. there is one node for PanDA, one node for Rucio (data management) and one node for monitoring. Only if one
of the nodes would fail, the applications would fail over to the other nodes. - The nodes are replicated through Active
Data Guard. The secondary nodes are not used by the applications. Sparsely, some people use the Active Data Guard
instance for data analytics.
8.3 Entity relation diagrams
The full Oracle SQL files to generate the tables can be generated on request. The purpose of this section is to give an
overview of the database structure and important table groups.
8.3.1 ATLAS_PANDA schema
Tasks and jobs
A task is a group of jobs. A task is submitted to process a dataset (=group of files) and is split to generate jobs.
New tasks are inserted into the JEDI_TASKS table. Each task entry has a unique JEDITASKID primary key, which
is used for referencing. The input and output dataset and file information is stored into the JEDI_DATASETS and
JEDI_DATASET_CONTENTS respectively.
Jobs for each task are generated and move (for historical reasons) across the JOBSDEFINED4, JOBSWAITING4,
JOBSACTIVE4 and JOBSARCHIVED4 tables as they progress in their state. Note that the details for the jobs tables have
been hidden in the overview diagram for visibility reasons, but the JOBSACTIVE4 table is displayed separately. The 4
job tables are mostly similar, the only differences should be in some indexes in JOBSACTIVE4 and JOBSARCHIVED4,
since these are the larges and most commonly queried tables. Each job has a unique PANDAID primary key, which is
used for referencing.
Jobs have auxiliary metadata and job parameters stored in teh METATABLE and JOBPARAMSTABLE. ] These entries
travel together with the jobs during the archival process.
Each job has associated files in FILESTABLE4. FILESTABLE4 and JEDI_DATASET_CONTENTS are often misunder-
stood. The main difference is that FILESTABLE4 has entries for each job retry and therefore can be queried by PANDAID,
8.3. Entity relation diagrams
265
PanDAWMS
while JEDI_DATASET_CONTENTS contains the “primary” entries and only the latest retry of the job will be shown in
the PANDAID column.
To track the state transition of tasks and jobs, there are auxiliary tables TASKS_STATUSLOG and JOBS_STATUSLOG.
Another important concept are work queues JEDI_WORK_QUEUE and global shares GLOBAL_SHARES. Global shares is
an extension of the work queue. When jobs are created, they are tagged with a work queue and global share. Work
queues are internal, parallel pipes in JEDI to handle orderly task and job processing. Global shares are nestable targets
for the work queues. In this way we can control the amount of jobs of each type processed and can share common
resources across groups.
Configurator
Configurator tables store the site hierarchy and associations between computing and storage endpoints.
A site
(SITE) can contain multiple storage endpoints DDM_ENDPOINT and multiple PanDA compute queues PANDA_SITE.
The compute queues must be configured to read from or write to the storage endpoints in the relation table
PANDA_DDM_RELATION in order to use it.
Additional information about a site can be stored in the SITE_STATS table in Key-Value format.
The connectivity between sites can be stored in the NETWORK_MATRIX_KV. NETWORK_MATRIX_KV_TEMP is an auxiliary
table used only to optimize the speed of the loading process.
All the PanDA queue configuration is stored in JSON format in the SCHEDCONFIG_JSON table. This table is an evolution
of the ATLAS_PANDAMETA.SCHEDCONFIG table, which needs to be updated each time a new attribute is added.
Harvester
Harvester is the resource facing tool in the PanDA ecosystem. Each Harvester has its own database (MySQL/MariaDB
or SQLite) and synchronizes its state with PanDA for centralized monitoring.
Harvester instances need to be registered in HARVESTER_INSTANCES for PanDA to accept the incoming data.
Individual worker information is stored in the HARVESTER_WORKERS table. This table can/should be cleaned with a
sliding window process. A worker can process multiple PanDA jobs, therefore the relationship to the jobs is stored in
the HARVESTER_REL_JOB_WORKERS table.
Worker data is aggregated into HARVESTER_WORKER_STATS. These statistics are used by Unified Pilot Streaming (UPS)
service to determine which types of workers need to be submitted next. With this information, UPS generates commands
in HARVESTER_COMMANDS, which are picked up by Harvester through the PanDA API.
Harvester also publishes service information (e.g. instance CPU/memory/disk usage and validity of proxies) in json for-
mat, which is useful for service dashboards and alarms. This information is stored in HARVESTER_METRICS. Harvester
sends error messages from the internal log files to PanDA and are stored in HARVESTER_DIALOGS.
Others
PanDA frequently requires to calculate aggregations of the jobs. It is very costly to run independent aggregations of the
tables within seconds. For this reason, in the background there are some Oracle jobs pre-calculating and filling these
tables every couple of minutes. This reduces the CPU load on the DB significantly.
There is also a table TABLEPART4COPYING used to track the status of the job archival from ATLAS_PANDA to
ATLAS_PANDAARCH.
Finally there are a variety of other tables to complete the overview of tables.
266
Chapter 8. Database
PanDAWMS
8.3. Entity relation diagrams
267
PanDAWMS
268
Chapter 8. Database
PanDAWMS
8.3. Entity relation diagrams
269
PanDAWMS
8.3.2 ATLAS_PANDAARCH schema
The PANDAARCH schema is simple. It contains an archive of old jobs and directly associated entries. The tables have
a slightly different name. They are not accessed by PanDA itself, but are queried by the monitoring to view older jobs.
There are some auxiliary tables used for the archival process.
8.3.3 ATLAS_PANDAMETA schema
The PANDAMETA schema was used initially to store Grid and PanDA configuration, but is becoming less relevant over
the years. SCHEDCONFIG stores PanDA queue configuration. We are moving to the SCHEDCONFIG_JSON table in the
main schema. This table is still used for joined queries, but once we are on a higher Oracle version that allows querying
the fielsds in the JSON documents, we should try to move away completely from SCHEDCONFIG. CLOUDCONFIG lists
the clouds (WLCG regional and political groupings) and the main Tier 1 queue in each cloud. INSTALLED_SW tracks
the software that is installed in each queue.
8.4 Partitioning
Whenever possible, old temporary entries are deleted through a sliding window procedure. However for many tables
the information is preferred to be kept over extended periods of time (years) in order to be able to browse the data in
the monitoring. This leads to very large databases with many million rows. In addition to the usage of several indexes,
Oracle partitioning is also used to optimize data access. Below are the current partitioning strategies and how new
partitions are added (automatically, in a procedure or manually).
• ATLAS_PANDA
270
Chapter 8. Database
PanDAWMS
8.4. Partitioning
271
PanDAWMS
272
Chapter 8. Database
PanDAWMS
8.4. Partitioning
273
PanDAWMS
Table
Partitioned by
Range
Handled
Datasets
modificationtime
1 month
AUTO
Filestable4
modificationtime
1 day
ADD_DAILYPART proc
Harvester_dialogs
creationtime
1 day
AUTO
Harvester_metrics
creation_time
1 day
AUTO
Harvester_rel_jobs_workers
pandaid
1E8
AUTO
Harvester_workers
lastupdate
1 month
AUTO
Jedi_datasets
Jeditaskid
500k
AUTO
Jedi_dataset_contents
Jeditaskid
500k
AUTO
Jedi_events
Jeditaskid
500k
MANUAL
Jedi_jobparams_template
Ins_UTC_TSTAMP
1 day
AUTO
Jedi_output_template
Jeditaskid
500k
AUTO
Jedi_taskparams
Jeditaskid
500k
AUTO
Jedi_tasks
Jeditaskid
500k
AUTO
JOBPARAMSTABLE
MODIFICATIONTIME
1 day
ADD_DAILYPART proc
JOBSARCHIVED4
MODIFICATIONTIME
1 day
ADD_DAILYPART proc
JOBS_STATUSLOG
MODIFICATIONTIME
1 day
AUTO
METATABLE
MODIFICATIONTIME
1 day
ADD_DAILYPART proc
PANDALOG
BINTIME
1 day
AUTO
PANDALOG_FAX
BINTIME
1 month
AUTO
TABLEPART4COPYING
COPIED_TO_ARCH
values
STATIC
TASKS_STATUSLOG
MODIFICATIONTIME
1 day
AUTO
TASKS_STATUSLOG
MODIFICATIONTIME
1 day
AUTO
• ATLAS_PANDAARCH
Table
Partitioned by
Range
Handled
FILESTABLE_ARCH
MODIFICATIONTIME
1 month
MANUAL
JOBPARAMSTABLE_ARCH
MODIFICATIONTIME
1 month
MANUAL
JOBSARCHIVED
MODIFICATIONTIME
3 days
MANUAL
METATABLE_ARCH
MODIFICATIONTIME
1 month
MANUAL
8.5 Archival strategy
PanDA itself does not depend on completed jobs to be archived, but it is relevant for monitoring and audits. ATLAS
chooses to keep all of their historical data, but does a yearly archival exercise for some of the tables. ATLAS has two
databases and different schemas to implement a two step archival as shown in the diagram. As mentioned before, the
archive database is never/rarely accessed and does not need to be very performant. Alternative strategies could be
envisioned to avoid having a separate archive database.
274
Chapter 8. Database
PanDAWMS
8.6 Exporting Oracle Database Schemas to PostgreSQL
8.6.1 PostgreSQL Setup
Information about the yum repository is at https://www.postgresql.org/download/linux/redhat/.
Here is an example of PostgreSQL 13 setup on a puppet-managed CC7.
$ yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-lat
$ cp /etc/yum.repos.d/pgdg-redhat-all.repo /etc/yum-puppet.repos.d/
$ yum install -y postgresql13-server pg_cron_13 pg_partman13 postgresql13-contrib
$ /usr/pgsql-13/bin/postgresql-13-setup initdb
$ systemctl enable postgresql-13
Edit /var/lib/pgsql/13/data/postgresql.conf
password_encryption = md5
listen_addresses = '*'
# port = 3130
shared_preload_libraries = 'pg_cron, pg_partman_bgw'
cron.database_name = 'postgres'
pg_partman_bgw.dbname = 'panda_db'
Add
local
all
panda trust
host
all
panda localhost trust
local
all
postgres trust
host
all
postgres localhost trust
host
all
all 0.0.0.0/0 md5
host
all
all ::0/0 md5
to /var/lib/pgsql/13/data/pg_hba.conf.
Start PostgreSQL, make the database and the user, and enable pg_cron.
$ systemctl start postgresql-13
$ su - postgres
$ psql << EOF
8.6. Exporting Oracle Database Schemas to PostgreSQL
275
PanDAWMS
CREATE DATABASE panda_db;
CREATE USER panda PASSWORD 'password'
ALTER ROLE panda SET search_path = doma_panda,public;
CREATE EXTENSION pg_cron;
GRANT USAGE ON SCHEMA cron TO panda;
¸panda_db;
CREATE SCHEMA partman;
CREATE EXTENSION pg_partman SCHEMA partman;
EOF
8.6.2 Setup ora2pg
The latest ora2pg is available at https://github.com/darold/ora2pg/releases.
$ yum install perl-devel
$ yum install perl-DBD-Oracle
$ wget https://github.com/darold/ora2pg/archive/refs/tags/v21.1.zip
$ unzip v*
$ cd ora2pg-*/
$ perl Makefile.PL PREFIX=../
$ make && make install
$ cd ..
$ export PERL5LIB=$PWD/ora2pg-21.1/lib
Preparation of Config File
# PostgreSQL version
PG_VERSION
13
# Oracle database connection
ORACLE_DSN dbi:Oracle:INT8R
# Schema
EXPORT_SCHEMA
1
# Non-privileged Oracle access
USER_GRANTS 1
TRANSACTION READONLY
# Username in PostgreSQL
FORCE_OWNER panda
# Skip foreign keys since in PostgreSQL a foreign key must reference columns that either␣
˓→are a primary key
# or form a unique constraint, which is not always the case in Oracle
SKIP fkeys
# Show progress
DEBUG 1
276
Chapter 8. Database
PanDAWMS
Testing
The DBA or schema owner account is required to access Oracle since only they can export database objects in the
schema.
$ export ORA2PG_PASSWD=<Oracle password>
$ ./usr/local/bin/ora2pg -t SHOW_VERSION -u <Oracle schema> -c ora2pg.conf
$ ./usr/local/bin/ora2pg -t SHOW_REPORT -u <Oracle schema> --estimate_cost -c ora2pg.conf
8.6.3 Exporting Schemas
It is possible to export tables and sequences almost automatically. Procedures need many patches, while functions are
directory created since they are very few.
Tables and Sequences
Loop over PANDA, PANDAARCH, and PANDAMETA.
$ # set the core name of the Oracle schema and its password
$ export PANDA_SCHEMA=<core name of schema>
$ export ORA2PG_PASSWD=<the password>
$ # make DLL to create tables and sequences
$ ./usr/local/bin/ora2pg -t "TABLE SEQUENCE" -u ATLAS_$PANDA_SCHEMA -n ATLAS_$PANDA_SCHEMA
-N DOM
$ # reset sequence values
$ mv SEQUENCE_$PANDA_SCHEMA.sql a.sql
$ sed -E "s/START +[0-9]+/START 1/" a.sql | sed
-E "s/MINVALUE +([0-9]+)/MINVALUE 1/"
> SEQUENCE_$P
$ # create tables
$ psql -d panda_db -f TABLE_$PANDA_SCHEMA.sql
$ # create sequences
$ psql -d panda_db -f SEQUENCE_$PANDA_SCHEMA.sql
$ # delete tables when failed
$ psql -d panda_db -c
"select 'drop table doma_"$PANDA_SCHEMA,,".' || table_name || ' cascade;'
FROM information_schema.tables
where table_schema='doma_"$PANDA_SCHEMA,,"'"
| grep drop | psql -
$ # delete sequences when failed
$ psql -d panda_db -c
"select 'drop sequence doma_"$PANDA_SCHEMA,,".' || sequence_name || ' cascade
FROM information_schema.sequences where sequence_schema='doma_"$PANDA_SCHEMA,,"'"
| grep drop | p
Note that the DDL script to create the PANDA tables requires small correction.
652c652
< CREATE UNIQUE INDEX jedi_job_retry_history_uq ON jedi_job_retry_history (jeditaskid,␣
˓→newpandaid, oldpandaid, originpandaid);
---
> CREATE UNIQUE INDEX jedi_job_retry_history_uq ON jedi_job_retry_history (jeditaskid,␣
˓→newpandaid, oldpandaid, originpandaid, ins_utc_tstamp);
655c655
(continues on next page)
8.6. Exporting Oracle Database Schemas to PostgreSQL
277
PanDAWMS
(continued from previous page)
< ALTER TABLE jedi_job_retry_history ADD UNIQUE (jeditaskid,oldpandaid,newpandaid,
˓→originpandaid);
---
> ALTER TABLE jedi_job_retry_history ADD UNIQUE (jeditaskid,oldpandaid,newpandaid,
˓→originpandaid,ins_utc_tstamp);
Functions
For PANDA.
$ psql -d panda_db << EOF
CREATE OR REPLACE FUNCTION doma_panda.bitor ( P_BITS1 integer, P_BITS2 integer ) RETURNS integer AS $bod
BEGIN
RETURN P_BITS1 | P_BITS2;
END;
$body$
LANGUAGE PLPGSQL
;
ALTER FUNCTION doma_panda.bitor ( P_BITS1 integer, P_BITS2 integer ) OWNER TO panda;
EOF
Procedures
Only PANDA.
$ export ORA2PG_PASSWD=<the password of Oracle PANDA>
$ export PANDA_SCHEMA=PANDA
$ # make DLL to create procedures
$ ./usr/local/bin/ora2pg -t PROCEDURE -u ATLAS_$PANDA_SCHEMA -n ATLAS_$PANDA_SCHEMA
-N DOMA_$PAN
$ # patches
$ sed -E "s/atlas_(panda[^˙]*)/doma_Ł/gi" a.sql | sed -E "s/ default [0-9]+ owner/ owner/gi"
| sed "s
$ # create procedures
$ psql -d panda_db -f PROCEDURE_$PANDA_SCHEMA.sql
$ # patch for MERGE
$ psql -d panda_db << EOF
SET search_path = doma_panda,public;
CREATE OR REPLACE PROCEDURE doma_panda.jedi_refr_mintaskids_bystatus () AS $body$
BEGIN
INSERT INTO JEDI_AUX_STATUS_MINTASKID
(status, min_jeditaskid)
SELECT status, MIN(jeditaskid) min_taskid from JEDI_TASKS WHERE status NOT IN ('broken', 'aborted', 'fin
ON CONFLICT (status)
DO
UPDATE SET min_jeditaskid=EXCLUDED.min_jeditaskid;
278
Chapter 8. Database
PanDAWMS
END;
$body$
LANGUAGE PLPGSQL
SECURITY DEFINER
;
ALTER PROCEDURE jedi_refr_mintaskids_bystatus () OWNER TO panda;
EOF
BIGPANDAMON
For PANDABIGMON.
$ export ORA2PG_PASSWD=<the password of Oracle PANDABIGMON>
$ export PANDA_SCHEMA=PANDABIGMON
$ # make DLL to create procedures
$ ./usr/local/bin/ora2pg -t "TABLE SEQUENCE FUNCTION TYPE TRIGGER VIEW " -u ATLAS_$PANDA_SCHEMA -n ATLA
$ # reset sequence values
$ sed -E "s/START +[0-9]+/START 1/" SEQUENCE_a.sql | sed
-E "s/MINVALUE +([0-9]+)/MINVALUE 1/"
>
$ # create tables
$ mv
TABLE_a.sql TABLE_$PANDA_SCHEMA.sql
$ psql -d panda_db -f TABLE_$PANDA_SCHEMA.sql
$ # create sequences
$ psql -d panda_db -f SEQUENCE_$PANDA_SCHEMA.sql
$ # patch views
$ sed -E "s/atlas_(panda[^˙]*)/doma_Ł/gi" VIEW_a.sql | sed "s/@ADCR_ADG//ig" > VIEW_$PANDA_SCHEMA.sql
$ # create views
$ psql -d panda_db -f VIEW_$PANDA_SCHEMA.sql
$ # patches types since pandamon_jobpage_* are not correctly exported
$ grep -v pandamon_jobspage TYPE_a.sql > TYPE_$PANDA_SCHEMA.sql
$ echo << EOF >> TYPE_$PANDA_SCHEMA.sql
CREATE TYPE pandamon_jobpage_obj AS (
PANDA_ATTRIBUTE VARCHAR(100),
ATTR_VALUE VARCHAR(300),
NUM_OCCURRENCES bigint
);
ALTER TYPE pandamon_jobpage_obj OWNER TO panda;
CREATE TYPE pandamon_jobspage_coll AS (pandamon_jobspage_coll pandamon_jobpage_obj[]);
ALTER TYPE pandamon_jobspage_coll OWNER TO panda;
EOF
$ # create types before triggers, functions, and procedures
$ psql -d panda_db -f TYPE_$PANDA_SCHEMA.sql
8.6. Exporting Oracle Database Schemas to PostgreSQL
279
PanDAWMS
$ # create triggers
$ mv TRIGGER_a.sql TRIGGER_$PANDA_SCHEMA.sql
$ psql -d panda_db -f TRIGGER_$PANDA_SCHEMA.sql
$ # create functions
$ sed -E "s/atlas_(panda[^˙]*)/doma_Ł/gi" FUNCTION_a.sql
| awk 'BEGINIGNORECASE=1/ALTER FUNCTION/ g
$ psql -d panda_db -f FUNCTION_$PANDA_SCHEMA.sql
$ create procedures
$ sed -E "s/atlas_(panda[^˙]*)/doma_Ł/gi" PROCEDURE_a.sql | sed "s/ATL DEFAULT NULL/ATL text DEFAULT NULL
DEFT
$ wget https://raw.githubusercontent.com/PanDAWMS/panda-docs/main/docs/source/database/sql/pg_DEFT_TABLE
$ psql -d panda_db -f pg_DEFT_TABLE.sql
8.6.4 Registration of Scheduler Jobs
Aggregation jobs are functional, while backup and deletion jobs to be studied.
$ psql << EOF
SELECT cron.schedule ('* * * * *', 'call doma_panda.jedi_refr_mintaskids_bystatus()');
SELECT cron.schedule ('* * * * *', 'call doma_panda.update_jobsdef_stats_by_gshare()');
SELECT cron.schedule ('* * * * *', 'call doma_panda.update_jobsact_stats_by_gshare()');
SELECT cron.schedule ('* * * * *', 'call doma_panda.update_jobsactive_stats()');
SELECT cron.schedule ('* * * * *', 'call doma_panda.update_num_input_data_files()');
SELECT cron.schedule ('* * * * *', 'call doma_panda.update_total_walltime()');
SELECT cron.schedule ('* * * * *', 'call doma_panda.update_ups_statss()');
SELECT cron.schedule ('* * * * *', 'call doma_panda.update_job_stats_hp()');
UPDATE cron.job SET database='panda_db',username='panda' WHERE command like '%doma_panda.%';
SELECT cron.schedule ('@daily', $$DELETE FROM cron.job_run_details WHERE end_time < now() – interval '3
SELECT cron.schedule ('@daily', 'call partman.run_maintenance_proc()');
UPDATE cron.job SET database='panda_db' WHERE command like '%partman.run_maintenance_proc%';
EOF
8.6.5 Partitioning
$ wget https://raw.githubusercontent.com/PanDAWMS/panda-docs/main/docs/source/database/sql/pg_PARTITION
$ psql -d panda_db -f pg_PANDA_SCHEDULER_JOBS.sql
280
Chapter 8. Database
CHAPTER
NINE
INSTALLATION
Here are installation manuals of PanDA components.
Note: They are complete guides. It is recommended to have look at Quick Admin Tutorial beforehand.
9.1 PanDA server
This is the setup guide of the PanDA server.
Note: This is a complete guide. It is recommended to have a look at Quick Admin Tutorial beforehand.
9.1.1 Software requirements
The PanDA server requires:
• CentOS 7 or similar Linux distribution
• httpd 2.4
• httpd-devel
• python 3.6
• pip
• gridsite
Dependent python packages are automatically installed by pip.
281
PanDAWMS
9.1.2 Installation
It is a good practice to do installation in virtual environment.
$ python3 -m venv <install dir>
$ . <install dir>/bin/activate
Then
$ pip install panda-server[<database type>]
which will install panda-server, panda-common, and dependent python packages. The <database type> is oracle,
postgres, or mysql depending on your database backend.
If the latest panda-server in the git master repository is required,
$ pip install git+https://github.com/PanDAWMS/panda-server.git
9.1.3 Configuration
There are two python, one httpd, and one system configuration files under ${VIRTUAL_ENV}/etc/panda.
panda_common.cfg
This configuration file sets various parameters for logging.
$ cd $VIRTUAL_ENV/etc/panda
$ mv panda_common.cfg.rpmnew panda_common.cfg
The following parameters need to be modified if any.
Table 1: panda-common parameters
Name
Description
Default
loghost
The hostname of PanDA monitor
panda.cern.ch
logdir
The directory name where common log files are placed
/var/log/panda
log_level
Logging level
DEBUG
panda_server.cfg
This configuration file sets various parameters of the PanDA server.
$ cd $VIRTUAL_ENV/etc/panda
$ mv panda_server.cfg.rpmnew panda_server.cfg
The following parameters need to be modified if any.
282
Chapter 9. Installation
PanDAWMS
Table 2: panda-server parameters
Name
Description
Default
logdir
The directory name where server’s log files are placed
/var/log/panda
dbhost
The database hostname
dbuser
The database username
dbpasswd
The database password
nDBConForFastCGIWSGI
The number of database connections in each Web application
1
backend
Set mysql to use MySQL database
oracle
pserveralias
The common name of the http server
pan-
daserver.cern.ch
adder_plugins
Adder plugins
setupper_plugins
Setupper plugins
token_authType
Set to oidc to enable OIDC-based auth
x509
auth_config
The directory name for OIDC-based auth configuration files
auth_policies
The policy file of OIDC-based auth
Parameters of PanDA Daemon are descrribed in Using PanDA Daemon.
panda_server-httpd.conf
This configuration file set varous parameters of httpd.
$ cd $VIRTUAL_ENV/etc/panda
$ mv panda_server-httpd-FastCGI.conf.rpmnew panda_server-httpd.conf
The following parameters need to be modified. See Apache doc for detailed explanation of each directive.
Table 3: httpd parameters
Name
Description
User
The userid under which httpd runs
Group
The group under which httpd runs
LoadModule
wsgi_module
The file path of the mod_wsgi module
ServerName
The httpd server name
Alias /auth/
The directory name for OIDC-based auth configuration files. Must be consistent with
panda_server.cfg
WSGIDaemonProcess
Config of WSGI daemons. Change processes and home if any
Also you need to get/generate host certificate and key files and place them at /etc/grid-security/hostcert.pem
and /etc/grid-security/hostkey.pem, respectively.
The following httpd parameters can be configured dynamically by setting corresponding environment variables
when the service gets started. The default values of those variables are defined in ${VIRTUAL_ENV}/etc/panda/
panda_server.sysconfig.
Table 4: httpd parameters dynamically configurable
Name
Environment variable
Default value
PANDA_SERVER_CONF_SERVERNAME
The common name of httpd service
pandaserver.cern.ch
PANDA_SERVER_CONF_MIN_WORKERS
The minimum number of httpd workers
25
PANDA_SERVER_CONF_MAX_WORKERS
The maximum number of httpd workers
512
PANDA_SERVER_CONF_NUM_WSGI
The number of WSGI deamons
12
9.1. PanDA server
283
PanDAWMS
panda_server.sysconfig
$ cd $VIRTUAL_ENV/etc/panda
$ mv panda_server.sysconfig.rpmnew panda_server.sysconfig
Table 5: httpd parameters
Name
Description
HOME
The non-NFS home directory to run the service
X509_USER_PROXY
Proxy file path
9.1.4 System Setup
Then you need to register the PanDA server as a system service, make some directories, and setup log rotation if any.
$ # register the PanDA server in the system
$ mkdir -p /etc/panda
$ ln -s $VIRTUAL_ENV/etc/panda/*.cfg /etc/panda/
$ mv $VIRTUAL_ENV/etc/idds/idds.cfg.client.template $VIRTUAL_ENV/etc/idds/idds.cfg
$ ln -fs $VIRTUAL_ENV/etc/panda/panda_server.sysconfig /etc/sysconfig/panda_server
$ ln -fs $VIRTUAL_ENV/etc/rc.d/init.d/panda_server /etc/rc.d/init.d/httpd-pandasrv
$ /sbin/chkconfig --add httpd-pandasrv
$ /sbin/chkconfig httpd-pandasrv on
$ # make dirs
$ mkdir -p <logdir in panda_common.cfg>/wsgisocks
$ chown -R <userid in httpd.conf>:<group in httpd.conf> <logdir in panda_common.cfg>
$ # setup log rotation if necessary
$ ln -fs $VIRTUAL_ENV/etc/panda/panda_server.logrotate /etc/logrotate.d/panda_server
9.1.5 Service Control
$ # start
$ /sbin/service httpd-pandasrv start
$ # stop
$ /sbin/service httpd-pandasrv stop
There should be log files in the logdir. If httpd doesn’t get started there could be clues in panda_server_error_log.
284
Chapter 9. Installation
PanDAWMS
9.1.6 Test
$ curl http://localhost:25080/server/panda/isAlive
It will show the following message if successful.
alive=yes
If not,
see log files under logdir,
especially panda_server_access_log,
panda_server_error_log,
panda-Entry.log, panda-DBProxyPool.log, and panda-DBProxy.log would help.
9.1.7 Deployment with Helm
It is possible to deploy PanDA server instances on Kubernetes cluster using Helm.
$ wget https://github.com/PanDAWMS/helm-k8s/raw/master/panda-server/panda-server-helm.tgz
$ tar xvfz panda-server-helm.tgz
$ cd panda-server-helm
First, copy your host certificate and key files in the current directory.
$ cp /somewhere/hostcert.pem .
$ cp /somewhere/hostkey.pem .
Next, edit panda_server_configmap.json where each json entry corresponds to the attribute in panda_server.
cfg. For example,
{
"server": {
...
"dbuser": "FIXME",
in panda_server_configmap.json corresponds to
[server]
...
dbuser = FIXME
in panda_server.cfg.
Finally, you can install the PanDA server.
$ helm install mysrv ./
The service doesn’t get started automatically. To start it, set autoStart to true in values.yaml before installing the PanDA
server.
autoStart: true
Or
$ helm install mysrv ./ --set autoStart=true
9.1. PanDA server
285
PanDAWMS
9.2 JEDI
Here is the setup guide of JEDI.
Note: This is a complete guide. It is recommended to have a look at Quick Admin Tutorial beforehand.
9.2.1 Software requirements
JEDI requires:
• CentOS 7 or similar Linux distribution
• python 3.6
• pip
Dependent python packages are automatically installed by pip.
9.2.2 Installation
Setup a virtual environment first.
$ python3 -m venv <install dir>
$ . <install dir>/bin/activate
Then
$ pip install panda-jedi
which will install panda-jedi and dependent python packages in addition to panda-server since JEDI runs on PanDA
server’s modules.
If the latest panda-jedi in the git master repository is required,
$ pip install git+https://github.com/PanDAWMS/panda-jedi.git
9.2.3 Configuration
There are two python and one system configuration files under ${VIRTUAL_ENV}/etc/panda.
286
Chapter 9. Installation
PanDAWMS
panda_common.cfg
See panda_common.cfg
panda_jedi.cfg
This configuration file sets various JEDI parameters.
$ cd $VIRTUAL_ENV/etc/panda
$ mv panda_jedi.cfg.rpmnew panda_jedi.cfg
• Global Parameters
The following parameters need to be modified if any.
Table 6: master parameters
Name
Description
uname
The userid under which JEDI runs
gname
The group under which JEDI runs
Table 7: database parameters
Name
Description
dbhost
The database hostname
dbuser
The database username
dbpasswd
The database password
• Agent Parameters
As explained in JEDI architecture page, JEDI agents/components have plugin structure. They need
to be configured in the following sections in panda_jedi.cfg:
ddm The component to access the data management system
confeeder Contents Feeder
taskrefine Task Refine
jobbroker Job Brokerage
jobthrottle The component to throttle job submission
jobgen Job Generator
postprocessor Post Processor
watchdog Watch Dog
taskbroker Task Brokerage
tcommando Task Commando
msgprocessor Message Processor
Most of them have two parameters, modConfig and procConfig. For example,
modConfig = wlcg:managed|test:pandajedi.jedidog.ProdWatchDog:ProdWatchDog
procConfig = wlcg:managed|test:2
9.2. JEDI
287
PanDAWMS
The first parameter modConfig defines what module and class is used for each virtual organization
and activity. The syntax is organization:activity:module_import_path:class_name<, ..
.>, where the first field specifies the organization name, the second field specifies the activity name,
the third field specifies the import path of the module, and the last field specifies the class name.
The organization and activity fields can be empty if it work regardless of organization or activity.
The activity field can also take a string concatenating activity names with | if it works for multiple
activities.
The second parameter in the above config example procConfig defines the number of processes
for each organization and activity. The syntax is experiment:activity:n_processes<, ...>,
where the first field specifies the organization name, the second field specifies the activity name, and
the third field specifies the number of processes. The experiment and activity fields are similar to that
of modConfig. If activity names are concatenated in the activity field those activities share the same
processes.
Parameters of Message Processor are described in Using Message Processor.
panda_jedi.sysconfig
$ cd $VIRTUAL_ENV/etc/panda
$ mv ../sysconfig/panda_jedi panda_jedi.sysconfig
Table 8: httpd parameters
Name
Description
HOME
The non-NFS home directory to run the service
X509_USER_PROXY
Proxy file path
9.2.4 System Setup
Then you need to register JEDI as a system service, make some directories, and setup log rotation if any. Check contents
in /etc/sysconfig/panda_server and /etc/sysconfig/panda_jedi just in case. Also make sure that log rotate
scripts of JEDI and the PanDA server don’t interfere with each other when they are installed on the same machine.
$ # register the PanDA server
$ ln -fs $VIRTUAL_ENV/etc/panda/panda_server.sysconfig /etc/sysconfig/panda_server
$ ln -fs $VIRTUAL_ENV/etc/panda/panda_jedi.sysconfig /etc/sysconfig/panda_jedi
$ ln -fs $VIRTUAL_ENV/etc/init.d/panda_jedi /etc/rc.d/init.d/panda_jedi
$ /sbin/chkconfig --add panda_jedi
$ /sbin/chkconfig panda_jedi on
$ # make dirs
$ mkdir -p <logdir in panda_common.cfg>
$ chown -R <userid in panda_jedi.cfg>:<group in panda_jedi.cfg> <logdir in panda_common.cfg>
$ # setup log rotation if necessary
$ ln -fs $VIRTUAL_ENV/etc/panda/panda_jedi.logrotate /etc/logrotate.d/panda_jedi
288
Chapter 9. Installation
PanDAWMS
9.2.5 Service Control
$ # start
$ /sbin/service panda_jedi start
$ # stop
$ /sbin/service panda_jedi stop
There should be log files in logdir. If it doesn’t get started there could be clues in panda_jedi_stdout.log and
panda_jedi_stderr.log.
9.2.6 Deployment with Helm
It is possible to deploy JEDI instances on Kubernetes cluster using Helm.
$ wget https://github.com/PanDAWMS/helm-k8s/raw/master/panda-jedi/panda-jedi-helm.tgz
$ tar xvfz panda-jedi-helm.tgz
$ cd panda-jedi-helm
First, copy your host certificate and key files in the current directory.
$ cp /somewhere/hostcert.pem .
$ cp /somewhere/hostkey.pem .
Next, edit panda_server_configmap.json and panda_jedi_configmap.json. Their json entries correspond to
attributes in panda_server.cfg and panda_jedi.cfg, respectively. For example,
{
"db": {
"dbhost": "FIXME",
in panda_jedi_configmap.json corresponds to
[db]
...
dbhost = FIXME
in panda_jedi.cfg.
Finally, you can install JEDI.
$ helm install myjedi ./
The service doesn’t get started automatically. To start it, set autoStart to true in values.yaml before installing JEDI.
autoStart: true
Or
$ helm install myjedi ./ --set autoStart=true
9.2. JEDI
289
PanDAWMS
9.3 Harvester
Harvester is a common project that extends beyond PanDA. The Harvester installation guide is available at an external
link.
290
Chapter 9. Installation
CHAPTER
TEN
SYSTEM CONFIGURATION PARAMETERS IN DATABASE
There is the PANDA.CONFIG table, so-called gdpconfig table in the database where you can define any configuration
parameter shared by all PanDA applications, so that system admins don’t have to tweak the static cfg files every time
they optimize the system.
The table has the following columns:
Name
Description
APP
The application name which uses the parameter
COMPONENT
The component name which uses the parameter
KEY
The parameter name
VALUE
The parameter value
TYPE
The parameter type
VO
The organization name which defines the parameter
DESCR
Description of the parameter
Applications get those parameters through the pandaserver.taskbuffer.TaskBuffer module.
from pandaserver.taskbuffer.TaskBuffer import taskBuffer
p = taskBuffer.getConfigValue(COMPONENT, KEY, APP, VO)
The method returns None if the parameter is undefined.
291
PanDAWMS
292
Chapter 10. System Configuration Parameters in Database
CHAPTER
ELEVEN
USING PANDA DAEMON
PanDA daemon is a sub-component of the PanDA server. Here is the configuration guide.
11.1 Configurations
11.1.1 Configuration File
The configurations of PanDA daemon should be written under [daemon] section in panda_server.cfg.
11.1.2 Example
The configurations may look like this:
[daemon]
# whether to run daemons for PanDA
enable = True
# user and group name to run daemons
uname = atlpan
gname = zp
# package path of script modules to run in daemon
package = pandaserver.daemons.scripts
# number of worker processes to run daemons
n_proc = 4
# when exceeding lifetime, worker process will be respawned
proc_lifetime = 14400
# configuration in json about daemons
# of the form {"daemon_name": {"module": <module_name>, "period": <period>, ...}, ...}
config = {
"dummy_test": {
"enable": true, "period": 120},
"add_main": {
"enable": true, "period": 240, "loop": true},
"add_sub": {
"enable": true, "period": 240},
(continues on next page)
293
PanDAWMS
(continued from previous page)
"evpPD2P": {
"enable": true, "period": 600},
"copyArchive": {
"enable": true, "period": 2400, "sync": true},
"datasetManager": {
"enable": true, "period": 2400, "sync": true},
"proxyCache": {
"module": "panda_activeusers_query", "enable": true, "period": 600},
"pilot_streaming": {
"module": "pilotStreaming", "enable": true, "period": 300},
"configurator": {
"enable": true, "module": "configurator", "period": 200, "sync": true},
"network_configurator": {
"enable": true, "module": "configurator", "arguments": ["--network"], "period":␣
˓→400, "sync": true},
"schedconfig_json": {
"enable": true, "module": "configurator", "arguments": ["--json_dump"], "period":
˓→200, "sync": true}
}
11.1.3 Descriptions of Parameters
• enable: Whether to enable PanDA daemon. When False, one will fail to start PanDA daemon service . Default
is False
• uname and gname: The user name and group name to run daemon processes with. Default is “nobody” for both
• package: The path of python package which contains the daemon scripts. Mandatory, while the existing scripts
are put under “pandaserver.daemons.scripts” in PanDA code
• n_proc: Number of worker processes to run daemons. Default is 1
• proc_lifetime: Lifetime of worker processes in seconds to run daemons. When a worker process is found
expired, it will terminate and PanDA Daemon will spawn a new worker. Meant to neutralize memory leak or
corruptions by daemon scripts. Default is 28800 seconds, i.e. 6 hours
• config: Configurations about the daemons in JSON format. Mandatory. More details below
11.1.4 Descriptions of config
Written as an object in JSON format.
Each element in the main object defines a daemon, as a key-value pair in form of "<daemon_name>": {...(daemon
config)...} , where
• "<daemon_name>": The name of the daemon. Can be an arbitrary string
• daemon config: A json object to set attributes of the daemon. Possible attributes are:
– "enable": Whether to run this daemon. Useful when one wants to temporary disable the daemon without
removing its configuration. Default
– "period": The time period in second in which the daemon runs. This attribute is mandatory. (Note that
if the run duration of a daemon script is longer than its period configured, PanDA daemon will not start to
run the same script until the existing one finishes. In this case, the actual period in real world is longer than
the period configured, and warning message is thrown out)
294
Chapter 11. Using PanDA Daemon
PanDAWMS
– "sync": Whether to synchronize among all PanDA servers. If true, only one PanDA server at a time can
run this daemon (implemented with process lock in DB), and the period of the daemon is considered among
all PanDA servers (it counts when any one PanDA server runs the script). Default is false
– "loop": Loop mode, whether to loop the daemon script. If true, the daemon script will be run in a loop.
The loop will keep going if daemon script returns True and will exit if the daemon script returns False.
This is useful for the scripts that needs to be run constantly (e.g. add_main, message-consumer like stuff).
Note that in loop mode, the loop of script is allowed to run longer than the daemon period configured, and
there will be no warning message if the script runs longer than the period. Default is false
– "module": The module name (under the package defined in package above) of the script to run in this
daemon. If omitted, its value will be the same as the "<daemon_name>" by default
– "arguments": An json array of additional arguments of the script. For example, if the daemon should
run the script as this command: run-me.py dump -n 100 , then in configuration in can be: "module":
"run-me", "arguments": ["dump", "-n", 100] . Default is empty array
11.2 Service Control
One can control PanDA daemon with the panda_daemon service script:
$ /opt/panda/etc/rc.d/init.d/panda_daemon start
$ /opt/panda/etc/rc.d/init.d/panda_daemon stop
which will start/stop PanDA daemon.
Or equivalently, one can control PanDA daemon with the httpd-pandasrv init.d script, with special argument:
$ /sbin/service httpd-pandasrv start-daemon
$ /sbin/service httpd-pandasrv stop-daemon
which will also start/stop PanDA daemon.
Note that, about the httpd-pandasrv init.d script, the start and stop argument:
$ /sbin/service httpd-pandasrv start
$ /sbin/service httpd-pandasrv stop
will start/stop both PanDA web application and PanDA daemon.
11.3 Logs
Daemon Master process:
<logdir>/panda_daemon_stdout.log
<logdir>/panda_daemon_stderr.log
Daemon Worker processes:
/var/log/panda/panda-daemons.log
11.2. Service Control
295
PanDAWMS
11.4 Translation from Crontab to Daemon Configuration
11.4.1 The script needs to run on every panda server independently
One can set them to have "sync": false (or just omit sync), and its period to be the same as the cron period.
E.g. add.py
0-59/4 * * * * atlpan /opt/panda/usr/bin/panda_server-add > /dev/null 2>&1
It runs every 4 minutes = 240 seconds. Thus, its daemon config can be
"add": {"period": 240}
11.4.2 The script can run (and had better run) on one panda server at a time
One can set them to have "sync": true.
The period in daemon configuration should be set as the period in which ANY PanDA server run the script.
E.g. copyArchive.py
5 1-19/6 * * * atlpan /opt/panda/usr/bin/panda_server-copyArchive > /dev/null 2>&1
Note that we set different time offsets in crontab on different PanDA servers to stagger the run of copyArchive by PanDA
servers.
Here, the script runs every 6 hours = 21600 seconds in crontab, on each PanDA server.
Say we have 9 PanDA servers; then on average, the script run in the period of 21600 / 9 = 2400 seconds
Thus, its daemon config can be
"copyArchive": {"period": 2400, "sync": true}
11.4.3 Exception
If the script needs to be run pretty frequently, and does not matter to run by multiple panda servers at a time, then one
may not need the sync.
E.g. pilotStreaming.py
0-59/5 * * * * atlpan /opt/panda/usr/bin/panda_server-pilot_streaming > /dev/null 2>&1
Here, the script runs every 5 minutes = 300 seconds in crontab, on each PanDA server.
Say we have 9 PanDA servers; then on average, the script run in the period of 300 / 9 = 33 seconds, which is rather
short. It is kinda overkill to have an unnecessary process lock in DB for a time less than one minute.
Hence we can just the script to run on every panda server independently. Thus, its daemon config can be
"pilot_streaming": {"module": "pilotStreaming", "period": 300}
296
Chapter 11. Using PanDA Daemon
CHAPTER
TWELVE
BROKERAGE
The brokerage is one of the most crucial functions in the system to distribute workload among computing resources. It
has the following goals:
• To assign enough jobs to computing resources to utilize all available CPUs continuously.
• To minimize the waiting time for each job to produce output data.
• To execute jobs in such a way that the jobs respect their priorities and resource allocations.
• To choose computing resources for each job based on characteristics of the job and constraints of the computing
resources.
It is not straightforward to satisfy those goals for all jobs since some of them are logically contradictory. The brokerage
has a plugin structure so that organizations can provide their algorithms according to their needs and use-cases.
This page explains the algorithms of some advanced plugins.
Table of Contents
• ATLAS Production Job Brokerage
– Release/cache Availability Check for releases=AUTO
– Network Weight
– Timeout Rules
12.1 ATLAS Production Job Brokerage
This is the general ATLAS production job brokerage flow:
1. Generate the list of preliminary candidates from one of the following:
• All queues while excluding any queue with case-insensitive ‘test’ in the name.
• A list of pre-assigned queues. Unified queues are resolved to pseudo-queues. Although merge jobs are pre-
assigned to avoid transferring small pre-merged files, the pre-assignment is ignored if the relevant queues
have been skipped for 24 hours.
2. Filter out preliminary candidates that don’t pass any of the following checks:
297
PanDAWMS
• The queue status must be online unless the queues are pre-assigned.
• Skip queues if their links to the nucleus are blocked.
• Skip queues if over the NQUEUED_SAT_CAP (defined in gdpconfig) files queued on their links to the nucleus.
• Skip
all
queues
if
the
number
of
files
to
be
aggregated
to
the
nucleus
is
larger
than
NQUEUED_NUC_CAP_FOR_JOBS (defined in gdpconfig).
• If priority 800 or scout jobs, skip queues unless associated with the nucleus.
• If priority 800 or scout jobs or merging jobs or pre-merged jobs, skip inactive queues (where no jobs got
started in the last 2 hours although activated jobs had been there).
• Zero Share, which is defined in the fairsharepolicy field in CRIC. For example type=evgensimul:100%,
in this case, only evgen or simul jobs can be assigned as others have zero shares.
• If the task ioIntensity is larger than IO_INTENSITY_CUTOFF (defined in gdpconfig), the total size of
missing files must be less than SIZE_CUTOFF_TO_MOVE_INPUT (defined in gdpconfig) and the number of
missing files must be less than NUM_CUTOFF_TO_MOVE_INPUT (defined in gdpconfig). I.e., if a queue needs
to transfer more input files, the queue is skipped.
• There is a general MAX_DISKIO_DEFAULT limit in gdpconfig. It is possible to overwrite the limit for a
particular queue through the maxDiskIO (in kB/sec per core) field in CRIC. The limit is applied in job
brokerage: when the average diskIO per core for running jobs in a queue exceeds the limit, the next cycles
of job brokerage will exclude tasks with diskIO higher than the defined limit to progressively get the diskIO
under the threshold.
• CPU Core count matching.
• Availability of ATLAS release/cache. This check is skipped when queues have ANY in the releases filed
in CRIC. If queues have AUTO in the releases filed, the brokerage uses the information published in a
json by CRIC as explained at this section.
• Queues publish maximum (and minimum) memory size per core. The expected memory site of each job
is estimated for each queue as
(𝑏𝑎𝑠𝑒𝑅𝑎𝑚𝐶𝑜𝑢𝑛𝑡+ 𝑟𝑎𝑚𝐶𝑜𝑢𝑛𝑡× 𝑐𝑜𝑟𝑒𝐶𝑜𝑢𝑛𝑡) × 𝑐𝑜𝑚𝑝𝑒𝑛𝑠𝑎𝑡𝑖𝑜𝑛
where compensation is 0.9, avoiding sending jobs to high-memory queues when their expected memory
usage is close to the lower limit. Queues are skipped if the estimated memory usage is not included in the
acceptable memory ranges.
• Skip queues if they don’t support direct access to read input files from the local storage, although the task
is configured to use only direct access.
• The disk usage for a job is estimated as
𝑖𝑛𝑝𝑢𝑡𝐷𝑖𝑠𝑘𝐶𝑜𝑢𝑛𝑡+ 𝑚𝑎𝑥(0.5𝐺𝐵, 𝑜𝑢𝑡𝐷𝑖𝑠𝑘𝐶𝑜𝑢𝑛𝑡× 𝑛𝐸𝑣𝑒𝑛𝑡𝑠) + 𝑤𝑜𝑟𝑘𝐷𝑖𝑠𝑘𝐶𝑜𝑢𝑛𝑡
where inputDiskCount is the total size of job input files, a discrete function of nEvents, and nEvents is
the smalles1t number allowed based on the task requirements. inputDiskCount is zero if the queues are
configured to read input files directly from the local storage. maxwdir is divided by coreCount at each
queue and the resultant value must be larger than the expected disk usage.
• DISK size check, free space in the local storage has to be over 200GB.
• Skip blacklisted storage endpoints.
• If scout or merge jobs, skip queues if their maxtime is less than 24 hours.
298
Chapter 12. Brokerage
PanDAWMS
• The estimated walltime for a job is
𝑐𝑝𝑢𝑇𝑖𝑚𝑒× 𝑛𝐸𝑣𝑒𝑛𝑡𝑠
𝐶× 𝑃× 𝑐𝑝𝑢𝐸𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑐𝑦+ 𝑏𝑎𝑠𝑒𝑇𝑖𝑚𝑒
nEvents is the same as the one used to estimate the disk usage. The estimated walltime must be less than
maxtime of the queue.
• wnconnectivity of the queue must be consistent if the task specifies ipConnectivity. The format of
wnconnectivity and ipConnectivity is network_connectivity#ip_stack. network_connectivity
of the queue is
– full: to accept any tasks since outbound network connectivity is fully available,
– http: to accept tasks with network_connectivity=http or none since only http access is available, or
– none: to accept tasks with network_connectivity=none since no outbound network connectivity is
available,
ip_stack of the queue is
– IPv4: to accept tasks with ip_stack=IPv4,
– IPv6: to accept tasks with ip_stack=IPv6, or
– ‘’ (unset): to accept tasks without specifying ip_stack.
• Settings for event service and the dynamic number of events.
• Too many transferring jobs: skip if transferring > max(transferring_limit, 2 x running), where transfer-
ring_limit limit is defined by site or 2000 if undefined.
• Use only the queues associated with the nucleus if the task sets t1Weight=-1 and normal jobs are being
generated.
• Skip queues without pilots for the last 3 hours.
• If processingType=*urgent* or priority 1000, the Network weight must be larger than or equal to
NW_THRESHOLD NW_WEIGHT_MULTIPLIER (both defined in gdpconfig).
3. Calculate brokerage weight for remaining candidates. The initial weight is based on running vs queued jobs. The
brokerage uses the largest one as the number of running jobs among the following numbers:
• The actual number of running jobs at the queue, Rreal.
• min(nBatchJob, 20) if Rreal < 20 and nBatchJob (the number of running+submitted batch workers at PQ) >
Rreal. Mainly for bootstrap.
• numSlots if it is set to a positive number for the queue to the proactive job assignment.
• The number of starting jobs if numSlots is set to zero, which is typically useful for Harvester to fetch jobs
when the number of available slots dynamically changes.
The number of assigned jobs is ignored for the weight calculation and the subsequent filtering if the input for the
jobs being considered is already available locally. Jobs waiting for data transfer do not block new jobs needing
no transfer.
𝑚𝑎𝑛𝑦𝐴𝑠𝑠𝑖𝑔𝑛𝑒𝑑= 𝑚𝑎𝑥(1, 𝑚𝑖𝑛(2, 𝑎𝑠𝑠𝑖𝑔𝑛𝑒𝑑
𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑒𝑑))
𝑤𝑒𝑖𝑔ℎ𝑡=
𝑟𝑢𝑛𝑛𝑖𝑛𝑔+ 1
(𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑒𝑑+ 𝑎𝑠𝑠𝑖𝑔𝑛𝑒𝑑+ 𝑠𝑡𝑎𝑟𝑡𝑖𝑛𝑔+ 𝑑𝑒𝑓𝑖𝑛𝑒𝑑+ 10) × 𝑚𝑎𝑛𝑦𝐴𝑠𝑠𝑖𝑔𝑛𝑒𝑑
Take data availability into consideration.
𝑤𝑒𝑖𝑔ℎ𝑡= 𝑤𝑒𝑖𝑔ℎ𝑡×
𝑎𝑣𝑎𝑖𝑙𝑎𝑏𝑙𝑒𝑆𝑖𝑧𝑒+ 𝑡𝑜𝑡𝑎𝑙𝑆𝑖𝑧𝑒
𝑡𝑜𝑡𝑎𝑙𝑆𝑖𝑧𝑒× (𝑛𝑢𝑚𝑀𝑖𝑠𝑠𝑖𝑛𝑔𝐹𝑖𝑙𝑒𝑠/100 + 1)
12.1. ATLAS Production Job Brokerage
299
PanDAWMS
Apply a Network weight based on connectivity between nucleus and satellite, since the output files are aggregated
to the nucleus.
𝑤𝑒𝑖𝑔ℎ𝑡= 𝑤𝑒𝑖𝑔ℎ𝑡× 𝑛𝑒𝑡𝑤𝑜𝑟𝑘𝑊𝑒𝑖𝑔ℎ𝑡
4. Apply further filters.
• Skip queues if activated + starting > 2 running.
• Skip queues if defined+activated+assigned+starting > 2 running.
5. If all queues are skipped, the task is pending for 1 hour. Otherwise, the remaining candidates are sorted by
weight, and the best 10 candidates are taken.
12.1.1 Release/cache Availability Check for releases=AUTO
Each queue publishes something like
"AGLT2": {
"cmtconfigs": [
"x86_64-centos7-gcc62-opt",
"x86_64-centos7-gcc8-opt",
"x86_64-slc6-gcc49-opt",
"x86_64-slc6-gcc62-opt",
"x86_64-slc6-gcc8-opt"
],
"containers": [
"any",
"/cvmfs"
],
"cvmfs": [
"atlas",
"nightlies"
],
"architectures": [
{
"arch": ["x86_64"],
"instr": ["avx2"],
"type": "cpu",
"vendor": ["intel","excl"]
},
{
"type": "gpu",
"vendor": ["nvidia","excl"],
"model":["kt100"]
}
],
"tags": [
{
"cmtconfig": "x86_64-slc6-gcc62-opt",
"container_name": "",
"project": "AthDerivation",
(continues on next page)
300
Chapter 12. Brokerage
PanDAWMS
(continued from previous page)
"release": "21.2.2.0",
"sources": [],
"tag": "VO-atlas-AthDerivation-21.2.2.0-x86_64-slc6-gcc62-opt"
},
{
"cmtconfig": "x86_64-slc6-gcc62-opt",
"container_name": "",
"project": "Athena",
"release": "21.0.38",
"sources": [],
"tag": "VO-atlas-Athena-21.0.38-x86_64-slc6-gcc62-opt"
}
]
}
Checks for CPU and/or GPU Hardware
The format of task architecture is sw_platform<@base_platform><#host_cpu_spec><&host_gpu_spec>
where
host_cpu_spec
is
architecture<-vendor<-instruction_set>>
and
host_gpu_spec
is
vendor<-model>.
If host_cpu_spec or host_gpu_spec is specified, the architectures of the queue is
checked. The architectures can contain two dictionaries to describe CPU and GPU hardware specifications. All
attributes of the dictionaries except for the type attribute take lists of strings. If ‘attribute’: [‘blah’], the queue accepts
tasks with attribute=’blah’ or without specifying the attribute. If ‘excl’ is included in the list, the queue accepts only
tasks with attribute=’blah’. For example, tasks with #x86_64 are accepted by queues with “arch”: [“x86_64”], “arch”:
[“”], or “arch”: [“x86_64”, “excl”], but not by “arch”: [“arm64”].
Checks for Fat Containers
If the task uses a container, i.e., the container_name attribute is set, the brokerage checks as follows:
• If the task uses only tags, i.e., it sets onlyTagsForFC, the container_name must be equal to the container_name
of a tag in the tags list or must be included in the sources of a tag in the tags list.
• If the task doesn’t set onlyTagsForFC,
– ‘any’ or ‘/cvmfs’ must be included in the containers list, or
– container_name must be forward-matched with one of the strings in the containers list, or
– container_name is resolved to the source path using the dictionary of the “ALL” queue, and the resolved
source path must be forward-matched with one of the strings in the containers list.
Checks for Releases, Caches, or Nightlies
Checks are as follows for releases, caches, and nightlies:
• ‘any’ or cvmfs_tag must be included in the cvmfs list, where cvmfs_tag is atlas for standard releases and caches
or nightlies for nightlies. In addition,
– ‘any’ or ‘/cvmfs’ must be included in the containers list, or
– the task sw_platform is extracted from the task architecture and must be included in the cmtconfigs
list.
12.1. ATLAS Production Job Brokerage
301
PanDAWMS
• If the above is not the case, ‘any’ must be in the containers list and the task sw_platform, sw_project, and
sw_version must be equal to cmtconfig, project, and release of a tag in the tags list.
12.1.2 Network Weight
The network data sources are
• the Network Weather Service as the dynamic source, and
• the CRIC closeness as a semi static source.
Given the accuracy of the data and the timelapse from decision to action, the network weight only aims to provide a
simple, dynamic classification of links. It is currently calculated as:
𝑛𝑒𝑡𝑊𝑜𝑟𝑘𝑊𝑒𝑖𝑔ℎ𝑡= 0.5 × (𝑞𝑢𝑒𝑢𝑒𝑑𝑊𝑒𝑖𝑔ℎ𝑡+ 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡𝑊𝑒𝑖𝑔ℎ𝑡)
where the queued and throughput weight are calculated as in the plot below:
Fig. 1: queuedWeight
It uses the most recent available data, so preferably data of the last 1 hour, in not available of last 1 day, if not available
of last 1 week. FTS Mbps are used, which are filled from Chicago elastic search. If there are no available network
metrics, the AGIS closeness (0 best to 11 worst) is used in a normalized way
𝑤𝑒𝑖𝑔ℎ𝑡𝑁𝑤𝑇ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡= 1 +
𝑀𝐴𝑋_𝐶𝐿𝑂𝑆𝐸𝑁𝐸𝑆𝑆−𝑐𝑙𝑜𝑠𝑒𝑛𝑒𝑠𝑠
𝑀𝐴𝑋_𝐶𝐿𝑂𝑆𝐸𝑁𝐸𝑆𝑆−𝑀𝐼𝑁_𝐶𝐿𝑂𝑆𝐸𝑁𝐸𝑆𝑆
302
Chapter 12. Brokerage
PanDAWMS
Fig. 2: throughputWeight
12.1.3 Timeout Rules
• 1 hour for pending jobs
• 4 hours for defined jobs
• 12 hours for assigned jobs
• 7 days for throttled jobs
• 2 days for activated or starting jobs
• 4 hours for activated or starting jobs with job.currentPriority>=800 at the queues where laststart in the
SiteData table is older than 2 hours
• 30 min for sent jobs
• 21 days for running jobs
• 2 hours for heartbeats from running or starting jobs. Each workflow can define own timeout value using HEART-
BEAT_TIMEOUT_<workflow> in gdpconfig
• the above HEARTBEAT_TIMEOUT_<workflow> for transferring jobs with the workflow and own stage-out
mechanism that sets not-null job.jobSubStatus
• 3 hours for holding jobs with job.currentPriority>=800, while days for holding jobs with job.currentPriority<800
• transtimehi days for transferring jobs with job.currentPriority>=800, while transtimelo days for transfer-
ring jobs with job.currentPriority<800
• disable all timeout rules when the queue status is paused or the queue has disableReassign in catchall
• fast rebrokerage for defined, assigned, activated, or starting jobs at the queues where
– nQueue_queue(gshare)/nRun_queue(gshare) is larger than FAST_REBRO_THRESHOLD_NQNR_RATIO
– nQueue_queue(gshare)/nQueue_total(gshare) is larger than FAST_REBRO_THRESHOLD_NQUEUE_FRAC
– nQueue_queue(gshare) is larger than FAST_REBRO_THRESHOLD_NQUEUE_<gshare>. Unless the
gshare defines the parameter it doesn’t trigger the fast rebrokerage
– FAST_REBRO_THRESHOLD_blah is defined in gdpconfig
12.1. ATLAS Production Job Brokerage
303
PanDAWMS
– nSlots is not defined in the Harvester_Slots table since it intentionally cause large nQueue when nRun
is small
304
Chapter 12. Brokerage
CHAPTER
THIRTEEN
JOB SIZING
JEDI generates jobs based on the following task parameters:
Name
Description
nFilesPerJob
The number of input files per job
nGBPerJob
The total size of input/output files and working directory
nEventsPerJob
The number of events per job
cpuTime
HS06sec per event
cpuEfficiency
CPU efficiency (0.9 by default)
baseTime
The part of the job execution time not scaling with CPU power
outDiskCount
The expect output size per event
workDiskCount
The working directory size
If one of the first three parameters n*PerJob is specified, jobs are generated accordingly. If some computing resources
cannot accept those jobs due to resource limitation, such as small scratch disks and short walltime, the brokerage avoids
those resources.
If they are not specified, jobs are generated to meet the limitation of each computing resource. The number of events
nEvents in each job must satisfy the following formulae:
𝑆≥𝑖𝑛𝑝𝑢𝑡𝐷𝑖𝑠𝑘𝐶𝑜𝑢𝑛𝑡+ 𝑚𝑎𝑥(0.5𝐺𝐵, 𝑜𝑢𝑡𝐷𝑖𝑠𝑘𝐶𝑜𝑢𝑛𝑡× 𝑛𝐸𝑣𝑒𝑛𝑡𝑠) + 𝑤𝑜𝑟𝑘𝐷𝑖𝑠𝑘𝐶𝑜𝑢𝑛𝑡
𝑊≥
𝑐𝑝𝑢𝑇𝑖𝑚𝑒× 𝑛𝐸𝑣𝑒𝑛𝑡𝑠
𝐶× 𝑃× 𝑐𝑝𝑢𝐸𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑐𝑦+ 𝑏𝑎𝑠𝑒𝑇𝑖𝑚𝑒
where S, W, C, and P are the scratch disk size, the wall time limit, the number of CPU cores, and the HS06 core-power at
the computing resource, respectively. inputDiskCount is the total size of job input files, a discrete function of nEvents.
Note that inputDiskCount is zero if the computing resource is configured to read input files directly from the local
storage resource.
305
PanDAWMS
306
Chapter 13. Job Sizing
CHAPTER
FOURTEEN
COMPUTING RESOURCE ALLOCATIONS
Each organization needs to allocate the amount of computing resources dedicated to each activity, to manage CPU
resource sharing among various parallel campaigns and to make sure that results can be delivered in time for important
deadlines. While dynamic and static shares on batch systems have been around for a long time, PanDA requires a global
solution since it needs to manage shares among computing resources distributed world-wide while getting rid of local
resource partitioning. The global solution is not straightforward, given different requirements of the activities (number
of cores, memory, I/O, and CPU intensity), the heterogeneity of resources (site/HW capabilities, batch configuration,
and queue setup) and constraints on data locality.
This paper describes the details. Briefly, PanDA implements resource allocations as follows:
14.1 1. Global Shares definition
Global Shares establish the amount of resources available instantaneously to a specific activity as a fraction of the
total amount of resources available to the organization. Global Shares are a nestable structure, where siblings have the
preference to occupy unused shares, before the unused share goes to upper levels.
14.2 2. Tagging of tasks and jobs
Tasks and jobs are tagged with a Global Share at creation time. The tagging is based on a table that defines regular
expressions matched against the most common task and job attributes.
14.3 3. Job generation per Global Share
There are multiple Job Generator agents concurrently running in JEDI. Each agent takes a Global Share and exclusively
locks it while generating jobs for it. I.e., each Global Share generates jobs in parallel so they don’t interfere with each
other.
307
PanDAWMS
14.4 4. Job dispatch respecting Global Share target
When a slot is freed up at a computing resource and requests a job, the Job Dispatcher component in the PanDA server
decides which of the assigned jobs should run next at the computing resource, respecting Global Share targets. Job
Dispatcher orders the jobs assigned to the computing resource by Global Share preference (the share furthest away from
its target) and priority inside the Global Share. This assumes a healthy distribution of jobs of different shares across
computing resources, avoiding all jobs of a particular share assigned to few computing resources. It also assumes that
there are enough jobs of each share available for dispatch, so one share can not block another through the job generation
chain.
308
Chapter 14. Computing Resource Allocations
CHAPTER
FIFTEEN
DYNAMIC OPTIMIZATION OF TASK PARAMETERS
JEDI automatically optimizes task parameters for compute/storage resource requirements and strategies to partition
workload while running those tasks. In the early stage of the task execution, JEDI generates several jobs for each task
using only a small portion of input data, collects various metrics such as data processing rate and memory footprints,
and adjusts the following task parameters. Those first jobs are called scout jobs. The automatic optimization is triggered
twice for each task;
1. when half of the scout jobs finished, and
2. when the first 100 jobs finished after the task avalanched.
Some task parameters specify the resource amount per event. If input data don’t have event information, the number of
events in each file is internally regarded as 1.
15.1 cpuTime
cpuTime is calculated for each job using the following formula:
𝑐𝑝𝑢𝑇𝑖𝑚𝑒= 𝑚𝑎𝑥(0, 𝑒𝑛𝑑𝑇𝑖𝑚𝑒−𝑠𝑡𝑎𝑟𝑡𝑇𝑖𝑚𝑒−𝑏𝑎𝑠𝑒𝑇𝑖𝑚𝑒) × 𝑐𝑜𝑟𝑒𝑃𝑜𝑤𝑒𝑟× 𝑐𝑜𝑟𝑒𝐶𝑜𝑢𝑛𝑡× 𝑐𝑝𝑢𝐸𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑐𝑦× 1.5
𝑛𝐸𝑣𝑒𝑛𝑡𝑠
where corePower is the HS06 core-power at the computing resource, cpuEfficiency is a task parameter representing
CPU efficiency and defaults to 90%, coreCount is the number of CPU cores that the job used, baseTime is another
task parameter representing the part of the job execution time not scaling with CPU power, such as initialization and
finalization steps, and nEvents is the number of events processed in the job. The 95th percentile of cpuTime of scout
jobs
• with nEvents 10 coreCount, or
• with fewer nEvents but endTime-startTime 6h
is used as a task parameter to estimate the expected execution time for remaining jobs. Other scout jobs with fewer
events and short execution time are ignored since they tend to skew the estimation. The percentile rank can be defined
as SCOUT_RAMCOUNT_RANK in gdpconfig.
cpuTimeUnit is a task parameter for the unit of cpuTime and is one of HS06sPerEvent, mHS06sPerEvent,
HS06sPerEventFixed, mHS06sPerEventFixed. The m prefix means that the cpuTime value is in milliseconds. If the
Fixed suffix is used, scout jobs don’t overwrite the preset cpuTime value.
Tasks can set cpuEfficiency to 0 to disable scaling with the number of events.
309
PanDAWMS
15.2 ramCount
The pilot monitors the memory usage of the job and reports the information to the PanDA server. ramCount is calcu-
lated for each job using the following formula:
𝑟𝑎𝑚𝐶𝑜𝑢𝑛𝑡= 𝑚𝑎𝑥(𝑚𝑎𝑥𝑃𝑆𝑆−𝑏𝑎𝑠𝑒𝑅𝑎𝑚𝐶𝑜𝑢𝑛𝑡
𝑐𝑜𝑟𝑒𝐶𝑜𝑢𝑛𝑡
× 𝑚𝑎𝑟𝑔𝑖𝑛, 𝑚𝑖𝑛𝑅𝑎𝑚𝐶𝑜𝑢𝑛𝑡)
It is the RSS per core, allowing some offset (baseRamCount) independent of core count (coreCount). baseRamCount is
a preset task parameter ad is not very important for single-core tasks. margin is defined as SCOUT_RAMCOUNT_MARGIN
in gdpconfig and 10 by default. If minRamCount is defined as SCOUT_RAMCOUNT_MIN in gdpconfig, it is used as the
lower limit.
The 75th percentile of ramCount of scout jobs is used as a task parameter to estimate the expected memory usage for
remaining jobs. The percentile rank can be defined as SCOUT_RAMCOUNT_RANK in gdpconfig.
ramCountUnit is a task parameter for the unit of ramCount and is either MBPerCore or MBPerCoreFixed. If the
latter, scout jobs don’t overwrite the preset value.
15.3 outDiskCount and workDiskCount
The 75th percentile of the total output size per event of scout jobs outDiskCount is used to estimate the output size
of remaining jobs. Scout jobs with less than ten events are ignored.
The pilot reports the total size of the working directory workDiskCount while the job is running. The maximum value
of workDiskCount of scout jobs is used to estimate the expected scratch disk usage of the remaining jobs. Note that
scout jobs don’t overwrite the preset workDisCount value when the measured value is smaller.
15.4 ioIntensity
ioIntensity is the total size of job input and output divided by the job execution time which roughly corresponds
to the data traffics over the wide-area network. The maximum value of ioIntensity is used in the job brokerage to
avoid redundant heavy data motion over WAN.
15.5 diskIO
The pilot reports the data size the job read and wrote from and to the local disk storage. diskIO is calculated for each
job using the following formula:
𝑑𝑖𝑠𝑘𝐼𝑂= 𝑚𝑖𝑛(𝑡𝑜𝑡𝑅𝐵𝑌𝑇𝐸𝑆+ 𝑡𝑜𝑡𝑊𝐵𝑌𝑇𝐸𝑆
𝑒𝑛𝑑𝑇𝑖𝑚𝑒−𝑠𝑡𝑎𝑟𝑡𝑇𝑖𝑚𝑒
, 𝑐𝑎𝑝𝑂𝑛𝐷𝑖𝑠𝑘𝐼𝑂)
roughly corresponding to the data traffics over the local-area network.
capOnDiskIO is defined as
SCOUT_DISK_IO_CAP in gdpconfig. used in the job brokerage to distribute IO-intensive workloads over many disk
storages.
310
Chapter 15. Dynamic Optimization of Task Parameters
PanDAWMS
15.6 nGBPerJob
JEDI generates jobs so that the expected disk usage of those jobs is less than a limit if the task parameter nGBPerJob
is specified. The parameter is adjusted based on outDiskCount and workDiskCout optimized by scout jobs, if the
task sets the target size of the output size, tgtMaxOutputForNG.
15.7 taskStatus=exhausted
The task status is set to exhausted when scouts detect
• huge memory leaks (the threshold is defined as SCOUT_MEM_LEAK_PER_CORE_<activity> in gdpconfig),
• too many jobs with short execution time (the time limit is defined as SCOUT_SHORT_EXECTIME_<activity> in
gdpconfig),
• the calculated ramCount or cpuTime so different from preset values,
• very low CPU efficiency (the threshold is defined as a task parameter minCpuEfficiency), or
• non-allocated CPUs being abused
to ask for user’s actions since they indicate those tasks are wrongly configured.
15.6. nGBPerJob
311
PanDAWMS
312
Chapter 15. Dynamic Optimization of Task Parameters
CHAPTER
SIXTEEN
USING MESSAGE PROCESSOR
JEDI Message Processor can talk to other systems through message brokers which supports STOMP protocol (e.g.
ActiveMQ, RabbitMQ, etc.).
16.1 JEDI Configuration
The configFile parameter: Specify the path of the json configuration file for Message Processor . If commented,
JEDI Message Processor will be disabled.
[msgprocessor]
# json config file of message processors
configFile = /etc/panda/jedi_msg_proc_config.json
16.2 JSON Configuration File
An example of the JSON content in configFile:
{
"mb_servers": {
"iDDS_mb": {
"host_port_list": ["some-mb.cern.ch:1234"],
"use_ssl": false,
"username": "<username>",
"passcode": "<passcode>",
"verbose": true
},
"rucio_mb": {
"host_port_list": ["another-mb.cern.ch:5678"],
"use_ssl": true,
"cert_file": "/path/of/cert",
"key_file": "/path/of/key",
"vhost": "/"
}
},
"queues": {
"idds": {
"server": "iDDS_mb",
(continues on next page)
313
PanDAWMS
(continued from previous page)
"destination": "/queue/Consumer.jedi.atlas.idds"
},
"rucio-events": {
"server": "rucio_mb",
"destination": "/queue/Consumer.panda.rucio.events"
}
},
"processors": {
"atlas-idds": {
"enable": true,
"module": "pandajedi.jedimsgprocessor.atlas_idds_msg_processor",
"name": "AtlasIddsMsgProcPlugin",
"in_queue": "idds",
"verbose": true
},
"panda-callback": {
"enable": true,
"module": "pandajedi.jedimsgprocessor.panda_callback_msg_processor",
"name": "PandaCallbackMsgProcPlugin",
"in_queue": "rucio-events"
}
}
}
In the JSON object, the configuration of message broker servers, queues, and message processors are defined.
Message Broker Servers
Defined under "mb_servers" object. In the "mb_servers" object, a key can be any arbitrary name standing for the
message broker server. In the example above, there are 2 message broker servers, named “iDDS_mb” and “rucio_mb”.
Parameters of a message broker server:
• "host_port_list": A list of host:port of the message broker servers. If multiple host:port are put in the list,
only random one of them will be connected and the others will be failover candidates. Also in host;port if a
hostname is used instead of IP address, all IP addresses mapped to the hostname according to DNS resolution
will be connected. Mandatory
• "use_ssl": STOMP option, whether to use SSL in authentication. Default is false
• "username" and "passcode": STOMP option, authenticate the message broker server with username and
passcode. Default is null
• "cert_file" and "key_file": STOMP option, authenticate the message broker server with key/cert pair.
Default is null
• "vhost": STOMP option, vhost of the message broker. Default is null
• "verbose": Whether to log verbosely about communication details with this message broker server. Default is
false
Queues
Defined under "queues" object. In the "queues" object, a key can be any arbitrary name standing for a message
queue. In the example above, there are 2 message queues, named “idds” and “rucio-events”.
Parameters of a message queue:
• "server": Name of the message broker server defined under "mb_servers" for this message queue. Mandatory
314
Chapter 16. Using Message Processor
PanDAWMS
• "destination": STOMP option, destination path on the message broker server for this message queue. Manda-
tory
Message Processors
Defined under "processors" object
In the "processors" object, a key can be any arbitrary name standing for a message processor. A message processor
running on JEDI consumes a message from a message queue and processes the message (and some message processor
sends a new message to another message queue). There are various message processor plugins for different workflows.
All message processors available in JEDI are in the message processor plugin repository.
Parameters of a message broker server:
• "enable": Whether to enable this message processor. Useful when one needs to stop the message processor
temporarily but still wants to keep it the configuration file. Default is true
• "module" and "name": Module and class name of the message processor plugin in JEDI. Mandatory
• "in_queue": Queue name defined under "queues" object, where the message processor consumes messages
from this queue. Default is null
• "out_queue": Queue name defined under "queues" object, where the message processor sends messages to
this queue. Not required if the processor does not send out messages. Default is null
• "verbose": Whether to log verbosely about this message processor. Default is false
16.2. JSON Configuration File
315
PanDAWMS
316
Chapter 16. Using Message Processor
CHAPTER
SEVENTEEN
INTEGRATION WITH CRIC
The Computing Resource Information System (CRIC) is a framework providing a centralized and flexible way to de-
scribe resources and their usage.
It is possible to integrate PanDA and CRIC so that administrators register various resources in CRIC by using WebUI
and PanDA fetches information from CRIC, avoiding manual registration in the database. One of the scripts launched
by PanDA daemon, configurator, periodically retrieves information from CRIC and populates database tables. To
enable PanDA daemon and configurator, you need in panda_server.cfg
[daemon]
# whether to run daemons for PanDA
enable = True
config = {
...
"configurator": {
"enable": true, "module": "configurator", "period": 200, "sync": true},
and set the following parameters:
Name
Description
Example
CRIC_URL_SCHEDCONFIG
URL to get sched-
config json
https://datalake-cric.cern.ch/api/atlas/pandaqueue/query/?json
CRIC_URL_SITES
URL to get site
json
https://datalake-cric.cern.ch/api/atlas/site/query/?json
CRIC_URL_DDMENDPOINTS
URL to get storage
json
https://datalake-cric.cern.ch/api/atlas/ddmendpoint/query/?json
CRIC_URL_DDMBLACKLIST
URL to get black-
list json
https://datalake-cric.cern.ch/api/atlas/ddmendpointstatus/query/
?json&activity=write_wan&fstate=OFF
CRIC_URL_CM
URL to get site
matrix json
https://atlas-cric.cern.ch/api/core/sitematrix/query/?json&json_
pretty=0
317
PanDAWMS
318
Chapter 17. Integration with CRIC
CHAPTER
EIGHTEEN
DEPLOYMENT OF CUSTOM IAM
This document explains how to deploy a custom Identity and Access Management (IAM) service.
18.1 1. Installation of Indigo IAM
First, you need to install Indigo IAM following Indigo IAM admin guide. Make sure that PUT and DELETE methods
are allowed when configuring Nginx since it is sometimes recommended to disable them for security purposes. Also,
you need to install a trusted host certificate to Nginx so that the IAM instance can communicate with external services
like CILogon.
18.2 2. Registration in CILogon
Next, register the IAM instance on CILogon OIDC client registration portal where the most important information you
need to provide is “Callback URLs” and “Scopes”. The former is the URL to which CILogon sends a callback once an
external identity provider successfully authenticates the user, and the scopes define OIDC claims the client receives.
They must be something like
Scopes: [org.cilogon.userinfo, profile, email, openid]
Callbacks: [https://{your_iam_hostname}/openid_connect_login]
You don’t have to enable Refresh Tokens.
Once your registration request is approved, you will get a client ID and secret and specify them in the IAM configuration.
18.3 3. Enabling Brokered OIDC auth through CILogon
CILogon must be added as an OIDC provider in /etc/iam-login-service/config/application-oidc.yml.
oidc:
providers:
- name: cilogon
issuer: https://cilogon.org
client:
clientId: ${IAM_CILOGON_CLIENT_ID}
clientSecret: ${IAM_CILOGON_CLIENT_SECRET}
redirectUris: ${iam.baseUrl}/openid_connect_login
scope: openid,profile,email,org.cilogon.userinfo
(continues on next page)
319
PanDAWMS
(continued from previous page)
loginButton:
text: Your ID Provider
style: btn-primary
image:
fa-icon: none
where IAM_CILOGON_CLIENT_ are specified in /etc/sysconfig/iam-login-service.
18.4 4. Customization
You can modify /etc/sysconfig/iam-login-service. E.g.,
# Java VM arguments
IAM_JAVA_OPTS=-Dspring.profiles.active=prod,registration,oidc
# Generic options
IAM_HOST=localhost
IAM_PORT=8080
IAM_BASE_URL=https://panda-iam-doma.cern.ch
IAM_ISSUER=https://panda-iam-doma.cern.ch
IAM_USE_FORWARDED_HEADERS=true
IAM_KEY_STORE_LOCATION=file:///opt/iam/iam-keystore.jwks
IAM_ORGANISATION_NAME=PanDA-DOMA
# customization for PanDA
IAM_TOPBAR_TITLE="PanDA DOMA"
IAM_ACCESS_TOKEN_INCLUDE_AUTHN_INFO=true
IAM_LOCAL_AUTHN_LOGIN_PAGE_VISIBILITY=hidden
IAM_REGISTRATION_OIDC_ISSUER=https://cilogon.org
IAM_CILOGON_CLIENT_ID=<your client ID>
IAM_CILOGON_CLIENT_SECRET=<your client secret>
where you need oidc in IAM_JAVA_OPTS and specify your client ID and secret in IAM_CILOGON_CLIENT_*.
18.5 5. Start IAM
Now you can start the IAM instance.
$ service iam-login-service start
Then go to https://{your_iam_hostname}/login?sll=y to enter the admin page as the local auth login page is hidden due
to IAM_LOCAL_AUTHN_LOGIN_PAGE_VISIBILITY in /etc/sysconfig/iam-login-service.
Note that normal users should go to https://{your_iam_hostname}/login.
320
Chapter 18. Deployment of Custom IAM
CHAPTER
NINETEEN
WORKING WITH IDDS
iDDS is an intelligent data delivery service orchestrating workflow and data management systems to optimize resource
usage in various workflows. It is possible to use iDDS on top of PanDA as a high-level service to support various
advanced workflows, such as
• Fine-grained data carousel
• Hyperparameter optimization among geographically distributed GPU resources
• Task chaining with directed acyclic graph
You need to configure Message Processor in JEDI so that iDDS and PanDA can talk through ActiveMQ. There is a
parameter in panda_jedi.cfg to specify the json configuration file for Message Processor.
[msgprocessor]
# json config file of message processors
configFile = /etc/panda/jedi_msg_proc_config.json
The contents of the json is something like
{
"mb_servers": {
"iDDS_mb": {
"host_port_list": ["atlas-mb.cern.ch:61013"],
"use_ssl": false,
"username": <user_name>,
"passcode": <password>,
"verbose": true
}
},
"queues": {
"idds": {
"server": "iDDS_mb",
"destination": "/queue/Consumer.jedi.atlas.idds"
}
},
"processors": {
"atlas-idds": {
"enable": true,
"module": "pandajedi.jedimsgprocessor.atlas_idds_msg_processor",
"name": "IddsMsgProcPlugin",
"in_queue": "idds",
"verbose": true
(continues on next page)
321
PanDAWMS
(continued from previous page)
}
}
}
where you specify the ActiveMQ server, user name, and password to connect to ActiveMQ, queue names, and plugins
to consume messages from iDDS. See Each workflow has a separate plugin in the plugin repository. You choose
appropriate plugins based on your needs.
A complete configuration guide for Message Processsor is availabe at the Using Message Processor page.
322
Chapter 19. Working with iDDS
CHAPTER
TWENTY
JOB RETRY MODULE
Jobs can fail for different reasons. The Job Retry Module greatly simplifies operations by taking actions based on error
codes/messages.
20.1 Actions
Here is a description of the currently available actions.
NAME
DESCRIPTION
no_retry
Do not retry the job again for certain hopeless errors.
limit_retry
Limit the number of retries to a certain maximum.
increase memory
Submit next job retries with a higher memory requirement.
increase CPU time
Submit next job retries with a longer walltime requirement.
Retry actions are recorded in the database table RETRYACTIONS. New actions need to be implemented and then regis-
tered in the table.
20.2 Rules
Rules are recorded in the database table RETRYERRORS. For new rules you have to specify:
• ID: unique ID
• Retry action: which action from the previous section you want to invoke
• Error source: the source of the error (payload, pilot, job dispatcher, task buffer)
• Error code
• Error message: a regular expression in python syntax (https://docs.python.org/3/library/re.html) to match
the error message. You can check your regular expressions in online services like https://pythex.org/ if you
don’t want to write the pythong snippet.
• Active: you can choose to run the rule in passive mode. In this case there will only be a log message
indicating that the rule would have been invoked, but it has no effect. This option is useful when you are
not sure of the scope of your new rule.
• Parameters: valid only for certain actions, such as limit_retry, where you want to specify the limit of
retries.
• Scope: there are a couple of columns (architecture, release, workqueue), where you can limit the scope of
the new rule. For example if you want to apply the rule only for a certain software release.
323
PanDAWMS
324
Chapter 20. Job Retry Module
CHAPTER
TWENTYONE
JEDI WATCHDOGS
JEDI Watchdogs are dogs. Each watchdog has specific periodic mission to execute.
Watchdogs run independently of JEDI agents and of other watchdogs; i.e. they can run in parallel without blocking
one another.
Watchdog is plugin-based so one can write new watchdogs to extend features of JEDI.
21.1 JEDI Configuration
General watchdog configuration is in jedi configuration
Specific configuration is mentioned in the section of each watchdog below.
21.1.1 Data Locality Updater
module
path:
pandajedi.jedidog.AtlasDataLocalityUpdaterWatchDog
class
name:
AtlasDataLocalityUpdaterWatchDog
configuration example:
[watchdog]
modConfig = (...),atlas:managed:pandajedi.jedidog.AtlasDataLocalityUpdaterWatchDog:
˓→AtlasDataLocalityUpdaterWatchDog:DataLocalityUpdater,(...)
procConfig = (...);atlas:managed:1:DataLocalityUpdater:43200;(...)
Here we ask JEDI to run 1 process of Data Locality Updater with a period of 43200 seconds (12 hours).
requirement: JEDI_Dataset_Locality table exists in PanDA DB.
Description
Data Locality Updater queries Rucio periodically about the RSEs which store input datasets of all active production
tasks.
The results are stored in JEDI_Dataset_Locality table.
This allows other JEDI components to find the available RSEs (and hence PQs) for a task from the DB, which is more
efficient than to query Rucio for the task on the fly.
GDPconfig Parameters
325
PanDAWMS
(None)
21.1.2 Task Withholder
module
path:
pandajedi.jedidog.AtlasTaskWithholderWatchDog
class
name:
AtlasTaskWithholderWatchDog
configuration example:
[watchdog]
modConfig = (...),atlas:managed:pandajedi.jedidog.AtlasTaskWithholderWatchDog:
˓→AtlasTaskWithholderWatchDog:TaskWithholder,(...)
procConfig = (...);atlas:managed:1:TaskWithholder:1800;(...)
Here we ask JEDI to run 1 process of Task Withholder with a period of 1800 seconds (30 minutes).
requirement: Data Locality Updater watchdog is running
Description
A tasks can become temporarily hopeless to run when all sites containing the required inputs are too busy or offline.
Task Withholder finds out these apparent hopeless tasks and withhold them by setting them to be in pending status, so
that JEDI brokerage does not need to broker for these tasks temporarily and thus increase performance.
The tasks set to be pending will be released as other pending tasks for other reasons will.
GDPconfig Parameters
• jedi.task_withholder.LIMIT_IOINTENSITY_managed : For prodSourceLabel=managed, only tasks with
ioIntensity higher than this value will be considered to be withheld
• jedi.task_withholder.LIMIT_PRIORITY_managed : For prodSourceLabel=managed, only tasks with cur-
rentPriority lower than this value will be considered to be withheld
21.1.3 Queue Filler
module path: pandajedi.jedidog.AtlasQueueFillerWatchDog class name: AtlasQueueFillerWatchDog
configuration example:
[watchdog]
modConfig = (...),atlas:managed:pandajedi.jedidog.AtlasQueueFillerWatchDog:
˓→AtlasQueueFillerWatchDog:QueueFiller,(...)
procConfig = (...);atlas:managed:1:QueueFiller:600;(...)
Here we ask JEDI to run 1 process of Queue Filler with a period of 600 seconds (10 minutes).
requirement: Data Locality Updater watchdog is running
Description
Queue Filler finds out empty sites and pre-assigns some proper production tasks to these sites to fill them with jobs.
Here a proper production tasks satisfies: * The certain site to be filled has the input datasets of the task * The tasks
satisfies the constraints about the site (defined on CRIC); e.g. memory * The task still has enough ready but un-
processed files, so that it will have enough jobs to fill the queue
326
Chapter 21. JEDI Watchdogs
PanDAWMS
When a site is no longer empty, the tasks pre-assign to the site by Queue Filler will be released.
GDPconfig Parameters
• jedi.queue_filler.MAX_PREASSIGNED_TASKS_managed : Max number of production tasks to pre-assign
to an empty queue per resource type
• jedi.queue_filler.MIN_FILES_READY_managed : Min number of ready files a task which can be pre-
assigned should have
• jedi.queue_filler.MIN_FILES_REMAINING_managed : Min number of remaining files a task which can be
pre-assigned should have
21.1. JEDI Configuration
327
PanDAWMS
328
Chapter 21. JEDI Watchdogs
PYTHON MODULE INDEX
p
pilot.api.analytics, 122
pilot.api.benchmark, 125
pilot.api.data, 125
pilot.api.memorymonitor, 129
pilot.api.services, 130
pilot.common.errorcodes, 130
pilot.common.exception, 139
pilot.control.data, 143
pilot.control.job, 146
pilot.control.monitor, 157
pilot.control.payload, 158
pilot.control.payloads.eventservice, 160
pilot.control.payloads.generic, 161
pilot.copytool.common, 164
pilot.copytool.gfal, 166
pilot.copytool.lsm, 166
pilot.copytool.mv, 167
pilot.copytool.rucio, 168
pilot.copytool.xrdcp, 170
pilot.eventservice.esprocess, 170
pilot.info.basedata, 171
pilot.info.configinfo, 172
pilot.info.dataloader, 173
pilot.info.extinfo, 174
pilot.info.infoservice, 175
pilot.info.jobdata, 177
pilot.info.jobinfo, 183
pilot.info.jobinfoservice, 184
pilot.info.queuedata, 184
pilot.info.storagedata, 186
pilot.resource.alcf, 187
pilot.resource.bnl, 187
pilot.resource.generic, 188
pilot.resource.nersc, 188
pilot.resource.summit, 188
pilot.resource.titan, 188
pilot.user.atlas.common, 189
pilot.user.atlas.container, 200
pilot.user.atlas.jobmetrics, 206
pilot.user.atlas.loopingjob_definitions, 207
pilot.user.atlas.memory, 207
pilot.user.atlas.nordugrid, 207
pilot.user.atlas.proxy, 209
pilot.user.atlas.setup, 210
pilot.user.atlas.utilities, 212
pilot.user.generic.common, 217
pilot.user.generic.container, 220
pilot.user.generic.jobmetrics, 220
pilot.user.generic.loopingjob_definitions,
221
pilot.user.generic.memory, 221
pilot.user.generic.proxy, 221
pilot.user.generic.setup, 221
pilot.util.auxiliary, 222
pilot.util.config, 225
pilot.util.constants, 226
pilot.util.container, 226
pilot.util.disk, 226
pilot.util.filehandling, 227
pilot.util.harvester, 234
pilot.util.https, 236
pilot.util.jobmetrics, 239
pilot.util.loopingjob, 240
pilot.util.math, 241
pilot.util.monitoring, 244
pilot.util.monitoringtime, 247
pilot.util.parameters, 247
pilot.util.processes, 248
pilot.util.proxy, 251
pilot.util.timer, 251
pilot.util.timing, 253
pilot.util.workernode, 256
pilot.workflow.generic, 257
329
PanDAWMS
330
Python Module Index
INDEX
Symbols
_ConfigurationSection (class in pilot.util.config),
225
__contains__() (pilot.info.jobdata.JobData method),
177
__dict__ (pilot.api.analytics.Fit attribute), 124
__dict__ (pilot.api.data.StagingClient attribute), 127
__dict__ (pilot.api.services.Services attribute), 130
__dict__
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 134
__dict__ (pilot.control.payloads.generic.Executor at-
tribute), 161
__dict__ (pilot.info.basedata.BaseData attribute), 171
__dict__ (pilot.info.configinfo.PilotConfigProvider at-
tribute), 172
__dict__ (pilot.info.dataloader.DataLoader attribute),
173
__dict__ (pilot.info.infoservice.InfoService attribute),
176
__dict__ (pilot.info.jobinfo.JobInfoProvider attribute),
183
__dict__ (pilot.user.atlas.common.DictQuery attribute),
189
__dict__ (pilot.user.atlas.nordugrid.XMLDictionary at-
tribute), 208
__dict__
(pilot.util.config._ConfigurationSection
attribute), 225
__dict__ (pilot.util.https.ctx attribute), 237
__dict__ (pilot.util.monitoringtime.MonitoringTime at-
tribute), 247
__dict__ (pilot.util.timer.TimedProcess attribute), 251
__dict__ (pilot.util.timer.TimedThread attribute), 252
__getattr__() (pilot.util.config._ConfigurationSection
method), 225
__getitem__()
(pilot.info.jobdata.JobData
method),
177
__getitem__() (pilot.util.config._ConfigurationSection
method), 225
__getnewargs__() (pilot.util.https._ctx method), 236
__init__() (pilot.api.analytics.Analytics method), 122
__init__() (pilot.api.analytics.Fit method), 124
__init__() (pilot.api.benchmark.Benchmark method),
125
__init__() (pilot.api.data.StagingClient method), 127
__init__() (pilot.api.memorymonitor.MemoryMonitoring
method), 129
__init__() (pilot.api.services.Services method), 130
__init__() (pilot.common.exception.BadXML method),
139
__init__() (pilot.common.exception.CommunicationFailure
method), 139
__init__() (pilot.common.exception.ConversionFailure
method), 139
__init__() (pilot.common.exception.ESFatal method),
139
__init__()
(pilot.common.exception.ESNoEvents
method), 140
__init__()
(pilot.common.exception.ESRecoverable
method), 140
__init__()
(pilot.common.exception.ExcThread
method), 140
__init__() (pilot.common.exception.ExceededMaxWaitTime
method), 140
__init__() (pilot.common.exception.ExecutedCloneJob
method), 140
__init__() (pilot.common.exception.FileHandlingFailure
method), 140
__init__() (pilot.common.exception.JobAlreadyRunning
method), 141
__init__() (pilot.common.exception.LogFileCreationFailure
method), 141
__init__()
(pilot.common.exception.MKDirFailure
method), 141
__init__()
(pilot.common.exception.MessageFailure
method), 141
__init__()
(pilot.common.exception.NoGridProxy
method), 141
__init__()
(pilot.common.exception.NoLocalSpace
method), 141
__init__()
(pilot.common.exception.NoSoftwareDir
method), 141
__init__()
(pilot.common.exception.NoSuchFile
method), 141
__init__()
(pilot.common.exception.NoVomsProxy
331
PanDAWMS
method), 141
__init__()
(pilot.common.exception.NotDefined
method), 141
__init__()
(pilot.common.exception.NotSameLength
method), 142
__init__()
(pilot.common.exception.PilotException
method), 142
__init__() (pilot.common.exception.QueuedataFailure
method), 142
__init__() (pilot.common.exception.QueuedataNotOK
method), 142
__init__() (pilot.common.exception.ReplicasNotFound
method), 142
__init__() (pilot.common.exception.RunPayloadFailure
method), 142
__init__()
(pilot.common.exception.SetupFailure
method), 142
__init__()
(pilot.common.exception.SizeTooLarge
method), 142
__init__()
(pilot.common.exception.StageInFailure
method), 143
__init__()
(pilot.common.exception.StageOutFailure
method), 143
__init__() (pilot.common.exception.TrfDownloadFailure
method), 143
__init__() (pilot.common.exception.UnknownException
method), 143
__init__() (pilot.control.payloads.eventservice.Executor
method), 160
__init__()
(pilot.control.payloads.generic.Executor
method), 161
__init__()
(pilot.info.configinfo.PilotConfigProvider
method), 172
__init__() (pilot.info.extinfo.ExtInfoProvider method),
174
__init__() (pilot.info.infoservice.InfoService method),
176
__init__() (pilot.info.jobdata.JobData method), 177
__init__() (pilot.info.jobinfo.JobInfoProvider method),
184
__init__()
(pilot.info.jobinfoservice.JobInfoService
method), 184
__init__() (pilot.info.queuedata.QueueData method),
184
__init__()
(pilot.info.storagedata.StorageData
method), 186
__init__() (pilot.user.atlas.nordugrid.XMLDictionary
method), 208
__init__()
(pilot.util.monitoringtime.MonitoringTime
method), 247
__init__() (pilot.util.timer.TimedProcess method), 251
__init__() (pilot.util.timer.TimedThread method), 252
__init__() (pilot.util.timer.TimeoutException method),
252
__module__ (pilot.api.analytics.Analytics attribute), 122
__module__ (pilot.api.analytics.Fit attribute), 124
__module__ (pilot.api.benchmark.Benchmark attribute),
125
__module__ (pilot.api.data.StageInClient attribute), 125
__module__
(pilot.api.data.StageOutClient
attribute),
126
__module__ (pilot.api.data.StagingClient attribute), 127
__module__ (pilot.api.memorymonitor.MemoryMonitoring
attribute), 129
__module__ (pilot.api.services.Services attribute), 130
__module__ (pilot.common.errorcodes.ErrorCodes at-
tribute), 136
__module__
(pilot.common.exception.BadXML
at-
tribute), 139
__module__ (pilot.common.exception.CommunicationFailure
attribute), 139
__module__ (pilot.common.exception.ConversionFailure
attribute), 139
__module__ (pilot.common.exception.ESFatal attribute),
139
__module__
(pilot.common.exception.ESNoEvents
attribute), 140
__module__ (pilot.common.exception.ESRecoverable at-
tribute), 140
__module__
(pilot.common.exception.ExcThread
at-
tribute), 140
__module__ (pilot.common.exception.ExceededMaxWaitTime
attribute), 140
__module__ (pilot.common.exception.ExecutedCloneJob
attribute), 140
__module__ (pilot.common.exception.FileHandlingFailure
attribute), 140
__module__ (pilot.common.exception.JobAlreadyRunning
attribute), 141
__module__ (pilot.common.exception.LogFileCreationFailure
attribute), 141
__module__ (pilot.common.exception.MKDirFailure at-
tribute), 141
__module__
(pilot.common.exception.MessageFailure
attribute), 141
__module__ (pilot.common.exception.NoGridProxy at-
tribute), 141
__module__ (pilot.common.exception.NoLocalSpace at-
tribute), 141
__module__ (pilot.common.exception.NoSoftwareDir at-
tribute), 141
__module__
(pilot.common.exception.NoSuchFile
attribute), 141
__module__ (pilot.common.exception.NoVomsProxy at-
tribute), 141
__module__
(pilot.common.exception.NotDefined
attribute), 142
__module__
(pilot.common.exception.NotSameLength
332
Index
PanDAWMS
attribute), 142
__module__ (pilot.common.exception.PilotException at-
tribute), 142
__module__ (pilot.common.exception.QueuedataFailure
attribute), 142
__module__ (pilot.common.exception.QueuedataNotOK
attribute), 142
__module__ (pilot.common.exception.ReplicasNotFound
attribute), 142
__module__ (pilot.common.exception.RunPayloadFailure
attribute), 142
__module__ (pilot.common.exception.SetupFailure at-
tribute), 142
__module__ (pilot.common.exception.SizeTooLarge at-
tribute), 143
__module__ (pilot.common.exception.StageInFailure at-
tribute), 143
__module__
(pilot.common.exception.StageOutFailure
attribute), 143
__module__ (pilot.common.exception.TrfDownloadFailure
attribute), 143
__module__ (pilot.common.exception.UnknownException
attribute), 143
__module__ (pilot.control.payloads.eventservice.Executor
attribute), 160
__module__ (pilot.control.payloads.generic.Executor at-
tribute), 161
__module__
(pilot.info.basedata.BaseData
attribute),
171
__module__
(pilot.info.configinfo.PilotConfigProvider
attribute), 172
__module__
(pilot.info.dataloader.DataLoader
at-
tribute), 173
__module__
(pilot.info.extinfo.ExtInfoProvider
at-
tribute), 174
__module__ (pilot.info.infoservice.InfoService attribute),
176
__module__ (pilot.info.jobdata.JobData attribute), 177
__module__
(pilot.info.jobinfo.JobInfoProvider
at-
tribute), 184
__module__ (pilot.info.jobinfoservice.JobInfoService at-
tribute), 184
__module__ (pilot.info.queuedata.QueueData attribute),
185
__module__
(pilot.info.storagedata.StorageData
at-
tribute), 186
__module__
(pilot.user.atlas.common.DictQuery
at-
tribute), 189
__module__ (pilot.user.atlas.nordugrid.XMLDictionary
attribute), 208
__module__ (pilot.util.config._ConfigurationSection at-
tribute), 225
__module__ (pilot.util.https._ctx attribute), 236
__module__ (pilot.util.https.ctx attribute), 237
__module__
(pilot.util.monitoringtime.MonitoringTime
attribute), 247
__module__
(pilot.util.timer.TimedProcess
attribute),
252
__module__ (pilot.util.timer.TimedThread attribute), 252
__module__
(pilot.util.timer.TimeoutException
at-
tribute), 252
__new__() (pilot.util.https._ctx static method), 236
__repr__() (pilot.info.basedata.BaseData method), 171
__repr__()
(pilot.util.config._ConfigurationSection
method), 225
__repr__() (pilot.util.https._ctx method), 236
__setitem__()
(pilot.info.jobdata.JobData
method),
177
__slots__ (pilot.util.https._ctx attribute), 236
__str__() (pilot.common.exception.JobAlreadyRunning
method), 141
__str__()
(pilot.common.exception.PilotException
method), 142
__str__() (pilot.util.timer.TimeoutException method),
252
__weakref__ (pilot.api.analytics.Fit attribute), 124
__weakref__
(pilot.api.data.StagingClient
attribute),
127
__weakref__ (pilot.api.services.Services attribute), 130
__weakref__ (pilot.common.errorcodes.ErrorCodes at-
tribute), 136
__weakref__
(pilot.common.exception.PilotException
attribute), 142
__weakref__
(pilot.control.payloads.generic.Executor
attribute), 161
__weakref__ (pilot.info.basedata.BaseData attribute),
171
__weakref__ (pilot.info.configinfo.PilotConfigProvider
attribute), 172
__weakref__
(pilot.info.dataloader.DataLoader
at-
tribute), 173
__weakref__
(pilot.info.infoservice.InfoService
at-
tribute), 176
__weakref__
(pilot.info.jobinfo.JobInfoProvider
at-
tribute), 184
__weakref__
(pilot.user.atlas.common.DictQuery
attribute), 189
__weakref__ (pilot.user.atlas.nordugrid.XMLDictionary
attribute), 208
__weakref__ (pilot.util.config._ConfigurationSection at-
tribute), 226
__weakref__ (pilot.util.https.ctx attribute), 237
__weakref__ (pilot.util.monitoringtime.MonitoringTime
attribute), 247
__weakref__ (pilot.util.timer.TimedProcess attribute),
252
__weakref__ (pilot.util.timer.TimedThread attribute),
252
Index
333
PanDAWMS
__weakref__
(pilot.util.timer.TimeoutException
at-
tribute), 252
_asdict() (pilot.util.https._ctx method), 236
_chi2 (pilot.api.analytics.Fit attribute), 124
_cmd (pilot.api.memorymonitor.MemoryMonitoring at-
tribute), 129
_ctx (class in pilot.util.https), 236
_define_tabledict_keys()
(in
module
pi-
lot.util.filehandling), 227
_dictionary (pilot.user.atlas.nordugrid.XMLDictionary
attribute), 208
_do_stageout() (in module pilot.control.data), 143
_error_messages
(pi-
lot.common.errorcodes.ErrorCodes attribute),
136
_field_defaults (pilot.util.https._ctx attribute), 236
_fields (pilot.util.https._ctx attribute), 236
_fields_defaults (pilot.util.https._ctx attribute), 236
_fit (pilot.api.analytics.Analytics attribute), 122
_get_all_output()
(pilot.info.jobdata.JobData
method), 177
_get_trace() (in module pilot.copytool.rucio), 168
_intersect (pilot.api.analytics.Fit attribute), 124
_keys (pilot.info.basedata.BaseData attribute), 171
_keys (pilot.info.jobdata.JobData attribute), 177
_keys (pilot.info.queuedata.QueueData attribute), 185
_keys (pilot.info.storagedata.StorageData attribute), 186
_load_data() (pilot.info.basedata.BaseData method),
171
_make() (pilot.util.https._ctx class method), 236
_model (pilot.api.analytics.Fit attribute), 124
_ntuple_diskusage (in module pilot.util.disk), 226
_rawdata (pilot.info.jobdata.JobData attribute), 178
_replace() (pilot.util.https._ctx method), 236
_resolve_checksum_option()
(in
module
pi-
lot.copytool.xrdcp), 170
_resolve_data()
(pilot.info.infoservice.InfoService
class method), 176
_slope (pilot.api.analytics.Fit attribute), 124
_ss (pilot.api.analytics.Fit attribute), 124
_ss2 (pilot.api.analytics.Fit attribute), 124
_stage_in() (in module pilot.control.data), 143
_stage_in_api() (in module pilot.copytool.rucio), 168
_stage_in_bulk() (in module pilot.copytool.rucio),
168
_stage_out_api() (in module pilot.copytool.rucio),
169
_stage_out_new() (in module pilot.control.data), 143
_stagefile() (in module pilot.copytool.xrdcp), 170
_tester() (in module pilot.util.https), 236
_validate_job() (in module pilot.control.job), 146
_validate_payload()
(in
module
pi-
lot.control.payload), 158
_x (pilot.api.analytics.Fit attribute), 124
_xm (pilot.api.analytics.Fit attribute), 124
_y (pilot.api.analytics.Fit attribute), 124
_ym (pilot.api.analytics.Fit attribute), 124
A
accessmode (pilot.info.jobdata.JobData attribute), 178
acopytools (pilot.info.queuedata.QueueData attribute),
185
acopytools_schemas (pilot.info.queuedata.QueueData
attribute), 185
actualcorecount
(pilot.info.jobdata.JobData
at-
tribute), 178
add_analytics_data()
(in
module
pi-
lot.user.atlas.jobmetrics), 206
add_asetup() (in module pilot.user.atlas.container),
200
add_athena_proc_number()
(in
module
pi-
lot.user.atlas.common), 189
add_data_structure_ids()
(in
module
pi-
lot.control.job), 146
add_error_code()
(pi-
lot.common.errorcodes.ErrorCodes
method),
138
add_error_codes() (in module pilot.control.job), 146
add_event_number()
(in
module
pi-
lot.user.atlas.jobmetrics), 206
add_lists() (in module pilot.util.math), 241
add_makeflags() (in module pilot.user.atlas.common),
190
add_memory_info() (in module pilot.control.job), 147
add_replicas() (pilot.api.data.StagingClient method),
127
add_size() (pilot.info.jobdata.JobData method), 178
add_timing_and_extracts()
(in
module
pi-
lot.control.job), 147
add_to_list() (pilot.user.atlas.nordugrid.XMLDictionary
method), 208
add_to_pilot_timing() (in module pilot.util.timing),
253
add_to_total_size()
(in
module
pi-
lot.util.filehandling), 227
add_workdir_size()
(pilot.info.jobdata.JobData
method), 178
allow_lan (pilot.info.queuedata.QueueData attribute),
185
allow_loopingjob_detection()
(in
module
pi-
lot.user.atlas.loopingjob_definitions), 207
allow_loopingjob_detection()
(in
module
pi-
lot.user.generic.loopingjob_definitions), 221
allow_memory_usage_verifications() (in module
pilot.user.atlas.memory), 207
allow_memory_usage_verifications() (in module
pilot.user.generic.memory), 221
334
Index
PanDAWMS
allow_timefloor()
(in
module
pi-
lot.user.atlas.common), 190
allow_timefloor()
(in
module
pi-
lot.user.generic.common), 217
allow_wan (pilot.info.queuedata.QueueData attribute),
185
allownooutput (pilot.info.jobdata.JobData attribute),
178
alrb_wrapper() (in module pilot.user.atlas.container),
200
alrbuserplatform
(pilot.info.jobdata.JobData
at-
tribute), 178
Analytics (class in pilot.api.analytics), 122
appdir (pilot.info.queuedata.QueueData attribute), 185
aprotocols (pilot.info.queuedata.QueueData attribute),
185
arprotocols
(pilot.info.storagedata.StorageData
attribute), 186
astorages (pilot.info.queuedata.QueueData attribute),
185
attemptnr (pilot.info.jobdata.JobData attribute), 178
B
BADALLOC
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 130
BADMEMORYMONITORJSON
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
BADQUEUECONFIGURATION
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
BadXML, 139
BADXML (pilot.common.errorcodes.ErrorCodes attribute),
130
BaseData (class in pilot.info.basedata), 171
Benchmark (class in pilot.api.benchmark), 125
BLACKHOLE
(pilot.common.errorcodes.ErrorCodes
attribute), 130
bytes2human() (in module pilot.util.math), 241
C
cacert (pilot.util.https._ctx property), 236
cacert (pilot.util.https.ctx attribute), 237
cacert() (in module pilot.util.https), 237
cacert_default_location()
(in
module
pi-
lot.util.https), 237
cache_time (pilot.info.infoservice.InfoService attribute),
176
calculate_adler32_checksum()
(in
module
pi-
lot.util.filehandling), 227
calculate_checksum()
(in
module
pi-
lot.util.filehandling), 227
calculate_md5_checksum()
(in
module
pi-
lot.util.filehandling), 227
capath (pilot.util.https._ctx property), 236
capath (pilot.util.https.ctx attribute), 237
capath() (in module pilot.util.https), 237
catchall (pilot.info.queuedata.QueueData attribute),
185
check_availablespace()
(pi-
lot.api.data.StageInClient method), 125
check_for_abort_job() (in module pilot.control.job),
147
check_for_final_server_update() (in module pi-
lot.util.auxiliary), 222
check_for_gfal() (in module pilot.copytool.gfal), 166
check_for_lsm() (in module pilot.copytool.lsm), 166
check_hz() (in module pilot.util.workernode), 256
check_job_monitor_waiting_time() (in module pi-
lot.control.job), 147
check_local_space()
(in
module
pi-
lot.util.monitoring), 244
check_output_file_sizes()
(in
module
pi-
lot.util.monitoring), 244
check_payload_stdout()
(in
module
pi-
lot.util.monitoring), 244
check_work_dir() (in module pilot.util.monitoring),
244
chi2() (in module pilot.util.math), 241
chi2() (pilot.api.analytics.Analytics method), 122
chi2() (pilot.api.analytics.Fit method), 124
CHKSUMNOTSUP
(pilot.common.errorcodes.ErrorCodes
attribute), 130
CHMODTRF
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 130
clean() (pilot.info.basedata.BaseData method), 171
clean() (pilot.info.jobdata.JobData method), 178
clean() (pilot.info.queuedata.QueueData method), 185
clean__container_options()
(pi-
lot.info.queuedata.QueueData
method),
185
clean__container_type()
(pi-
lot.info.queuedata.QueueData
method),
185
clean__corecount()
(pilot.info.jobdata.JobData
method), 178
clean__corecount() (pilot.info.queuedata.QueueData
method), 185
clean__jobparams()
(pilot.info.jobdata.JobData
method), 178
clean__platform()
(pilot.info.jobdata.JobData
method), 178
clean__timefloor() (pilot.info.queuedata.QueueData
method), 185
clean_boolean()
(pilot.info.basedata.BaseData
method), 171
clean_dictdata()
(pilot.info.basedata.BaseData
method), 172
Index
335
PanDAWMS
clean_listdata()
(pilot.info.basedata.BaseData
method), 172
clean_numeric()
(pilot.info.basedata.BaseData
method), 172
clean_string()
(pilot.info.basedata.BaseData
method), 172
cleanup() (in module pilot.util.processes), 248
cleanup_broken_links()
(in
module
pi-
lot.user.atlas.common), 190
cleanup_looping_payload()
(in
module
pi-
lot.user.atlas.common), 190
cleanup_payload()
(in
module
pi-
lot.user.atlas.common), 190
collect_workernode_info()
(in
module
pi-
lot.util.workernode), 256
collect_zombies()
(pilot.info.jobdata.JobData
method), 179
command (pilot.info.jobdata.JobData attribute), 179
command_fix() (in module pilot.resource.titan), 188
CommunicationFailure, 139
COMMUNICATIONFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
config
(pilot.info.configinfo.PilotConfigProvider
at-
tribute), 172
container_options
(pilot.info.queuedata.QueueData
attribute), 185
container_type (pilot.info.queuedata.QueueData at-
tribute), 185
containerise_executable()
(in
module
pi-
lot.util.container), 226
containeroptions
(pilot.info.jobdata.JobData
at-
tribute), 179
control() (in module pilot.control.data), 144
control() (in module pilot.control.job), 147
control() (in module pilot.control.monitor), 157
control() (in module pilot.control.payload), 158
ConversionFailure, 139
CONVERSIONFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
convert() (in module pilot.util.filehandling), 227
convert_mb_to_b() (in module pilot.util.math), 242
convert_ps_to_dict()
(in
module
pi-
lot.util.processes), 248
convert_text_file_to_dictionary()
(in
module
pilot.user.atlas.utilities), 212
convert_to_int() (in module pilot.util.parameters),
247
convert_to_pilot_error_code()
(in
module
pi-
lot.util.auxiliary), 222
convert_to_prettyprint()
(in
module
pi-
lot.user.atlas.nordugrid), 208
convert_to_xml()
(in
module
pi-
lot.user.atlas.nordugrid), 208
convert_unicode_string()
(in
module
pi-
lot.user.atlas.utilities), 212
coprocess (pilot.info.jobdata.JobData attribute), 179
copy() (in module pilot.copytool.mv), 167
copy() (in module pilot.util.filehandling), 228
copy_in() (in module pilot.copytool.gfal), 166
copy_in() (in module pilot.copytool.lsm), 166
copy_in() (in module pilot.copytool.mv), 167
copy_in() (in module pilot.copytool.rucio), 169
copy_in() (in module pilot.copytool.xrdcp), 170
copy_in_bulk() (in module pilot.copytool.rucio), 169
copy_in_old() (in module pilot.copytool.lsm), 166
copy_out() (in module pilot.copytool.gfal), 166
copy_out() (in module pilot.copytool.lsm), 166
copy_out() (in module pilot.copytool.mv), 167
copy_out() (in module pilot.copytool.rucio), 169
copy_out() (in module pilot.copytool.xrdcp), 170
copy_out_old() (in module pilot.copytool.lsm), 167
copy_pilot_source()
(in
module
pi-
lot.util.filehandling), 228
copytool_in() (in module pilot.control.data), 144
copytool_modules
(pilot.api.data.StagingClient
at-
tribute), 127
copytool_out() (in module pilot.control.data), 144
copytools (pilot.info.queuedata.QueueData attribute),
185
corecount (pilot.info.jobdata.JobData attribute), 179
corecount (pilot.info.queuedata.QueueData attribute),
185
corecounts (pilot.info.jobdata.JobData attribute), 179
COREDUMP
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 130
cpuconsumptiontime (pilot.info.jobdata.JobData at-
tribute), 179
cpuconsumptionunit (pilot.info.jobdata.JobData at-
tribute), 179
cpuconversionfactor (pilot.info.jobdata.JobData at-
tribute), 179
create_core_dump() (in module pilot.util.loopingjob),
240
create_data_payload() (in module pilot.control.job),
148
create_job() (in module pilot.control.job), 148
create_k8_link() (in module pilot.control.job), 148
create_log() (in module pilot.control.data), 144
create_middleware_container_command() (in mod-
ule pilot.user.atlas.container), 200
create_output_list() (in module pilot.copytool.mv),
168
create_release_setup()
(in
module
pi-
lot.user.atlas.container), 201
create_release_setup_old()
(in
module
pi-
lot.user.atlas.container), 201
336
Index
PanDAWMS
create_root_container_command() (in module pi-
lot.user.atlas.container), 202
create_stagein_container_command() (in module
pilot.user.generic.container), 220
create_symlink() (in module pilot.util.filehandling),
228
create_trace_report()
(in
module
pi-
lot.control.data), 144
ctx (class in pilot.util.https), 237
currentsize (pilot.info.jobdata.JobData attribute), 179
cut_output() (in module pilot.util.auxiliary), 222
D
DataLoader (class in pilot.info.dataloader), 173
datasetin (pilot.info.jobdata.JobData attribute), 179
dbdata (pilot.info.jobdata.JobData attribute), 179
DBRELEASEFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
dbtime (pilot.info.jobdata.JobData attribute), 179
debug (pilot.info.jobdata.JobData attribute), 179
debug_command (pilot.info.jobdata.JobData attribute),
179
delayed_space_check() (in module pilot.control.job),
148
destinationdblock
(pilot.info.jobdata.JobData
attribute), 179
detect_client_location()
(pi-
lot.api.data.StagingClient
class
method),
128
DictQuery (class in pilot.user.atlas.common), 189
diff_lists() (in module pilot.util.math), 242
direct_access_lan
(pilot.info.queuedata.QueueData
attribute), 185
direct_access_wan
(pilot.info.queuedata.QueueData
attribute), 185
direct_localinput_allowed_schemas
(pi-
lot.api.data.StagingClient attribute), 128
direct_remoteinput_allowed_schemas
(pi-
lot.api.data.StagingClient attribute), 128
discover_new_outdata()
(in
module
pi-
lot.user.atlas.common), 190
discover_new_output()
(in
module
pi-
lot.user.atlas.common), 190
disk_usage() (in module pilot.util.disk), 226
display_architecture_info()
(in
module
pi-
lot.util.auxiliary), 222
display_oom_info() (in module pilot.util.monitoring),
244
do_use_container()
(in
module
pi-
lot.user.atlas.container), 202
do_use_container()
(in
module
pi-
lot.user.generic.container), 220
download_command()
(in
module
pi-
lot.user.atlas.common), 191
download_transform()
(in
module
pi-
lot.user.atlas.setup), 210
download_transform()
(in
module
pi-
lot.user.generic.setup), 221
dump() (in module pilot.util.filehandling), 228
dump() (in module pilot.util.harvester), 234
dump_stack_trace() (in module pilot.util.processes),
248
E
EMPTYOUTPUTFILE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
environment variable
X509_CERT_DIR, 237
X509_USER_PROXY, 237
ErrorCodes (class in pilot.common.errorcodes), 130
es_stageout_gap (pilot.info.queuedata.QueueData at-
tribute), 185
ESFatal, 139
ESFATAL
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 130
ESNoEvents, 139
ESNOEVENTS (pilot.common.errorcodes.ErrorCodes at-
tribute), 130
ESRecoverable, 140
ESRECOVERABLE (pilot.common.errorcodes.ErrorCodes
attribute), 130
establish_logging()
(in
module
pi-
lot.util.filehandling), 228
ExceededMaxWaitTime, 140
EXCEEDEDMAXWAITTIME
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
ExcThread (class in pilot.common.exception), 140
execute() (in module pilot.util.container), 226
execute() (pilot.api.memorymonitor.MemoryMonitoring
method), 129
execute() (pilot.util.timer.TimedThread method), 252
execute_payloads()
(in
module
pi-
lot.control.payload), 158
execute_request() (in module pilot.util.https), 237
execute_urllib() (in module pilot.util.https), 238
execute_utility_command()
(pi-
lot.control.payloads.generic.Executor method),
161
ExecutedCloneJob, 140
EXECUTEDCLONEJOB
(pi-
lot.common.errorcodes.ErrorCodes attribute),
130
Executor (class in pilot.control.payloads.eventservice),
160
Index
337
PanDAWMS
Executor (class in pilot.control.payloads.generic), 161
exeerrorcode
(pilot.info.jobdata.JobData
attribute),
179
exeerrordiag
(pilot.info.jobdata.JobData
attribute),
179
exitcode (pilot.info.jobdata.JobData attribute), 179
exitmsg (pilot.info.jobdata.JobData attribute), 179
ExtInfoProvider (class in pilot.info.extinfo), 174
extract_atlas_setup()
(in
module
pi-
lot.user.atlas.container), 202
extract_container_image()
(pi-
lot.info.jobdata.JobData method), 179
extract_from_table()
(pilot.api.analytics.Analytics
method), 122
extract_full_atlas_setup()
(in
module
pi-
lot.user.atlas.container), 202
extract_memory_usage_value()
(in
module
pi-
lot.util.auxiliary), 222
extract_output_file_guids()
(in
module
pi-
lot.user.atlas.common), 191
extract_platform_and_os()
(in
module
pi-
lot.user.atlas.container), 202
extract_setup()
(pi-
lot.control.payloads.generic.Executor method),
161
extract_stderr_error()
(pi-
lot.common.errorcodes.ErrorCodes
method),
138
extract_stderr_warning()
(pi-
lot.common.errorcodes.ErrorCodes
method),
138
extract_time_left()
(in
module
pi-
lot.user.atlas.proxy), 209
extract_turls() (in module pilot.user.atlas.common),
191
F
fail_monitored_job() (in module pilot.control.job),
148
failed_post() (in module pilot.control.payload), 159
FAILEDBYSERVER (pilot.common.errorcodes.ErrorCodes
attribute), 131
fast_job_monitor() (in module pilot.control.job), 148
fast_monitor_tasks() (in module pilot.control.job),
149
FILEEXISTS (pilot.common.errorcodes.ErrorCodes at-
tribute), 131
FileHandlingFailure, 140
FILEHANDLINGFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
fileinfo (pilot.info.jobdata.JobData attribute), 179
filter_files_for_log()
(in
module
pi-
lot.control.data), 145
find_cmd_pids() (in module pilot.util.processes), 248
find_executable() (in module pilot.util.filehandling),
229
find_last_line() (in module pilot.util.filehandling),
229
find_latest_modified_file()
(in
module
pi-
lot.util.filehandling), 229
find_pid() (in module pilot.util.processes), 248
find_processes_in_group()
(in
module
pi-
lot.util.processes), 248
find_text_files() (in module pilot.util.filehandling),
229
findfile() (in module pilot.util.harvester), 234
Fit (class in pilot.api.analytics), 124
fit() (pilot.api.analytics.Analytics method), 123
fit() (pilot.api.analytics.Fit method), 124
float_to_rounded_string()
(in
module
pi-
lot.util.math), 242
format_diagnostics()
(pi-
lot.common.errorcodes.ErrorCodes
method),
138
G
GENERALCPUCALCPROBLEM
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
GENERALERROR
(pilot.common.errorcodes.ErrorCodes
attribute), 131
get() (pilot.info.jobdata.JobData method), 179
get() (pilot.user.atlas.common.DictQuery method), 189
get()
(pilot.util.monitoringtime.MonitoringTime
method), 247
get_alrb_export() (in module pilot.user.atlas.setup),
210
get_analysis_run_command()
(in
module
pi-
lot.user.atlas.common), 191
get_analysis_run_command()
(in
module
pi-
lot.user.generic.common), 217
get_analysis_trf() (in module pilot.user.atlas.setup),
210
get_analysis_trf()
(in
module
pi-
lot.user.generic.setup), 221
get_and_verify_payload_proxy_from_server()
(in module pilot.user.atlas.container), 203
get_asetup() (in module pilot.user.atlas.setup), 210
get_asetup_options()
(in
module
pi-
lot.user.atlas.setup), 210
get_average_summary_dictionary() (in module pi-
lot.user.atlas.utilities), 212
get_average_summary_dictionary_prmon()
(in
module pilot.user.atlas.utilities), 212
get_batchsystem_jobid()
(in
module
pi-
lot.util.auxiliary), 222
338
Index
PanDAWMS
get_benchmark_setup()
(in
module
pi-
lot.user.atlas.utilities), 212
get_bucket()
(pilot.common.exception.ExcThread
method), 140
get_cgroups_base_path()
(in
module
pi-
lot.util.processes), 249
get_checksum_type()
(in
module
pi-
lot.util.filehandling), 229
get_checksum_value()
(in
module
pi-
lot.util.filehandling), 229
get_command() (pilot.api.memorymonitor.MemoryMonitoring
method), 129
get_condor_node_name()
(in
module
pi-
lot.util.workernode), 256
get_container_options()
(in
module
pi-
lot.user.atlas.container), 203
get_copysetup() (in module pilot.copytool.common),
164
get_cpu_consumption_time()
(in
module
pi-
lot.control.job), 149
get_cpu_consumption_time()
(in
module
pi-
lot.util.processes), 249
get_cpu_model() (in module pilot.util.workernode),
256
get_cpu_times() (in module pilot.user.atlas.common),
191
get_cpuinfo() (in module pilot.util.workernode), 256
get_curl_command() (in module pilot.util.https), 238
get_curl_config_option()
(in
module
pi-
lot.util.https), 238
get_current_cpu_consumption_time() (in module
pilot.util.processes), 249
get_cvmfs_path()
(pilot.info.extinfo.ExtInfoProvider
static method), 174
get_data_structure() (in module pilot.control.job),
149
get_db_info() (in module pilot.user.atlas.common),
191
get_db_info_str()
(in
module
pi-
lot.user.atlas.common), 191
get_ddmendpoint() (pilot.info.infoservice.InfoService
method), 176
get_debug_command() (in module pilot.control.job),
149
get_debug_stdout() (in module pilot.control.job), 149
get_default_copytools()
(pi-
lot.api.data.StagingClient
static
method),
128
get_detail() (pilot.common.exception.PilotException
method), 142
get_dictionary()
(pi-
lot.user.atlas.nordugrid.XMLDictionary
method), 208
get_direct_access_variables()
(pi-
lot.api.data.StageInClient method), 125
get_disk_space() (in module pilot.util.workernode),
256
get_disk_usage() (in module pilot.util.filehandling),
229
get_dispatcher_dictionary()
(in
module
pi-
lot.control.job), 149
get_display_info() (in module pilot.util.auxiliary),
222
get_distinguished_name()
(in
module
pi-
lot.util.proxy), 251
get_elapsed_real_time()
(in
module
pi-
lot.util.timing), 253
get_error_code()
(pi-
lot.common.exception.PilotException method),
142
get_error_code_translation_dictionary()
(in
module pilot.util.auxiliary), 223
get_error_info() (in module pilot.copytool.common),
164
get_error_message()
(pi-
lot.common.errorcodes.ErrorCodes
method),
138
get_event_status_file()
(in
module
pi-
lot.util.harvester), 234
get_exception_error_code()
(in
module
pi-
lot.util.monitoring), 244
get_executor_dictionary()
(in
module
pi-
lot.user.atlas.common), 192
get_executor_type()
(pi-
lot.control.payloads.eventservice.Executor
method), 160
get_exit_info() (in module pilot.user.atlas.common),
192
get_fake_job() (in module pilot.control.job), 150
get_file_info_from_output()
(in
module
pi-
lot.copytool.xrdcp), 170
get_file_last_update_time()
(pi-
lot.info.dataloader.DataLoader class method),
173
get_file_open_command()
(in
module
pi-
lot.user.atlas.common), 192
get_file_system_root_path()
(in
module
pi-
lot.user.atlas.setup), 210
get_file_transfer_info()
(in
module
pi-
lot.user.atlas.common), 192
get_filename() (pilot.api.memorymonitor.MemoryMonitoring
method), 129
get_files() (in module pilot.util.filehandling), 230
get_final_update_time()
(in
module
pi-
lot.util.timing), 253
get_finished_or_failed_job()
(in
module
pi-
lot.control.job), 150
get_fitted_data()
(pilot.api.analytics.Analytics
Index
339
PanDAWMS
method), 123
get_full_asetup()
(in
module
pi-
lot.user.atlas.container), 203
get_general_command_stdout()
(in
module
pi-
lot.control.job), 150
get_generic_payload_command()
(in
module
pi-
lot.user.atlas.common), 192
get_getjob_time() (in module pilot.util.timing), 253
get_grid_image_for_singularity() (in module pi-
lot.user.atlas.container), 203
get_guid() (in module pilot.util.filehandling), 230
get_guids_from_jobparams()
(in
module
pi-
lot.user.atlas.common), 192
get_heartbeat_period()
(in
module
pi-
lot.control.job), 150
get_initial_setup_time()
(in
module
pi-
lot.util.timing), 253
get_initial_work_report()
(in
module
pi-
lot.util.harvester), 234
get_input_file_dictionary()
(in
module
pi-
lot.control.data), 145
get_instant_cpu_consumption_time() (in module
pilot.util.processes), 249
get_job() (in module pilot.resource.titan), 188
get_job()
(pilot.control.payloads.generic.Executor
method), 162
get_job_definition() (in module pilot.control.job),
150
get_job_definition_from_file() (in module pi-
lot.control.job), 150
get_job_definition_from_server() (in module pi-
lot.control.job), 151
get_job_from_queue() (in module pilot.control.job),
151
get_job_label() (in module pilot.control.job), 151
get_job_metrics()
(in
module
pi-
lot.user.atlas.jobmetrics), 206
get_job_metrics()
(in
module
pi-
lot.user.generic.jobmetrics), 220
get_job_metrics() (in module pilot.util.jobmetrics),
239
get_job_metrics_entry()
(in
module
pi-
lot.util.jobmetrics), 240
get_job_metrics_string()
(in
module
pi-
lot.user.atlas.jobmetrics), 206
get_job_option_for_input_name()
(pi-
lot.info.jobdata.JobData method), 180
get_job_request_file_name()
(in
module
pi-
lot.util.harvester), 234
get_job_retrieval_delay()
(in
module
pi-
lot.control.job), 151
get_job_scheduler_id()
(in
module
pi-
lot.util.auxiliary), 223
get_job_status() (in module pilot.control.job), 151
get_job_status_from_server()
(in
module
pi-
lot.control.job), 151
get_key_value() (in module pilot.util.auxiliary), 223
get_kill_signal_error_code()
(pi-
lot.common.errorcodes.ErrorCodes
method),
138
get_kmap() (pilot.info.jobdata.JobData static method),
180
get_last_error()
(pi-
lot.common.exception.PilotException method),
142
get_last_value() (in module pilot.user.atlas.utilities),
213
get_latest_log_tail() (in module pilot.control.job),
152
get_lfns_and_guids()
(pilot.info.jobdata.JobData
method), 180
get_local_disk_space()
(in
module
pi-
lot.util.workernode), 256
get_local_file_size()
(in
module
pi-
lot.util.filehandling), 230
get_local_size_limit_stdout()
(in
module
pi-
lot.util.monitoring), 244
get_logger() (in module pilot.util.auxiliary), 223
get_looping_job_limit()
(in
module
pi-
lot.util.loopingjob), 240
get_ls() (in module pilot.control.job), 152
get_max_allowed_work_dir_size() (in module pi-
lot.util.monitoring), 244
get_max_input_size()
(in
module
pi-
lot.util.monitoring), 245
get_max_memory_monitor_value() (in module pi-
lot.user.atlas.utilities), 213
get_max_memory_usage_from_cgroups() (in module
pilot.util.processes), 249
get_max_running_time()
(in
module
pi-
lot.control.monitor), 157
get_max_workdir_size() (pilot.info.jobdata.JobData
method), 180
get_maximum_input_sizes()
(in
module
pi-
lot.util.parameters), 248
get_meminfo() (in module pilot.util.workernode), 257
get_memory_monitor_info()
(in
module
pi-
lot.user.atlas.utilities), 213
get_memory_monitor_info_path() (in module pi-
lot.user.atlas.utilities), 213
get_memory_monitor_output_filename() (in mod-
ule pilot.user.atlas.utilities), 213
get_memory_monitor_setup()
(in
module
pi-
lot.user.atlas.utilities), 213
get_memory_monitor_setup_old() (in module pi-
lot.user.atlas.utilities), 214
get_memory_monitor_summary_filename() (in mod-
ule pilot.user.atlas.utilities), 214
340
Index
PanDAWMS
get_memory_usage() (in module pilot.util.auxiliary),
223
get_memory_values()
(in
module
pi-
lot.user.atlas.utilities), 214
get_message_for_pattern()
(pi-
lot.common.errorcodes.ErrorCodes
method),
138
get_metadata() (in module pilot.user.atlas.common),
193
get_metadata()
(in
module
pi-
lot.user.generic.common), 217
get_metadata_dict_from_txt()
(in
module
pi-
lot.user.atlas.utilities), 215
get_middleware_container()
(in
module
pi-
lot.user.atlas.container), 203
get_middleware_container_script()
(in
module
pilot.user.atlas.container), 203
get_middleware_type()
(in
module
pi-
lot.user.atlas.container), 203
get_network_monitor_setup()
(in
module
pi-
lot.user.atlas.utilities), 215
get_node_name() (in module pilot.util.workernode),
257
get_nonexistant_path()
(in
module
pi-
lot.util.filehandling), 230
get_normal_payload_command()
(in
module
pi-
lot.user.atlas.common), 193
get_nthreads() (in module pilot.user.atlas.common),
193
get_number_in_string()
(in
module
pi-
lot.user.atlas.jobmetrics), 206
get_number_of_child_processes() (in module pi-
lot.util.processes), 249
get_object_size() (in module pilot.util.auxiliary),
223
get_opts_pargs() (pilot.info.jobdata.JobData static
method), 180
get_outfiles_records()
(in
module
pi-
lot.user.atlas.common), 193
get_panda_server() (in module pilot.control.job), 152
get_path()
(pilot.api.data.StageOutClient
class
method), 126
get_payload_command()
(in
module
pi-
lot.user.atlas.common), 193
get_payload_command()
(in
module
pi-
lot.user.generic.common), 217
get_payload_command()
(pi-
lot.control.payloads.generic.Executor method),
162
get_payload_environment_variables() (in module
pilot.user.atlas.setup), 210
get_payload_execution_time()
(in
module
pi-
lot.util.timing), 253
get_payload_executor()
(in
module
pi-
lot.control.payload), 159
get_payload_log_tail()
(in
module
pi-
lot.control.job), 152
get_payload_proxy()
(in
module
pi-
lot.user.atlas.container), 204
get_pid_for_command()
(in
module
pi-
lot.user.atlas.utilities), 215
get_pid_for_jobid()
(in
module
pi-
lot.user.atlas.utilities), 215
get_pid_for_trf()
(in
module
pi-
lot.user.atlas.utilities), 215
get_pid_from_command()
(in
module
pi-
lot.util.auxiliary), 223
get_pilot_id() (in module pilot.util.auxiliary), 223
get_pilot_pid_from_processes() (in module pi-
lot.util.processes), 249
get_pilot_state() (in module pilot.util.auxiliary),
223
get_pilot_work_dir()
(in
module
pi-
lot.util.filehandling), 230
get_postgetjob_time() (in module pilot.util.timing),
254
get_precopostprocess_command() (in module pi-
lot.user.atlas.common), 193
get_preferred_replica()
(pi-
lot.api.data.StagingClient
class
method),
128
get_prefetcher_setup()
(in
module
pi-
lot.user.atlas.utilities), 216
get_process_commands()
(in
module
pi-
lot.util.processes), 249
get_process_info() (in module pilot.control.monitor),
158
get_proper_pid() (in module pilot.user.atlas.utilities),
216
get_proper_state() (in module pilot.control.job), 152
get_protocol() (in module pilot.copytool.rucio), 169
get_ps_info() (in module pilot.user.atlas.utilities), 216
get_redundant_path()
(in
module
pi-
lot.user.atlas.common), 193
get_redundants()
(in
module
pi-
lot.user.atlas.common), 193
get_release_setup_name()
(in
module
pi-
lot.user.atlas.container), 204
get_requested_log_tail()
(in
module
pi-
lot.control.job), 152
get_resimevents()
(in
module
pi-
lot.user.atlas.common), 194
get_resource_name() (in module pilot.util.auxiliary),
224
get_results() (pilot.api.memorymonitor.MemoryMonitoring
method), 129
get_ret() (pilot.info.jobdata.JobData static method),
180
Index
341
PanDAWMS
get_root_container_script()
(in
module
pi-
lot.user.atlas.container), 204
get_rse() (in module pilot.control.data), 145
get_score() (in module pilot.util.monitoring), 245
get_security_key()
(pi-
lot.info.storagedata.StorageData
method),
186
get_setup() (in module pilot.resource.alcf ), 187
get_setup() (in module pilot.resource.bnl), 187
get_setup() (in module pilot.resource.generic), 188
get_setup() (in module pilot.resource.nersc), 188
get_setup() (in module pilot.resource.summit), 188
get_setup() (in module pilot.resource.titan), 188
get_setup_time() (in module pilot.util.timing), 254
get_size() (in module pilot.util.auxiliary), 224
get_size() (pilot.info.jobdata.JobData method), 180
get_special_setup()
(pi-
lot.info.storagedata.StorageData
method),
187
get_stagein_time() (in module pilot.util.timing), 254
get_stageout_label()
(in
module
pi-
lot.user.atlas.common), 194
get_stageout_time() (in module pilot.util.timing),
254
get_status() (pilot.info.jobdata.JobData method), 180
get_storage_id()
(pilot.info.infoservice.InfoService
method), 176
get_table() (pilot.api.analytics.Analytics method), 123
get_table_from_file()
(in
module
pi-
lot.util.filehandling), 230
get_task_id() (in module pilot.control.job), 152
get_time_difference() (in module pilot.util.timing),
254
get_time_for_last_touch()
(in
module
pi-
lot.util.loopingjob), 240
get_time_measurement() (in module pilot.util.timing),
255
get_time_since() (in module pilot.util.timing), 255
get_time_since_multijob_start() (in module pi-
lot.util.timing), 255
get_time_since_start() (in module pilot.util.timing),
255
get_timeout() (in module pilot.copytool.common), 165
get_total_pilot_time() (in module pilot.util.timing),
255
get_trace_report_variables()
(in
module
pi-
lot.control.data), 145
get_trf_command()
(in
module
pi-
lot.user.atlas.utilities), 216
get_trimmed_dictionary()
(in
module
pi-
lot.util.processes), 250
get_ucore_scale_factor()
(in
module
pi-
lot.user.atlas.memory), 207
get_urlopen2_output() (in module pilot.util.https),
238
get_urlopen_output() (in module pilot.util.https), 238
get_utility_after_payload_started() (in module
pilot.user.atlas.common), 194
get_utility_command()
(pi-
lot.control.payloads.generic.Executor method),
162
get_utility_command_execution_order() (in mod-
ule pilot.user.atlas.common), 194
get_utility_command_execution_order() (in mod-
ule pilot.user.generic.common), 217
get_utility_command_kill_signal()
(in
module
pilot.user.atlas.common), 194
get_utility_command_kill_signal()
(in
module
pilot.user.generic.common), 217
get_utility_command_output_filename() (in mod-
ule pilot.user.atlas.common), 194
get_utility_command_output_filename() (in mod-
ule pilot.user.generic.common), 217
get_utility_command_setup()
(in
module
pi-
lot.user.atlas.common), 194
get_utility_command_setup()
(in
module
pi-
lot.user.generic.common), 218
get_utility_commands()
(in
module
pi-
lot.user.atlas.common), 194
get_utility_commands()
(in
module
pi-
lot.user.generic.common), 218
get_valid_base_urls()
(in
module
pi-
lot.user.atlas.setup), 211
get_valid_base_urls()
(in
module
pi-
lot.user.generic.setup), 221
get_valid_path_from_list()
(in
module
pi-
lot.util.filehandling), 230
get_vars() (in module pilot.util.https), 238
get_worker_attributes_file()
(in
module
pi-
lot.util.harvester), 234
get_writetoinput_filenames()
(in
module
pi-
lot.user.atlas.setup), 211
get_xcache_command()
(in
module
pi-
lot.user.atlas.common), 195
GETADMISMATCH (pilot.common.errorcodes.ErrorCodes
attribute), 131
GETGLOBUSSYSERR
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
getjob_server_command()
(in
module
pi-
lot.control.job), 153
GETMD5MISMATCH (pilot.common.errorcodes.ErrorCodes
attribute), 131
grep() (in module pilot.util.filehandling), 230
H
handle_backchannel_command()
(in
module
pi-
lot.control.job), 153
342
Index
PanDAWMS
handle_rucio_error()
(in
module
pi-
lot.copytool.rucio), 169
has_instruction_set()
(in
module
pi-
lot.util.auxiliary), 224
has_instruction_sets()
(in
module
pi-
lot.util.auxiliary), 224
has_job_completed() (in module pilot.control.job),
153
has_remoteio() (pilot.info.jobdata.JobData method),
180
homepackage (pilot.info.jobdata.JobData attribute), 180
https_setup() (in module pilot.util.https), 239
human2bytes() (in module pilot.util.math), 242
I
imagename (pilot.info.jobdata.JobData attribute), 180
imagename_jobdef
(pilot.info.jobdata.JobData
at-
tribute), 180
IMAGENOTFOUND (pilot.common.errorcodes.ErrorCodes
attribute), 131
indata (pilot.info.jobdata.JobData attribute), 180
infilesguids
(pilot.info.jobdata.JobData
attribute),
180
InfoService (class in pilot.info.infoservice), 175
init() (pilot.info.infoservice.InfoService method), 176
init() (pilot.info.jobdata.JobData method), 181
interceptor() (in module pilot.control.job), 153
INTERNALPILOTPROBLEM
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
interpret_proxy_info()
(in
module
pi-
lot.user.atlas.proxy), 209
interrupt() (in module pilot.workflow.generic), 257
intersect() (pilot.api.analytics.Analytics method), 123
intersect() (pilot.api.analytics.Fit method), 124
is_already_processed()
(in
module
pi-
lot.control.data), 145
is_analysis()
(pilot.info.jobdata.JobData
method),
181
is_build_job() (pilot.info.jobdata.JobData method),
181
is_child() (in module pilot.util.processes), 250
is_cvmfs (pilot.info.queuedata.QueueData attribute),
185
is_deterministic (pilot.info.storagedata.StorageData
attribute), 187
is_eventservice
(pilot.info.jobdata.JobData
at-
tribute), 181
is_eventservicemerge (pilot.info.jobdata.JobData at-
tribute), 181
is_file_expired() (pilot.info.dataloader.DataLoader
class method), 173
is_greater_or_equal() (in module pilot.util.math),
243
is_harvester_mode() (in module pilot.util.harvester),
234
is_hpo (pilot.info.jobdata.JobData attribute), 181
is_json() (in module pilot.util.filehandling), 231
is_local() (pilot.info.jobdata.JobData method), 181
is_process_running()
(in
module
pi-
lot.util.processes), 250
is_python3() (in module pilot.common.exception), 143
is_python3() (in module pilot.util.auxiliary), 224
is_python3() (in module pilot.util.container), 226
is_queue_empty() (in module pilot.control.job), 153
is_recoverable()
(pi-
lot.common.errorcodes.ErrorCodes
class
method), 138
is_release_setup()
(in
module
pi-
lot.user.atlas.container), 204
is_standard_atlas_job()
(in
module
pi-
lot.user.atlas.setup), 211
is_string() (in module pilot.util.auxiliary), 224
is_valid_for_copy_in()
(in
module
pi-
lot.copytool.gfal), 166
is_valid_for_copy_in()
(in
module
pi-
lot.copytool.lsm), 167
is_valid_for_copy_in()
(in
module
pi-
lot.copytool.mv), 168
is_valid_for_copy_in()
(in
module
pi-
lot.copytool.rucio), 169
is_valid_for_copy_in()
(in
module
pi-
lot.copytool.xrdcp), 170
is_valid_for_copy_out()
(in
module
pi-
lot.copytool.gfal), 166
is_valid_for_copy_out()
(in
module
pi-
lot.copytool.lsm), 167
is_valid_for_copy_out()
(in
module
pi-
lot.copytool.mv), 168
is_valid_for_copy_out()
(in
module
pi-
lot.copytool.rucio), 169
is_valid_for_copy_out()
(in
module
pi-
lot.copytool.xrdcp), 170
is_virtual_machine() (in module pilot.util.auxiliary),
224
is_zombie() (in module pilot.util.processes), 250
J
job (pilot.info.jobinfo.JobInfoProvider attribute), 184
job_monitor() (in module pilot.control.job), 153
job_monitor_tasks()
(in
module
pi-
lot.util.monitoring), 245
JobAlreadyRunning, 140
JOBALREADYRUNNING
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
JobData (class in pilot.info.jobdata), 177
Index
343
PanDAWMS
jobdefinitionid
(pilot.info.jobdata.JobData
at-
tribute), 181
jobid (pilot.info.jobdata.JobData attribute), 181
JobInfoProvider (class in pilot.info.jobinfo), 183
JobInfoService (class in pilot.info.jobinfoservice), 184
jobparams (pilot.info.jobdata.JobData attribute), 181
jobsetid (pilot.info.jobdata.JobData attribute), 181
JSONRETRIEVALTIMEOUT
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
K
kill() (in module pilot.util.processes), 250
kill_child_processes()
(in
module
pi-
lot.util.processes), 250
kill_looping_job() (in module pilot.util.loopingjob),
240
kill_orphans() (in module pilot.util.processes), 250
kill_process() (in module pilot.util.processes), 250
kill_process_group()
(in
module
pi-
lot.util.processes), 250
kill_processes() (in module pilot.util.processes), 250
kill_worker() (in module pilot.util.harvester), 234
KILLPAYLOAD (pilot.common.errorcodes.ErrorCodes at-
tribute), 131
killpg() (in module pilot.util.processes), 251
KILLSIGNAL (pilot.common.errorcodes.ErrorCodes at-
tribute), 131
L
LFNTOOLONG (pilot.common.errorcodes.ErrorCodes at-
tribute), 131
list_hardware() (in module pilot.util.auxiliary), 224
list_work_dir() (in module pilot.user.atlas.common),
195
load() (pilot.info.jobdata.JobData method), 181
load() (pilot.info.queuedata.QueueData method), 185
load() (pilot.info.storagedata.StorageData method), 187
load_data() (pilot.info.dataloader.DataLoader class
method), 173
load_queuedata()
(pilot.info.extinfo.ExtInfoProvider
class method), 174
load_schedconfig_data()
(pi-
lot.info.extinfo.ExtInfoProvider class method),
175
load_storage_data()
(pi-
lot.info.extinfo.ExtInfoProvider class method),
175
load_url_data()
(pilot.info.dataloader.DataLoader
class method), 174
locate_core_file() (in module pilot.util.auxiliary),
224
locate_file() (in module pilot.util.filehandling), 231
locate_job_definition()
(in
module
pi-
lot.control.job), 154
logdata (pilot.info.jobdata.JobData attribute), 181
LogFileCreationFailure, 141
LOGFILECREATIONFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
looping_check (pilot.info.jobdata.JobData attribute),
181
looping_job() (in module pilot.util.loopingjob), 240
LOOPINGJOB (pilot.common.errorcodes.ErrorCodes at-
tribute), 131
M
make_job_report() (in module pilot.control.job), 154
maxcpucount (pilot.info.jobdata.JobData attribute), 181
maxinputsize
(pilot.info.queuedata.QueueData
at-
tribute), 185
maxrss (pilot.info.queuedata.QueueData attribute), 186
maxtime (pilot.info.queuedata.QueueData attribute), 186
maxwdir (pilot.info.queuedata.QueueData attribute), 186
mean() (in module pilot.util.math), 243
memory_usage() (in module pilot.user.atlas.memory),
207
memory_usage() (in module pilot.user.generic.memory),
221
memorymonitor (pilot.info.jobdata.JobData attribute),
181
MemoryMonitoring (class in pilot.api.memorymonitor),
129
merge_destinations()
(in
module
pi-
lot.copytool.common), 165
merge_dict_data() (in module pilot.info.dataloader),
174
MessageFailure, 141
MESSAGEHANDLINGFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
metadata (pilot.info.jobdata.JobData attribute), 181
MIDDLEWAREIMPORTFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
MISSINGCREDENTIALS
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
MISSINGINPUTFILE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
MISSINGINSTALLATION
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
MISSINGOUTPUTFILE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
344
Index
PanDAWMS
MISSINGRELEASEUNPACKED
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
MISSINGUSERCODE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
MKDIR (pilot.common.errorcodes.ErrorCodes attribute),
131
MKDirFailure, 141
mkdirs() (in module pilot.util.filehandling), 231
mode (pilot.api.data.StageInClient attribute), 125
mode (pilot.api.data.StageOutClient attribute), 126
mode (pilot.api.data.StagingClient attribute), 128
module
pilot.api.analytics, 122
pilot.api.benchmark, 125
pilot.api.data, 125
pilot.api.memorymonitor, 129
pilot.api.services, 130
pilot.common.errorcodes, 130
pilot.common.exception, 139
pilot.control.data, 143
pilot.control.job, 146
pilot.control.monitor, 157
pilot.control.payload, 158
pilot.control.payloads.eventservice, 160
pilot.control.payloads.generic, 161
pilot.copytool.common, 164
pilot.copytool.gfal, 166
pilot.copytool.lsm, 166
pilot.copytool.mv, 167
pilot.copytool.rucio, 168
pilot.copytool.xrdcp, 170
pilot.eventservice.esprocess, 170
pilot.info.basedata, 171
pilot.info.configinfo, 172
pilot.info.dataloader, 173
pilot.info.extinfo, 174
pilot.info.infoservice, 175
pilot.info.jobdata, 177
pilot.info.jobinfo, 183
pilot.info.jobinfoservice, 184
pilot.info.queuedata, 184
pilot.info.storagedata, 186
pilot.resource.alcf, 187
pilot.resource.bnl, 187
pilot.resource.generic, 188
pilot.resource.nersc, 188
pilot.resource.summit, 188
pilot.resource.titan, 188
pilot.user.atlas.common, 189
pilot.user.atlas.container, 200
pilot.user.atlas.jobmetrics, 206
pilot.user.atlas.loopingjob_definitions,
207
pilot.user.atlas.memory, 207
pilot.user.atlas.nordugrid, 207
pilot.user.atlas.proxy, 209
pilot.user.atlas.setup, 210
pilot.user.atlas.utilities, 212
pilot.user.generic.common, 217
pilot.user.generic.container, 220
pilot.user.generic.jobmetrics, 220
pilot.user.generic.loopingjob_definitions,
221
pilot.user.generic.memory, 221
pilot.user.generic.proxy, 221
pilot.user.generic.setup, 221
pilot.util.auxiliary, 222
pilot.util.config, 225
pilot.util.constants, 226
pilot.util.container, 226
pilot.util.disk, 226
pilot.util.filehandling, 227
pilot.util.harvester, 234
pilot.util.https, 236
pilot.util.jobmetrics, 239
pilot.util.loopingjob, 240
pilot.util.math, 241
pilot.util.monitoring, 244
pilot.util.monitoringtime, 247
pilot.util.parameters, 247
pilot.util.processes, 248
pilot.util.proxy, 251
pilot.util.timer, 251
pilot.util.timing, 253
pilot.util.workernode, 256
pilot.workflow.generic, 257
MonitoringTime (class in pilot.util.monitoringtime),
247
move() (in module pilot.copytool.gfal), 166
move() (in module pilot.copytool.lsm), 167
move() (in module pilot.copytool.mv), 168
move() (in module pilot.util.filehandling), 231
move_all_files() (in module pilot.copytool.mv), 168
move_all_files_in() (in module pilot.copytool.gfal),
166
move_all_files_in() (in module pilot.copytool.lsm),
167
move_all_files_out() (in module pilot.copytool.gfal),
166
move_all_files_out() (in module pilot.copytool.lsm),
167
N
name (pilot.info.queuedata.QueueData attribute), 186
name (pilot.info.storagedata.StorageData attribute), 187
Index
345
PanDAWMS
nevents (pilot.info.jobdata.JobData attribute), 181
neventsw (pilot.info.jobdata.JobData attribute), 181
NFSSQLITE
(pilot.common.errorcodes.ErrorCodes
attribute), 131
NOCTYPES
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 131
noexecstrcnv
(pilot.info.jobdata.JobData
attribute),
181
NoGridProxy, 141
NoLocalSpace, 141
NOLOCALSPACE
(pilot.common.errorcodes.ErrorCodes
attribute), 131
NONDETERMINISTICDDM
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
NOOUTPUTINJOBREPORT
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
NOPAYLOADMETADATA
(pi-
lot.common.errorcodes.ErrorCodes attribute),
131
NOPROXY
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 131
NORELEASEFOUND (pilot.common.errorcodes.ErrorCodes
attribute), 131
NOREMOTESPACE (pilot.common.errorcodes.ErrorCodes
attribute), 131
NOREPLICAS (pilot.common.errorcodes.ErrorCodes at-
tribute), 131
NoSoftwareDir, 141
NOSOFTWAREDIR (pilot.common.errorcodes.ErrorCodes
attribute), 132
NOSTORAGE
(pilot.common.errorcodes.ErrorCodes
attribute), 132
NOSTORAGEPROTOCOL
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
NoSuchFile, 141
NOSUCHFILE (pilot.common.errorcodes.ErrorCodes at-
tribute), 132
NOSUCHPROCESS (pilot.common.errorcodes.ErrorCodes
attribute), 132
NotDefined, 141
NOTDEFINED (pilot.common.errorcodes.ErrorCodes at-
tribute), 132
NOTIMPLEMENTED (pilot.common.errorcodes.ErrorCodes
attribute), 132
NotSameLength, 142
NOTSAMELENGTH (pilot.common.errorcodes.ErrorCodes
attribute), 132
NOUSERTARBALL (pilot.common.errorcodes.ErrorCodes
attribute), 132
NoVomsProxy, 141
NOVOMSPROXY (pilot.common.errorcodes.ErrorCodes at-
tribute), 132
now() (in module pilot.control.job), 154
O
only_copy_to_scratch() (pilot.info.jobdata.JobData
method), 181
open_file() (in module pilot.util.filehandling), 231
open_remote_files()
(in
module
pi-
lot.user.atlas.common), 195
order_log_transfer() (in module pilot.control.job),
154
outdata (pilot.info.jobdata.JobData attribute), 181
output_line_scan()
(in
module
pi-
lot.copytool.common), 165
OUTPUTFILETOOLARGE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
overwrite_queuedata (pilot.info.jobdata.JobData at-
tribute), 181
overwrite_storagedata
(pilot.info.jobdata.JobData
attribute), 181
P
PANDAKILL
(pilot.common.errorcodes.ErrorCodes
attribute), 132
PANDAQUEUENOTACTIVE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
parse_args()
(pilot.info.jobdata.JobData
class
method), 181
parse_job_definition_file()
(in
module
pi-
lot.util.harvester), 235
parse_jobreport_data()
(in
module
pi-
lot.user.atlas.common), 195
pause_queue_monitor() (in module pilot.control.job),
154
payload (pilot.info.jobdata.JobData attribute), 182
PAYLOADEXCEEDMAXMEM
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
PAYLOADEXECUTIONEXCEPTION
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
PAYLOADEXECUTIONFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
PAYLOADOUTOFMEMORY
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
PAYLOADSIGSEGV (pilot.common.errorcodes.ErrorCodes
attribute), 132
perform_initial_payload_error_analysis()
(in
module pilot.control.payload), 159
pgrp (pilot.info.jobdata.JobData attribute), 182
346
Index
PanDAWMS
pid
(pilot.api.memorymonitor.MemoryMonitoring
attribute), 129
pid (pilot.info.jobdata.JobData attribute), 182
pilot.api.analytics
module, 122
pilot.api.benchmark
module, 125
pilot.api.data
module, 125
pilot.api.memorymonitor
module, 129
pilot.api.services
module, 130
pilot.common.errorcodes
module, 130
pilot.common.exception
module, 139
pilot.control.data
module, 143
pilot.control.job
module, 146
pilot.control.monitor
module, 157
pilot.control.payload
module, 158
pilot.control.payloads.eventservice
module, 160
pilot.control.payloads.generic
module, 161
pilot.copytool.common
module, 164
pilot.copytool.gfal
module, 166
pilot.copytool.lsm
module, 166
pilot.copytool.mv
module, 167
pilot.copytool.rucio
module, 168
pilot.copytool.xrdcp
module, 170
pilot.eventservice.esprocess
module, 170
pilot.info.basedata
module, 171
pilot.info.configinfo
module, 172
pilot.info.dataloader
module, 173
pilot.info.extinfo
module, 174
pilot.info.infoservice
module, 175
pilot.info.jobdata
module, 177
pilot.info.jobinfo
module, 183
pilot.info.jobinfoservice
module, 184
pilot.info.queuedata
module, 184
pilot.info.storagedata
module, 186
pilot.resource.alcf
module, 187
pilot.resource.bnl
module, 187
pilot.resource.generic
module, 188
pilot.resource.nersc
module, 188
pilot.resource.summit
module, 188
pilot.resource.titan
module, 188
pilot.user.atlas.common
module, 189
pilot.user.atlas.container
module, 200
pilot.user.atlas.jobmetrics
module, 206
pilot.user.atlas.loopingjob_definitions
module, 207
pilot.user.atlas.memory
module, 207
pilot.user.atlas.nordugrid
module, 207
pilot.user.atlas.proxy
module, 209
pilot.user.atlas.setup
module, 210
pilot.user.atlas.utilities
module, 212
pilot.user.generic.common
module, 217
pilot.user.generic.container
module, 220
pilot.user.generic.jobmetrics
module, 220
pilot.user.generic.loopingjob_definitions
module, 221
pilot.user.generic.memory
module, 221
pilot.user.generic.proxy
module, 221
pilot.user.generic.setup
module, 221
pilot.util.auxiliary
Index
347
PanDAWMS
module, 222
pilot.util.config
module, 225
pilot.util.constants
module, 226
pilot.util.container
module, 226
pilot.util.disk
module, 226
pilot.util.filehandling
module, 227
pilot.util.harvester
module, 234
pilot.util.https
module, 236
pilot.util.jobmetrics
module, 239
pilot.util.loopingjob
module, 240
pilot.util.math
module, 241
pilot.util.monitoring
module, 244
pilot.util.monitoringtime
module, 247
pilot.util.parameters
module, 247
pilot.util.processes
module, 248
pilot.util.proxy
module, 251
pilot.util.timer
module, 251
pilot.util.timing
module, 253
pilot.util.workernode
module, 256
pilot.workflow.generic
module, 257
pilot_version_banner()
(in
module
pi-
lot.util.auxiliary), 225
PilotConfigProvider (class in pilot.info.configinfo),
172
piloterrorcode (pilot.info.jobdata.JobData attribute),
182
piloterrorcodes
(pilot.info.jobdata.JobData
at-
tribute), 182
piloterrordiag (pilot.info.jobdata.JobData attribute),
182
piloterrordiags
(pilot.info.jobdata.JobData
at-
tribute), 182
PilotException, 142
pk (pilot.info.storagedata.StorageData attribute), 187
platform (pilot.info.jobdata.JobData attribute), 182
platform (pilot.info.queuedata.QueueData attribute),
186
pledgedcpu (pilot.info.queuedata.QueueData attribute),
186
post_memory_monitor_action()
(in
module
pi-
lot.user.atlas.utilities), 216
post_payload() (pilot.control.payloads.generic.Executor
method), 162
post_prestagein_utility_command()
(in
module
pilot.user.atlas.common), 196
post_prestagein_utility_command()
(in
module
pilot.user.generic.common), 218
post_setup() (pilot.control.payloads.generic.Executor
method), 162
post_utility_command_action()
(in
module
pi-
lot.user.atlas.common), 196
post_utility_command_action()
(in
module
pi-
lot.user.generic.common), 218
postprocess (pilot.info.jobdata.JobData attribute), 182
postprocess_workdir()
(in
module
pi-
lot.resource.titan), 188
POSTPROCESSFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
pre_payload() (pilot.control.payloads.generic.Executor
method), 162
pre_setup()
(pilot.control.payloads.generic.Executor
method), 162
precleanup() (in module pilot.user.atlas.utilities), 216
prepare_destinations()
(pi-
lot.api.data.StageOutClient method), 126
prepare_infiles()
(pilot.info.jobdata.JobData
method), 182
prepare_inputddms()
(pilot.api.data.StagingClient
method), 128
prepare_outfiles()
(pilot.info.jobdata.JobData
method), 182
prepare_sources()
(pilot.api.data.StagingClient
method), 128
preprocess (pilot.info.jobdata.JobData attribute), 182
preprocess_debug_command()
(in
module
pi-
lot.user.atlas.common), 196
PREPROCESSFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
print_node_info() (in module pilot.control.job), 154
proceed_with_getjob() (in module pilot.control.job),
154
process_debug_command()
(in
module
pi-
lot.user.atlas.common), 196
process_debug_command()
(in
module
pi-
lot.user.generic.common), 218
process_debug_mode() (in module pilot.control.job),
155
348
Index
PanDAWMS
process_jobreport() (in module pilot.resource.titan),
188
process_remote_file_traces()
(in
module
pi-
lot.user.atlas.common), 196
process_writetofile()
(pilot.info.jobdata.JobData
method), 182
processingtype (pilot.info.jobdata.JobData attribute),
182
produserid (pilot.info.jobdata.JobData attribute), 182
publish_harvester_reports()
(in
module
pi-
lot.control.job), 155
publish_job_report()
(in
module
pi-
lot.util.harvester), 235
publish_stageout_files()
(in
module
pi-
lot.util.harvester), 235
publish_work_report()
(in
module
pi-
lot.util.harvester), 235
put_error_codes
(pi-
lot.common.errorcodes.ErrorCodes attribute),
139
PUTADMISMATCH (pilot.common.errorcodes.ErrorCodes
attribute), 132
PUTGLOBUSSYSERR
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
PUTMD5MISMATCH (pilot.common.errorcodes.ErrorCodes
attribute), 132
Q
queue_monitor() (in module pilot.control.job), 155
queue_monitoring() (in module pilot.control.data),
145
QueueData (class in pilot.info.queuedata), 184
QUEUEDATA
(pilot.common.errorcodes.ErrorCodes
attribute), 132
QueuedataFailure, 142
QueuedataNotOK, 142
QUEUEDATANOTOK (pilot.common.errorcodes.ErrorCodes
attribute), 132
R
REACHEDMAXTIME (pilot.common.errorcodes.ErrorCodes
attribute), 132
read() (in module pilot.util.config), 226
read_file() (in module pilot.util.filehandling), 231
read_json() (in module pilot.util.filehandling), 232
read_list() (in module pilot.util.filehandling), 232
read_pilot_timing() (in module pilot.util.timing),
255
recoverable_error_codes
(pi-
lot.common.errorcodes.ErrorCodes attribute),
139
register_signals()
(in
module
pi-
lot.workflow.generic), 257
REMOTEFILECOULDNOTBEOPENED
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
remoteinput_allowed_schemas
(pi-
lot.api.data.StagingClient attribute), 128
remove() (in module pilot.util.filehandling), 232
remove_archives()
(in
module
pi-
lot.user.atlas.common), 196
remove_container_string()
(in
module
pi-
lot.user.atlas.container), 204
remove_core_dumps()
(in
module
pi-
lot.util.filehandling), 232
remove_dir_tree() (in module pilot.util.filehandling),
232
remove_empty_directories()
(in
module
pi-
lot.util.filehandling), 232
remove_error_code()
(pi-
lot.common.errorcodes.ErrorCodes
method),
139
remove_files() (in module pilot.util.filehandling), 232
remove_from_stageout()
(in
module
pi-
lot.user.atlas.common), 197
remove_job_request_file()
(in
module
pi-
lot.util.harvester), 235
remove_no_output_files()
(in
module
pi-
lot.user.atlas.common), 197
remove_pilot_logs_from_list()
(in
module
pi-
lot.control.job), 155
remove_redundant_files()
(in
module
pi-
lot.user.atlas.common), 197
remove_redundant_files()
(in
module
pi-
lot.user.generic.common), 218
remove_special_files()
(in
module
pi-
lot.user.atlas.common), 197
remove_unwanted_files()
(in
module
pi-
lot.user.atlas.loopingjob_definitions), 207
remove_unwanted_files()
(in
module
pi-
lot.user.generic.loopingjob_definitions), 221
rename_log_files()
(pi-
lot.control.payloads.generic.Executor method),
162
replace_last_command()
(in
module
pi-
lot.user.atlas.container), 204
replace_lfns_with_turls()
(in
module
pi-
lot.user.atlas.setup), 211
REPLICANOTFOUND
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
ReplicasNotFound, 142
report_errors()
(pi-
lot.common.errorcodes.ErrorCodes
method),
139
request() (in module pilot.util.https), 239
request_new_jobs() (in module pilot.util.harvester),
Index
349
PanDAWMS
235
require_init()
(pilot.info.infoservice.InfoService
method), 176
require_protocols()
(pilot.api.data.StagingClient
method), 128
reset_errors() (pilot.info.jobdata.JobData method),
182
resimevents (pilot.info.jobdata.JobData attribute), 182
resolve_allowed_schemas()
(pi-
lot.info.queuedata.QueueData
method),
186
resolve_common_transfer_errors() (in module pi-
lot.copytool.common), 165
resolve_ddmendpoint_storageid()
(pi-
lot.info.infoservice.InfoService
method),
176
resolve_protocol()
(pilot.api.data.StagingClient
class method), 128
resolve_protocols()
(pilot.api.data.StagingClient
method), 128
resolve_queuedata()
(pi-
lot.info.configinfo.PilotConfigProvider
method), 172
resolve_queuedata()
(pi-
lot.info.extinfo.ExtInfoProvider
method),
175
resolve_queuedata()
(pi-
lot.info.infoservice.InfoService
method),
176
resolve_queuedata()
(pi-
lot.info.jobinfo.JobInfoProvider
method),
184
resolve_replica()
(pilot.api.data.StageInClient
method), 125
resolve_replicas()
(pilot.api.data.StagingClient
method), 128
resolve_schedconf_sources()
(pi-
lot.info.configinfo.PilotConfigProvider
method), 172
resolve_schedconf_sources()
(pi-
lot.info.infoservice.InfoService
method),
176
resolve_schedconf_sources()
(pi-
lot.info.jobinfo.JobInfoProvider
method),
184
resolve_storage_data()
(pi-
lot.info.extinfo.ExtInfoProvider
method),
175
resolve_storage_data()
(pi-
lot.info.infoservice.InfoService
method),
176
resolve_storage_data()
(pi-
lot.info.jobinfo.JobInfoProvider
method),
184
resolve_surl()
(pilot.api.data.StageOutClient
method), 126
resolve_transform_error()
(pi-
lot.common.errorcodes.ErrorCodes
method),
139
resource (pilot.info.queuedata.QueueData attribute),
186
resource (pilot.info.storagedata.StorageData attribute),
187
RESOURCEUNAVAILABLE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
retrieve() (in module pilot.control.job), 155
rmdirs() (in module pilot.util.filehandling), 232
rprotocols
(pilot.info.storagedata.StorageData
at-
tribute), 187
RUCIOLISTREPLICASFAILED
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
RUCIOLOCATIONFAILED
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
RUCIOSERVICEUNAVAILABLE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
run() (in module pilot.workflow.generic), 257
run() (pilot.common.exception.ExcThread method), 140
run() (pilot.control.payloads.generic.Executor method),
162
run() (pilot.util.timer.TimedProcess method), 252
run() (pilot.util.timer.TimedThread method), 252
run_checks() (in module pilot.control.monitor), 158
run_command() (pilot.control.payloads.generic.Executor
method), 162
run_payload() (pilot.control.payloads.eventservice.Executor
method), 160
run_payload() (pilot.control.payloads.generic.Executor
method), 162
run_preprocess()
(pi-
lot.control.payloads.generic.Executor method),
163
run_utility_after_payload_finished()
(pi-
lot.control.payloads.generic.Executor method),
163
RunPayloadFailure, 142
S
sanity_check() (in module pilot.user.atlas.common),
197
sanity_check()
(in
module
pi-
lot.user.generic.common), 219
scan_file() (in module pilot.util.filehandling), 232
send_heartbeat_if_time()
(in
module
pi-
lot.control.job), 156
350
Index
PanDAWMS
send_state() (in module pilot.control.job), 156
serverstate (pilot.info.jobdata.JobData attribute), 182
SERVICENOTAVAILABLE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
132
Services (class in pilot.api.services), 130
set_accessmode()
(pilot.info.jobdata.JobData
method), 182
set_acopytools()
(pilot.api.data.StagingClient
method), 128
set_chi2() (pilot.api.analytics.Fit method), 124
set_cpu_consumption_time()
(in
module
pi-
lot.control.payload), 159
set_error_code_from_stderr()
(in
module
pi-
lot.control.payload), 159
set_inds() (in module pilot.user.atlas.setup), 211
set_intersect() (pilot.api.analytics.Fit method), 125
set_job_workdir() (in module pilot.resource.titan),
189
set_number_used_cores()
(in
module
pi-
lot.util.monitoring), 245
set_pilot_state() (in module pilot.util.auxiliary),
225
set_platform() (in module pilot.user.atlas.container),
205
set_scratch_workdir()
(in
module
pi-
lot.resource.titan), 189
set_slope() (pilot.api.analytics.Fit method), 125
set_status_for_direct_access()
(pi-
lot.api.data.StageInClient method), 126
set_xcache_var()
(in
module
pi-
lot.user.atlas.common), 197
setup (pilot.info.jobdata.JobData attribute), 182
SetupFailure, 142
SETUPFAILURE
(pilot.common.errorcodes.ErrorCodes
attribute), 132
SETUPFATAL (pilot.common.errorcodes.ErrorCodes at-
tribute), 132
shell_exit_code() (in module pilot.util.auxiliary),
225
should_abort_payload()
(in
module
pi-
lot.util.monitoring), 245
should_pilot_prepare_setup()
(in
module
pi-
lot.user.atlas.setup), 212
should_update_logstash()
(in
module
pi-
lot.user.atlas.common), 198
show_access_settings() (pilot.info.jobdata.JobData
static method), 182
show_memory_usage() (in module pilot.util.auxiliary),
225
SIGBUS (pilot.common.errorcodes.ErrorCodes attribute),
132
SIGQUIT
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 133
SIGSEGV
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 133
SIGTERM
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 133
SIGUSR1
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 133
SIGXCPU
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 133
singularity_wrapper()
(in
module
pi-
lot.user.atlas.container), 205
SINGULARITYBINDPOINTFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
SINGULARITYFAILEDUSERNAMESPACE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
SINGULARITYGENERALFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
SINGULARITYIMAGEMOUNTFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
SINGULARITYNEWUSERNAMESPACE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
SINGULARITYNOLOOPDEVICES
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
SINGULARITYNOTINSTALLED
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
SINGULARITYRESOURCEUNAVAILABLE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
site (pilot.info.queuedata.QueueData attribute), 186
site (pilot.info.storagedata.StorageData attribute), 187
sizes (pilot.info.jobdata.JobData attribute), 183
SizeTooLarge, 142
SIZETOOLARGE
(pilot.common.errorcodes.ErrorCodes
attribute), 133
skip_special_files() (in module pilot.control.data),
145
slope() (pilot.api.analytics.Analytics method), 123
slope() (pilot.api.analytics.Fit method), 125
sort_replicas()
(pilot.api.data.StagingClient
class
method), 129
special_setup (pilot.info.storagedata.StorageData at-
tribute), 187
split_version() (in module pilot.util.math), 243
ssl_context (pilot.util.https._ctx property), 236
ssl_context (pilot.util.https.ctx attribute), 237
stage_in_auto() (in module pilot.control.data), 145
stage_out_auto() (in module pilot.control.data), 146
STAGEINAUTHENTICATIONFAILURE
(pi-
Index
351
PanDAWMS
lot.common.errorcodes.ErrorCodes attribute),
133
StageInClient (class in pilot.api.data), 125
STAGEINFAILED (pilot.common.errorcodes.ErrorCodes
attribute), 133
StageInFailure, 143
STAGEINTIMEOUT (pilot.common.errorcodes.ErrorCodes
attribute), 133
stageout (pilot.info.jobdata.JobData attribute), 183
StageOutClient (class in pilot.api.data), 126
STAGEOUTFAILED (pilot.common.errorcodes.ErrorCodes
attribute), 133
StageOutFailure, 143
STAGEOUTTIMEOUT
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
StagingClient (class in pilot.api.data), 127
state (pilot.info.jobdata.JobData attribute), 183
state (pilot.info.queuedata.QueueData attribute), 186
state (pilot.info.storagedata.StorageData attribute), 187
STATFILEPROBLEM
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
status (pilot.info.jobdata.JobData attribute), 183
status (pilot.info.queuedata.QueueData attribute), 186
STDOUTTOOBIG
(pilot.common.errorcodes.ErrorCodes
attribute), 133
stop_utilities()
(pi-
lot.control.payloads.generic.Executor method),
163
StorageData (class in pilot.info.storagedata), 186
store_jobid() (in module pilot.control.job), 156
sum_dev() (in module pilot.util.math), 243
sum_square_dev() (in module pilot.util.math), 243
swrelease (pilot.info.jobdata.JobData attribute), 183
symlink() (in module pilot.copytool.mv), 168
T
t0 (pilot.info.jobdata.JobData attribute), 183
tail() (in module pilot.util.filehandling), 233
tar_files() (in module pilot.util.filehandling), 233
taskid (pilot.info.jobdata.JobData attribute), 183
threads_aborted() (in module pilot.util.processes),
251
time_stamp() (in module pilot.util.timing), 255
TimedProcess (class in pilot.util.timer), 251
TimedThread (class in pilot.util.timer), 252
timefloor (pilot.info.queuedata.QueueData attribute),
186
timeout() (in module pilot.util.timer), 252
TimeoutException, 252
Timer (in module pilot.util.timer), 252
timing_report() (in module pilot.util.timing), 255
to_json() (pilot.info.jobdata.JobData method), 183
token (pilot.info.storagedata.StorageData attribute), 187
touch() (in module pilot.util.filehandling), 233
transexitcode (pilot.info.jobdata.JobData attribute),
183
transfer() (pilot.api.data.StagingClient method), 129
transfer_files()
(pilot.api.data.StageInClient
method), 126
transfer_files()
(pilot.api.data.StageOutClient
method), 126
transfer_files()
(pilot.api.data.StagingClient
method), 129
transfertype
(pilot.info.jobdata.JobData
attribute),
183
transformation (pilot.info.jobdata.JobData attribute),
183
TRANSFORMNOTFOUND
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
TrfDownloadFailure, 143
TRFDOWNLOADFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
tryint() (in module pilot.util.math), 244
type (pilot.info.storagedata.StorageData attribute), 187
U
UNKNOWNCHECKSUMTYPE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UNKNOWNCOPYTOOL
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UnknownException, 143
UNKNOWNEXCEPTION
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UNKNOWNPAYLOADFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UNKNOWNTRFFAILURE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UNREACHABLENETWORK
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UNRECOGNIZEDTRFARGUMENTS
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UNRECOGNIZEDTRFSTDERR
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
UNSUPPORTEDSL5OS
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
352
Index
PanDAWMS
update()
(pilot.util.monitoringtime.MonitoringTime
method), 247
update_alrb_setup()
(in
module
pi-
lot.user.atlas.container), 205
update_extension()
(in
module
pi-
lot.util.filehandling), 233
update_for_user_proxy()
(in
module
pi-
lot.user.atlas.container), 205
update_forced_accessmode()
(in
module
pi-
lot.user.atlas.common), 198
update_indata() (in module pilot.control.data), 146
update_job_data()
(in
module
pi-
lot.user.atlas.common), 198
update_job_data()
(in
module
pi-
lot.user.generic.common), 219
update_output_for_hpo()
(in
module
pi-
lot.user.atlas.common), 198
update_server() (in module pilot.control.job), 156
update_server() (in module pilot.user.atlas.common),
198
update_server()
(in
module
pi-
lot.user.generic.common), 219
update_stagein()
(in
module
pi-
lot.user.atlas.common), 198
update_stagein()
(in
module
pi-
lot.user.generic.common), 219
use_pcache (pilot.info.queuedata.QueueData attribute),
186
use_vp (pilot.info.jobdata.JobData attribute), 183
usecontainer
(pilot.info.jobdata.JobData
attribute),
183
user (pilot.api.memorymonitor.MemoryMonitoring at-
tribute), 129
user_agent (pilot.util.https._ctx property), 236
user_agent (pilot.util.https.ctx attribute), 237
USERDIRTOOLARGE
(pi-
lot.common.errorcodes.ErrorCodes attribute),
133
USERKILL
(pilot.common.errorcodes.ErrorCodes
at-
tribute), 133
utilities (pilot.info.jobdata.JobData attribute), 183
utility_after_payload_finished()
(pi-
lot.control.payloads.generic.Executor method),
163
utility_after_payload_started()
(pi-
lot.control.payloads.generic.Executor method),
163
utility_after_payload_started_new()
(pi-
lot.control.payloads.generic.Executor method),
163
utility_before_payload()
(pi-
lot.control.payloads.generic.Executor method),
163
utility_monitor() (in module pilot.util.monitoring),
245
utility_with_payload()
(pi-
lot.control.payloads.generic.Executor method),
164
V
validate() (in module pilot.control.job), 156
validate() (in module pilot.user.atlas.common), 198
validate() (in module pilot.user.generic.common), 219
validate_post() (in module pilot.control.payload),
160
validate_pre() (in module pilot.control.payload), 160
value() (pilot.api.analytics.Fit method), 125
verify_arcproxy() (in module pilot.user.atlas.proxy),
209
verify_catalog_checksum()
(in
module
pi-
lot.copytool.common), 165
verify_ctypes() (in module pilot.control.job), 157
verify_disk_usage()
(in
module
pi-
lot.util.monitoring), 245
verify_error_code() (in module pilot.control.job),
157
verify_extracted_output_files() (in module pi-
lot.user.atlas.common), 198
verify_file_list()
(in
module
pi-
lot.util.filehandling), 233
verify_gridproxy() (in module pilot.user.atlas.proxy),
209
verify_job() (in module pilot.user.atlas.common), 199
verify_job() (in module pilot.user.generic.common),
219
verify_lfn_length()
(in
module
pi-
lot.user.atlas.common), 199
verify_looping_job()
(in
module
pi-
lot.util.monitoring), 246
verify_memory_usage()
(in
module
pi-
lot.util.monitoring), 246
verify_ncores() (in module pilot.user.atlas.common),
199
verify_output_files()
(in
module
pi-
lot.user.atlas.common), 199
verify_proxy() (in module pilot.user.atlas.proxy), 209
verify_proxy() (in module pilot.user.generic.proxy),
221
verify_release_string()
(in
module
pi-
lot.user.atlas.common), 199
verify_running_processes()
(in
module
pi-
lot.util.monitoring), 246
verify_stage_out() (in module pilot.copytool.rucio),
169
verify_user_proxy()
(in
module
pi-
lot.util.monitoring), 246
verify_vomsproxy() (in module pilot.user.atlas.proxy),
209
Index
353
PanDAWMS
W
wait_for_aborted_job_stageout() (in module pi-
lot.control.job), 157
wait_graceful()
(pi-
lot.control.payloads.eventservice.Executor
method), 160
wait_graceful()
(pi-
lot.control.payloads.generic.Executor method),
164
whoami() (in module pilot.util.auxiliary), 225
whoami()
(pilot.info.infoservice.InfoService
class
method), 177
workdir
(pilot.api.memorymonitor.MemoryMonitoring
attribute), 129
workdir (pilot.info.jobdata.JobData attribute), 183
workdirsizes
(pilot.info.jobdata.JobData
attribute),
183
wrapper() (in module pilot.user.atlas.container), 205
wrapper() (in module pilot.user.generic.container), 220
write_file() (in module pilot.util.filehandling), 233
write_heartbeat_to_file()
(in
module
pi-
lot.control.job), 157
write_json() (in module pilot.util.filehandling), 233
write_output() (in module pilot.control.data), 146
write_pilot_timing() (in module pilot.util.timing),
256
write_utility_output()
(in
module
pi-
lot.control.data), 146
write_utility_output()
(pi-
lot.control.payloads.generic.Executor method),
164
writetofile (pilot.info.jobdata.JobData attribute), 183
X
X509_CERT_DIR, 237
X509_USER_PROXY, 237
xcache_activation_command()
(in
module
pi-
lot.user.atlas.common), 199
xcache_deactivation_command()
(in
module
pi-
lot.user.atlas.common), 200
xcache_proxy() (in module pilot.user.atlas.common),
200
XMLDictionary (class in pilot.user.atlas.nordugrid), 207
XRDCPERROR (pilot.common.errorcodes.ErrorCodes at-
tribute), 133
Z
ZEROFILESIZE
(pilot.common.errorcodes.ErrorCodes
attribute), 133
zipmap (pilot.info.jobdata.JobData attribute), 183
zombies (pilot.info.jobdata.JobData attribute), 183
354
Index
