---
id: project_structure
title: Project Structure
---

The Rucio code structure is broken into multiple modules, to allow it to be grouped into packages without requiring code duplication.
Before making a change, it is important to consider how your change will be used across different releases.
This will greatly impact the structure of your change.

![A simplified diagram of Rucio's code structure. It is broken into 3 boxes labeled "Rucio", "Rucio Web" and "Rucio Client". The "Rucio" box contains the web and client boxes.](/img/code_structure.png)

## Core
Core contains the main functionality of the project, it can be conceptualized as rucio's central logic.
Any changes that need to impact all of rucio should be made in core.
For example: if a new feature introduces a new type of DID, the logic for this is included in the core, and then the ways for users and operators to use this logic is included in different parts of the code (namely, in client, gateway, and web).

## Common
Common contains code used across client and server modules.
Code in common is distributed with server and client releases, so if the functionality is required in both releases, it is included in common.
This includes things like configuration parsing, logging, and generalized utility/helper functions.

## Client
Client is a standalone module which sends requests to a server.
It can make calls to the Common module, but does not require direct knowledge of core's operation.
It contacts the Rest API hosted in web.

bin/rucio and bin/rucio-admin utilize functionality hosted in client.

If your change impacts how the user interacts with rucio, (the name of calls, arguments), this change should be made in client.

## Daemons
Daemons are auxiliary server side code to run asynchronous work without direct user interaction.
They do not need to be run, but are included with server releases.
Changes impacting daemons are generally self-contained in the daemons.

## Gateway
Gateway is a series of recipes for basic operations using core functionality.
It allows for functionality of the core to be used with correct database sessions, authentication parameters.
It is an intermediate layer between the core and the Rest API - the Rest API utilizes the gateway to make calls to core.

All public core methods should have a corresponding gateway method to handle sessions and authentication.

## Web
Web contains the flask-based Rest API for the client to contact the server.
(Found at `/lib/rucio/web/rest/flaskapi/v1`).
It also contains the routing for a Web UI front end.
When adding new functionality, or modifying the arguments of an existing function, it is imperative to update the Rest API.

## Other

### DB
As the name implies, DB contains database schema and connection utilities.
It is included in server releases of rucio.
Changes in the database dramatically impact all other parts of the code, such as core's database queries.
A migration instruction script must be included with DB changes to ensure data is not lost during upgrade or downgrades.

### RSE
RSE provides different protocols to access RSEs (ex: ssh, srm.)
It is included client distributions, and all RSE protocols must be able to run without direct access to core.

### Tests
Tests contains utilities for testing.

### Transfertool
Similar to RSE, transfertool contains protocols for executing file transfers.
It can include calls to core, as transfer protocols sit server side.
---
title: REST API Documentation
---
Rucio provides a Rest API interface for client-server communication. The code
for the server is located in the Rucio project under `lib/rucio/rest/flask/v1`.

The Rest API Documentation provides a sufficient documentation of all endpoints
and should be descriptive enough so that it is not required to look at the code
anymore.

## Tools

The main framework for the Rest API Documentation is **OpenAPI**. APISpec
extracts the OpenAPI specification conform method documentation strings from the
Flask API and constructs the overall OpenAPI specification file. This
specification file contains all endpoints and their parameter and return value
documentation.

### APISpec

`APISpec` is a python framework to extract Python doc comments and generate a
valid OpenAPI spec file from it. The documentation for each endpoint is a
yaml-conform python method doc string starting after `---`. The
`apispec_webframeworks.flask` library connects the Python doc comments with the
endpoints given by Flask.

`tools/generate_rest_api_doc.py` generates the OpenAPI specification file with
APISpec.

:::note

The latest OpenAPI specification for the Rest API Documentation is available
[here](pathname:///yaml/rest_api_doc_spec.yaml).

:::

### ReDoc

ReDoc creates a static html front-end for a given specification file. ReDoc has
several advantages over other front-end tools:

- It is fast.
- The generated output is a static file, thus no "hosting" is needed.
- The generated file is structured, easy to read, contains a search field, and
  the possibility to display examples.

:::note

Front-end generators only need the spec file and some configuration to generate
a user friendly view of the documentation. Select a generator from the [OpenAPI
Tools](https://openapi.tools/#documentation) and generate your own front-end.

:::

To generate the ReDoc html file run

```bash
npm install -g redoc-cli
redoc-cli build rest_api_doc_spec_file.yaml --output rest_api_doc.html
```

### @redocly/openapi-cli

The `openapi-cli` verifies the integrity of the generated spec file. Common
problems, e.g. typos in keywords and duplicated path specifications, get
filtered and produces an error, if found.

`openapi-cli` is integrated in the Github Actions, they run in the `syntax` test
suite. To manually check the generated spec file, download the [`rucio documentation` repository](https://github.com/rucio/documentation) and run

```bash
tools/check_rest_api_documentation.sh FILE
```

## Tips

### Build/verify often

Syntax errors are not easy to spot and appear often. To efficiently add new
documentation or edit multiple old ones, gradually apply your changes and
build/verify the spec file (both steps take not more than a few seconds). This
way any error gets caught in the development cycle, not at the end.

### Commit the doc and code changes together

The doc changes are tightly coupled with the code. Making a lot of changes to
the code and then one commit with all the documentation changes leads to a
divergent history (What if the code commits get reverted?).

### Skim the [OpenAPI](https://swagger.io/specification/) definition

OpenAPI is feature rich and may have some easier/standardized way to express
what you think. E.g. deprecated fields can be marked with `deprecated: true`.
Knowing the framework and library you're working with is always a good idea. ;-)

### [OpenAPI Tools](https://openapi.tools/)

The OpenAPI Tools are a collection of tools to support writing, verifying and
displaying Rest API Documentations. They also provide some ideas on how to
further integrate the documentation into other parts of your code base, e.g. for
input validation.
---
id: setting_up_mac_apple_silicon
title: Setting up a Rucio Developer environment on Mac with Apple Silicon
---

## Setting up a Rucio Developer environment on Mac with Apple Silicon

Currently Rucio packages and containers are only available for the `x86_64` architecture.

A Mac equipped with Apple Silicon can execute code compiled for the `x86_64` instruction set via a translation mechanism known as [Rosetta 2](https://support.apple.com/en-gb/guide/security/secebb113be1/web).

- To install Rosetta 2 run:

      /usr/sbin/softwareupdate --install-rosetta

This will initiate the Rosetta installer, and you will need to consent to a license agreement.

Make sure that [Docker Desktop](https://docs.docker.com/desktop/install/mac-install/) is installed and updated.

## Docker environment

In order to force Docker to run commands with platform `linux/amd64` instead of `linux/arm64` by default on macOS Apple Silicon, you have two options:

Set Docker default platform to `linux/amd64`:

```bash
    export DOCKER_DEFAULT_PLATFORM=linux/amd64
```

Or run it as part of the command a single time:
```bash
    DOCKER_DEFAULT_PLATFORM=linux/amd64 docker-compose -f <docker-compose-file.yml>
```
---
id: dev_style_guide
title: Style Guide
---

**TL;DR**  - Install the provided pre-commits, follow their recommendations

# General Style
Rucio follows [flake8](https://flake8.pycqa.org/en/latest/user/index.html) style, ([with exclusions listed here](https://github.com/rucio/rucio/blob/master/.flake8)).
To use them to lint your code, run:
```
python{version} -m pip install flake8
flake8 --extend-ignore {codes to ignore} /your/code/path
```

## Imports
* Never import using `from x import *`
* Order alphabetically, then separated into sections for internal and external dependencies. Group internal imports at the end of the block, and group imports from the same external package.
* Order modules such that `import {packageA}` is before `from {packageB} import {Module}`
* Do not import whole packages when single modules would suffice.
* Unused imports must be removed.
* When a large number of individual imports form a single package/module is required, group them together with `()` and separate them on their own lines.
* When importing a module specifically for type checking (e.g. a core module that may not be included in every distribution of rucio, a type from SQLAlchemy), contain them in a block using
```python
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from {package} import {module}
```

[`ruff's isort` implementation](https://docs.astral.sh/ruff/faq/#how-does-ruffs-import-sorting-compare-to-isort) handles import sorting in the rucio `pre-commit`s.

#### Examples:
```python
# Wrong
import rucio
from datetime import *
import os

# Right
import os
from datetime import datetime, timedelta

from rucio.core.did import add_did
```

```python
# Wrong
from packageA import moduleA, moduleB, moduleC, moduleD, moduleE, moduleF, moduleG, ...

# Right
from packageA import (
    moduleA,
    moduleB,
    moduleC,
    moduleD,
    moduleE,
    moduleF,
    moduleG,
    ...
)
```

# SQLAlchemy Query Guide

The Rucio project has adopted a particular coding style for its interaction with the database.
It can be split into two parts: constructing the SQL statement, and executing it and handling its results.

## Query construction

### Rationale

Statements involving the use of SQLAlchemy are not exactly Python code; they are SQL masquerading as Python code.
Hence, there are benefits to adopting a style that can be considered somewhat un-Pythonic:

 1. It is visually distinct from regular Python code.
    Thus, it stands out and assists the developer in entering an ‘SQL context’.
 2. It is closer to how one would format actual long SQL statements.

Of course, there are some downsides to this approach.
The use of a code formatter is rendered almost impossible, thus requiring manual effort during development.
However, code is written once but read many times.
As such, we believe that the benefits outweigh the downsides.

### Variable Assignment

SQL statements should be assigned to a variable, then executed separately.
The name `stmt` is a common choice.

```python
# Wrong
rses = session.execute(select(models.RSE)).scalars().all()

# Right
stmt = select(
    models.RSE
)
rses = session.execute(stmt).scalars().all()
```

### SQLAlchemy Syntax

All new code should use the more recent SQLAlchemy 2.0 syntax.
Existing code using the older 1.4 syntax should be migrated to the 2.0 syntax.

```python
# Wrong
rses = (session.query(models.RSE)
               .all())

# Right
stmt = select(
    models.RSE
)
rses = session.execute(stmt).scalars().all()
```

### Whitespace

The functions that return basic SQL constructs (e.g. `select()`, `update()`, and `delete()`) should have a newline after the opening parenthesis and before the closing parenthesis.
Same applies to all methods of those constructs (e.g. `distinct()`, `join()`, and `where()`).
The latter should be ordered in a way that matches the syntax of SQL, when permittable.
Inside the parentheses, each argument should be indented and put on a separate line.

```python
# Wrong
stmt = (
    select(models.RSEAttrAssociation.value)
    .where(models.RSEAttrAssociation.key == 'fts')
    .distinct()
)

# Right
stmt = select(
    models.RSEAttrAssociation.value
).distinct(
).where(
    models.RSEAttrAssociation.key == 'fts'
)
```

### Discouraged Logical Operators

The functions `and_()` and `or_()` should be used instead of Python’s bitwise operators `&` and `|`.

```python
# Wrong
stmt = select(
    models.Request,
    models.DataIdentifier
).join(
    models.DataIdentifier,
    (models.Request.scope == models.DataIdentifier.scope) & (models.Request.name == models.DataIdentifier.name)
)

# Right
stmt = select(
    models.Request,
    models.DataIdentifier
).join(
    models.DataIdentifier,
    and_(models.DataIdentifier.scope == models.DataIdentifier.scope,
         models.DataIdentifier.name == models.DataIdentifier.name)
)
```

### Python Keywords

The functions `true()`, `false()`, and `null()` should be used instead of Python’s own keywords.

```python
# Wrong
stmt = select(
    models.RSE
).where(
    models.RSE.deleted == False
)

# Right
stmt = select(
    models.RSE
).where(
    models.RSE.deleted == false()
)
```

### UPDATE statements

The `values()` method should be used with a dictionary as its sole argument.
The keys should be entities from the models.
The opening and closing braces of the dictionary should be paired with the parentheses of `values()`.

```python
# Wrong
stmt = update(
    models.Account
).where(
    models.Account == InternalAccount('user')
).values(
    status=AccountStatus.DELETED,
    deleted_at=datetime.now()
)

# Wrong
stmt = update(
    models.Account
).where(
    models.Account == InternalAccount('user')
).values(
    {
        'status': AccountStatus.DELETED,
        'deleted_at': datetime.now()
    }
)

# Right
stmt = update(
    models.Account
).where(
    models.Account == InternalAccount('user')
).values({
    models.Account.status: AccountStatus.DELETED,
    models.Account.deleted_at: datetime.now()
})
```

# Pre-commits
Rucio uses the [`flake8`](https://github.com/PyCQA/flake8) precommit as a linter, [`ruff`](https://github.com/astral-sh/ruff-pre-commit) as a formatter,
a custom whitespace remover, and a script to verify a uniform file-header format.
Please use these before submitting a pull request.

The Rucio repo provides a `pre-commit` that does this automatically.
Install it with the below commands.

```shell
pip install pre-commit
pre-commit install
```

# GitHub Actions
Code style is checked during a pull request with a GitHub action.
The action checks the header and type annotations (including a count and veracity).
More information about type annotations can be found [here](./type_annotation_guide.md).
These checks can also be run locally using

```shell
tools/count_missing_type_annotations_utils.sh
tools/run_pyright.sh generate {report_output_path.json}
```
The first action will raise an error if your commits introduce more un-annotated types than it solves,
and the second ensures the added types are consistent with the rest of the codebase.


# Docstring Style
Python Client documentation is autogenerated from docstrings, and needs to follow a defined style.
Client Docstrings use the `numpy` style, a [description of the style can be found here](https://numpydoc.readthedocs.io/en/latest/format.html)
and [an example can be found here](https://www.sphinx-doc.org/en/master/usage/extensions/example_numpy.html).

Notably, docstrings should **not** explicitly include types, as they are automatically added from the type annotation.

A basic example can be seen below:

```python

 def function(param1: int, param2: Optional[str] = None) -> None:
    """
    The function description

    Parameters
    ----------
    param1
        The first parameter
    param2
        The second parameter

    Returns
    -------
    None
        Nothing is returned

    Raises
    -------
    TypeError
        If something goes wrong


    Examples
    --------
    >>> function(1)
    >>> function(1, "one")
    """

```---
title: Dependency management
---

## Requirements file structure

The dependencies needed by Rucio are described in the [`rucio/requirements/`](https://github.com/rucio/rucio/tree/master/requirements) directory.

The requirements in this directory are divided based on the application:

- `requirements.server*`: Dependencies needed to run the Rucio server and daemons
- `requirements.client*`: Dependencies needed to run the Rucio client
- `requirements.dev*`: Dependencies needed for testing and development

### `.in` and `.txt` files
- `.in` files represent input files to `pip-compile`. These list the **primary** dependencies.
    - For `client`, the `.in` file is not necessary, as we do not pin `client` dependencies.
- `.txt` files represent the actual requirements files used at installation time.
    - For `server` and `dev`, the `.txt` file is generated by `pip-compile`,
    pinning both **primary** and **secondary** dependencies.
    - For `client`, the `.txt` file is compiled manually,
    and only describes the **primary** dependencies.

## Updating dependencies

### `pip-compile` and compiling dependencies
We use `pip-compile` from [`pip-tools`](https://github.com/jazzband/pip-tools) for `server` and `dev`
in order to ensure we pin secondary dependencies to specific versions,
to improve the stability of Rucio server, testing and development.
See [this issue](https://github.com/rucio/rucio/issues/6694) for an example
of a CI issue that was caused by an unpinned secondary dependency.

### I want to add/remove/upgrade a `server` dependency. What should I do?
1. Make your changes in the `requirements.server.in` file
2. Run `pip-compile requirements.server.in`
3. Run `pip-compile requirements.dev.in`
  - (`dev` dependencies include dependencies from `requirements.server.txt`,
  so make sure to do these steps in order)

### I want to add/remove/upgrade a `dev` dependency. What should I do?
1. Make your changes in the `requirements.dev.in` file
2. Run `pip-compile requirements.dev.in`

### I want to add/remove/upgrade a `client` dependency. What should I do?
1. Make your changes in the `requirements.client.txt` file

## Major dependency upgrades
Dependencies are generally upgraded to the latest possible version on every Rucio major release.
This work is performed approximately a month prior to the major release,
in order to address possible breaking changes and monitor test outcomes and runtime behaviour for errors.

For secondary dependencies, `pip-compile --upgrade` is used. This flag attempts to upgrade
all secondary dependencies to their latest versions.

To perform this major dependency upgrade:
1. Manually update all primary dependencies (where possible; be mindful of breaking changes) in:
    1. `requirements.server.in`
    2. `requirements.dev.in`
    3. `requirements.client.txt`
2. Run `pip-compile --upgrade requirements.server.in`
3. Run `pip-compile --upgrade requirements.dev.in`

## Security updates
For critical security updates, we rely on [Dependabot](https://github.com/dependabot)
to create alerts for dependencies listed in our requirements.

Dependabot supports `pip-compile`, and is able to automatically create PRs
to upgrade both primary and secondary dependencies.
When a primary dependency is upgraded in a `.in` file, Dependabot re-compiles
that file into the `.txt` file as well.

# FAQ

## Why are `client` dependencies not pinned?
In certain use cases, the Rucio client is used as a library in other applications
(See [this issue](https://github.com/rucio/rucio/issues/6663) for an example).
Because of this, `client` dependencies are left unpinned unless necessary.
---
id: setting_up_vscode_dev_env
title: Setting up a Rucio Developer environment using Visual Studio Code
---

## Local Setup vs Remote Setup

It is possible to run the containers for rucio development locally or on a remote VM.

For local development machines with limited resources `(<16 GB RAM, < 4 Cores)`, it is recommended to run the containers on a remote VM. This will allow you to use your local machine for other tasks while the containers are running on the remote VM.

Local development has been tested only on Linux hosts.
Remote development has been tested on Linux(for the remote VM) and Linux/MacOS(for the local instance of vscode).
The additional steps required for setting up a remote development environment are described in the sections beginning with `Remote Only: `. These sections can be ignored if you are setting up a local development environment.

## Prerequisites

### VSCode

Install [Visual Studio Code](https://code.visualstudio.com/Download)

### Remote Only: Additional Setup VSCode

For development on a remote VM, you will need to install the [Remote Development Extension Pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)

You should setup SSH access to your remote VM. For more information, see [Connecting to a remote host](https://code.visualstudio.com/docs/remote/ssh)

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Remote-SSH: Open SSH Configuration File`
  ![Open SSH Configuration File](/img/vscode/remote_ssh.jpg)

  You will be asked to create a new SSH configuration file or select an existing one. If you have an existing SSH configuration file, select it. If you do not have an existing SSH configuration file, select `Create New File`.

- Update the SSH configuration file:

  ```
  Host <fqdn of your remote VM>
      HostName <hostname or ip address of your remote VM>
      User <remote user>
  ```

  Additionally, you can use the `IdentityFile` option to specify the SSH key to use for authentication. For example:

  ```
  Host <fqdn of your remote VM>
      HostName <hostname or ip address of your remote VM>
      User <remote user>
      IdentityFile ~/.ssh/<ssh key>
  ```

  If your remote VM is behind a firewall, you will need to setup a proxy. For example, at CERN, we can use lxplus as a ssh proxy to connect to the remote VMs, which are not accessible outside the CERN network. The following is an example of an SSH configuration file that uses lxplus as a proxy:

  ```
  Host lxplus
      HostName lxplus.cern.ch
      User <cern_username>

  Host <fqdn of your remote VM>
      HostName <hostname or ip address of your remote VM>
      User <remote user>
      ProxyCommand ssh -q -W %h:%p lxplus
  ```

  For more information, see [Connecting to a remote host](https://code.visualstudio.com/docs/remote/ssh)

- Test connection to Remote VM

  - Open the Command Palette (Ctrl+Shift+P)
  - Run the command `Remote-SSH: Connect to Host...`
  - Select the remote VM from the list of hosts
  - Depending on your SSH Configuration, you may be prompted for the password/s in the following order:
    - remote user on the proxy host
    - ssh key password or the password of the remote VMs user.
  - The bottom right corner of VS Code will show the status of the connection. If the connection is successful, you will see `SSH: <fqdn of your remote VM>` in the bottom right corner:

    ![Successful Connection](/img/vscode/remote_ssh_success.jpg)

- For troubleshooting and Tips/Tricks, see [Troubleshooting](https://code.visualstudio.com/docs/remote/troubleshooting)

**NOTE for remote development**: From hereon out, all the instructions should be run in the VSCode instance connected to your remote VM. ( i.e. the VSCode instance that shows `SSH: <fqdn of your remote VM>` in the bottom right corner)
You can use the terminal in the VSCode instance to run commands on the remote VM directly. To do so, open the terminal in VSCode instance connected to your remote VM by clicking on `Terminal` in the menu bar and then clicking on `New Terminal`.

### VSCode Extensions

Install the following extensions:

- [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python)
- [Pylance](https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance)
- [Docker](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker)
- [Remote - Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)

VSCode comes with a plethora of extensions that can significantly improve your development experience. For more information, see [Extensions](https://code.visualstudio.com/docs/editor/extension-gallery)

### Docker and Docker Compose

- Install [Docker](https://docs.docker.com/get-docker/)
- Install [Docker Compose](https://docs.docker.com/compose/install/)

#### If you get a `docker-compose: command not found` error
Docker has deprecated the `docker-compose` command in favour of `docker compose`.
To turn on `docker compose` support in `docker` command, please make sure you have installed the `docker-compose-plugin` package from the `docker-ce-stable` repository.

The full command to install `docker` and `docker compose` on RPM-based installations is:

```bash
yum install docker-ce docker-compose-plugin
```

### SSL Certificates

If you would like to debug the rucio webui and x509 authentication capabilities of the rucio server, you will need to obtain SSL certificates for your development environment. For remote VMs, you can request them from your IT department or you can use letsencrypt to generate a certificate. For local development machines, you can use [mkcert](https://github.com/FiloSottile/mkcert) to generate a certificate.

Whatever route you go, you should have a `hostcert.pem` and a `hostkey.pem` file accessible for your development environment.

## Setting up Rucio and Configuring VSCode

- Clone the rucio repository
  ```
  gh repo clone rucio/rucio
  ```
- Clone the vscode configuration repository
  ```
  gh repo clone rucio/rucio-vscode-dev-env
  ```
- Move the vscode repo inside the rucio repo
  ```
  mv rucio-vscode-dev-env rucio/.vscode
  ```
- Copy the SSL certificates to the `rucio/.vscode/certs` directory
- Open VSCode. If you are using a remote VM, open the VSCode instance connected to your remote VM.
- Go to `File` -> `Open Folder` and select the `rucio` folder
- Edit the rucio/.vscode/docker-compose.yml and change the `RUCIO_HOST` environment variable for the rucio-dev container to `http://localhost` if you are using a local development environment or to the fqdn and port of your remote VM if you are using a remote development environment.
  ![RUCIO_HOST](/img/vscode/docker-compose-rucio-host.jpg)
- Add the default url for webui in `rucio.cfg.template`, which is required to handle Cross Origin Requests (CORS) in the webui. Add the following line to the `[webui]` section of `rucio.cfg.template`:
  `      urls = <http://url-of-webui:port>
     `

  - If your server is running on a remote VM, but webui will be running locally, then the requests originating from webui to the server will have the origin `http://localhost:3000`.

  - If both the server and webui are running locally, then the requests originating from webui to the server will have the origin `http://localhost:3000`. In this case, you can set the `urls` to `http://localhost:3000`.

  - If both the server and webui are running on the same remote VM, then the requests originating from webui to the server will have the origin `http://<fqdn-of-remote-vm>:3000`. In this case, you can set the `urls` to `http://<fqdn-of-remote-vm>:3000`.

## VSCode Tasks

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- You should see a list of pre-configured tasks

![tasks](/img/vscode/rucio-tasks.jpg)

These tasks are helpful utilities for your to quickly start/stop rucio containers and to setup different types of debuggers ( server, webui, pytest, clients, ...).

### Starting Rucio

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- Select `start-rucio`
- This will start and initialize the rucio development environment. The first time you run this task, it will take a while to fetch and build the docker images. Subsequent runs will be much faster.

<video width="100%" controls>
  <source src="/documentation/img/vscode/start_rucio_task.mp4"/>
</video>

- After the command has finished, you can run `docker ps -a` to check if all the containers are up and running. You should see something like this:
  ![docker ps -a](/img/vscode/docker-ps.jpg)

### Stopping Rucio
To take down all the containers, run the `stop-rucio` task. This is equivalent to a `docker-compose down` command.

## Debugging Overview

After you have run the `start-rucio` task, you can start debugging the rucio server, webui, clients, etc.

The debugging process for Rucio in VSCode is broken down into 3 steps:

- Start the process inside the `rucio-dev` container that you want to debug
- Attach the debugger to the process
- Insert breakpoints in the code and start debugging

After you are done debugging, you should remember to

- detach the debugger from the process
- run the `cleanup` task to remove all processes inside the container that were used for debugging.

### Debugging Rucio Server

After you have run the `start-rucio` task, you can start debugging the rucio server.

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- Select `rucio-server:debug`
- Attach the debugger to the rucio server by clicking on the `Debug` icon in the left sidebar and selecting `Server: Debug` from the dropdown menu.

  ![attach to rucio-server](/img/vscode/rucio-server-debug.jpeg)

- Press `F5` to start debugging the rucio server.
- You can now set breakpoints in the rucio server code and debug the server.

After you have finished debugging, you can detach the debugger from the rucio server by clicking on the `Stop` button in the debug toolbar.

**NOTE: ** After detaching the debugger, run the `cleanup` task to remove all processes inside the container that were used for debugging.

<video width="100%" controls>
  <source src="/documentation/img/vscode/rucio-vscode-server-debug.mp4"/>
</video>

### Debugging Rucio WebUI

After you have run the `start-rucio` task, you can start debugging the rucio webui.

To debug the rucio webui, you will need to start the rucio server in non-debug mode and the rucio webui in debug mode.

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- Select `rucio-server` to start a rucio-server in non-debug mode
- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- Select `rucio-ui:debug`
- Attach the debugger to the rucio webui by clicking on the `Debug` icon in the left sidebar and selecting `UI: Debug` from the dropdown menu.
- Press `F5` to start debugging the rucio webui.

### Debugging Rucio CLI Clients ( `rucio-admin` and `rucio`)

#### Debug rucio cli client

After you have run the `start-rucio` task, you can start debugging the rucio cli clients.

To debug the rucio cli clients, you will need to start the rucio server in non-debug mode

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- Select `rucio-server` to start a rucio-server in non-debug mode

Then, you need to start the `rucio-cli:pre-debug` task to inject the debug configuration into the rucio cli clients.

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- Select `rucio-cli:pre-debug`

- Start the `Rucio:CLI Debug` launch configuration by clicking on the `Debug` icon in the left sidebar and selecting `Rucio:CLI Debug` from the dropdown menu.

- Press `F5` to start debugging the rucio cli clients.

- After that, you should open a shell inside the `rucio-dev` container

  ```
  docker exec -it rucio-dev bash
  ```

- Insert breakpoints in the rucio cli clients code.

- Run a rucio cli command to trigger the breakpoint.

After you are finished debugging, you should remember to

- detach the debugger from the debug process
- run the `rucio-cli:post-debug` task to remove the debug configuration from the rucio cli clients.
- run the `cleanup` task to remove all processes inside the container that were used for debugging.

#### Debug rucio-admin client

The instructions are the same as for the `rucio` client, except that you need to start the `rucio-admin:pre-debug` task to inject the debug configuration into the rucio-admin client.

After debugging, you should run the `rucio-admin:post-debug` task to remove the debug configuration from the rucio-admin client.

### Debugging Rucio Tests

After you have run the `start-rucio` task, you can start debugging the rucio tests.

To debug the rucio tests, you will need to start the rucio server in non-debug mode

- Open the Command Palette (Ctrl+Shift+P)
- Run the command `Tasks: Run Task`
- Select `rucio-server` to start a rucio-server in non-debug mode

Then, you should open the file containing the test you want to debug in VSCode.

- Insert breakpoints in the test code.

- Start the `rucio-pytest:debug` to start the test in debug mode
- Attach the debugger to the test by clicking on the `Debug` icon in the left sidebar and selecting `Pytest: Debug` from the dropdown menu.
- Press `F5` to start debugging the rucio tests.

After you are finished debugging, you should remember to

- detach the debugger from the debug process
- run the `cleanup` task to remove all processes inside the container that were used for debugging.
---
title: Type Annotation Guide
---

The purpose of this document is to collaboratively create the developer
guidelines for static type checking of rucio's codebase.

**TL;DR** New code has to be type annotated, old code should be migrated. Look
into [Best Practices](#Best-Practices) for specific instructions on how to use
it in our repository.

## Abstract

Python is a dynamically-typed programming language. Dynamic type checked
programming languages verify the type safety at runtime. Type-related bugs thus
occur at runtime. Tests are in place to check the types and prevent
bugs. However, tests do not always cover all possible combinations of types.

[PEP 484](https://peps.python.org/pep-0484/), which got adopted into the _Python
Language Reference_ of Python3.6 and is thus a part of Python itself, introduces
_type hints_ to Python. Type hints add more information to the source code and
allow us to automatically check the code for type mismatches. Type-related bugs
will thereby be checked at compile time (pre-runtime), rather than at
runtime. Type hints also increase the descriptiveness of our code and make it
easier to read.

Rucio does not have type hints at the moment. The plan is to introduce them
consistently to the entire project. Adding type hints to a big project is
challenging. Since the code-base is too large to introduce them with only one
PR, we will introduce the hints incrementally.

## Type Annotations

### General Information

- [**PEP 483 – The Theory of Type Hints**](https://peps.python.org/pep-0483/)
  This _PEP_ explains the theory behind type hints, as well as backgrounds to
  certain decisions.

### Syntax

There are comprehensive and descriptive documentations on the web on how to
annotate python code with type hints. E.g.:

- [**MyPy type hints cheat
  sheet**](https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html) The cheat
  sheet contains information on the syntax of type annotations and _which ones_
  to use _when_.

- [**PEP 484 – Type Hints**](https://peps.python.org/pep-0484/) Contains general
  information about type hints in Python. This includes the motivation, the
  definition, what to do with edge cases, and much more.

- [**PEP 589 – TypedDict: Type Hints for Dictionaries with a Fixed Set of
  Keys**](https://peps.python.org/pep-0589/) Explains how to use `TypedDict` and
  what to regard while using them.

## Why

Dynamically typed programming languages execute many common programming
behaviours, that static programming languages perform during compilation, at
runtime. While this leads to quick prototyping, big projects could suffer
from some consequences. In particular:

- The code is harder to read: Parameters and return types are specified in typed
  languages, they give some hints on how to call a function and what to
  expect. Bugs and inconsistencies can be easier to spot (e.g. when a "get"
  function return a list), and some IDEs display these information for a more
  convenient coding experience. All of this is missing in dynamically typed
  programming languages.
- Type related bugs do not get noticed: Calling a function with a wrong type
  (e.g. `None`) gets spotet by typed checkers. In dynamically typed programming
  languages this needs to be verified on every call.

While we have strong arguments for type annotations, there are some drawbacks:

- It takes more time to write code: The type annotations need to be specified
  and added, which is tedious in a big code base.
- They add little value if dicts are used heavily: The keys of dicts are not
  type checked, only the potential values. Big dictionaries thus have a lot of
  different value types, while the check if a key exists still needs to be done.

## Type Annotations in Rucio

Adding type annotations is not always trivial. Some types might be to ambiguous,
some might be too generic. A good reference point is existing type annotated
_Rucio_ code. It will give hints on what types may be used and how to properly
use them in the code.

:::warning
Don't just copy the types from existing code, think about them!

Ask yourself: Is this use-case the same? Could I be more specific than just
`Any`. Could I use a class instead of a `Dict`? Should I introduce a new type
instead of using an existing one? ...
:::

### What not to type annotate

Not all Python code needs type annotations. Type annotations in the `tests`, for
example, would just add clutter and add very little benefits.

The following modules will **not** be type annotated:

- `tools`
    - The tools folder contains some Python scripts. While type annotations
      would help fixing bugs, the code itself is not shipped and will not be run
      in a production environment.
    - We could add support later, however this is not our main concern atm.
- `bin`
    - The Rucio executables don't call the core or api call directly, but rather
      use the client. We could activate it once to Python2 support is dropped.
- `lib/rucio/tests`
    - The tests are volatile and should be easy to change. Type annotations
      would just add clutter and very little benefits.
- `lib/rucio/db`
    - The db module is used as a dependency of core. While we need the types, we
      use very little functions out of it. We might activate support later,
      however we want to focus on core right now.
- `lib/rucio/core/oidc.py`
    - `pyjwkest` is no longer maintained and needs to be replaced,
      and some functions are planned to be removed in the future.
      It is best to skip this file for now.

### Dependencies

To properly use the benefits of type annotated code, we also have to look into
our dependencies. All of our frequently used dependencies provide type
annotations out of the box or via extensions:

- _Python standard library_
    - Typehints were added in 3.5.0
- `sqlalchemy`
    - Type hints were introduced in version 2.0
    - `sqlalchemy-stubs` provide types for versions < 2.0
- `alembic`
    - Type hints are provided. Not important at the moment, since we are not
      adding type hints for the db migration.
- `flask`
    - Type hints are provided
- `six`
    - The `types-six` package provides typehints.
    - `six` might be removed from the repository in the future.
- `requests`
    - The `types-requests` package provides typehints.

Some types from our dependencies, like the _sqlalchemy_ `orm.session.Session`,
can be used directly. It is not needed to create our own equivalent then, except
if they get translated to a rucio owned type.

### GitHub Actions

A GitHub actions job ensures that newly written code contains type hints:

The `Check Python Type Annotations` job in the autotests checks, if new code
contains type annotations. It does this by comparing the number of missing
python type annotations before the changes with the number of missing python
type annotations after the changes. If the number before is less than the number
after, new code, which is not typed, was added. The script then exits with a
non-zero exit code. If it is equals or bigger, type annotations have been added
to the repository.

As of now, only the number of _missing_ type annotations will be used. The job
does not check for wrong type hints or inconsistencies. This (specifically
`mypy`) will be enabled once enough python type hints are added. For this
purpose, we will always add type annotations to functions, even when the type
can be inferred.

### Best Practices

To ensure a consistent usage of type hints, you should pay attention to the
following best practices:

**Use _Python 3.6_ style type hints**

 - There are multiple ways to specify type hints in Python. We agreed to use
  the Python 3.6 style, since it's easy to read and we don't need the
  backwards-compatibility.
  - E.g. favor `def add_rse(rse: str, vo: str = 'def', ...) -> str:` over `def
  add_rse(rse, vo='def', ...): # type: (str, str, ...) -> str`

**Use _bare_ type hints over [ones with
quotes](https://peps.python.org/pep-0484/#runtime-or-type-checking) and `if
tying.TYPE_CHECKING:`**

  - Quoted type hints enable "forward references". This enables us to not
  execute expensive code while still having type checks.
  - As long as the performance is immesurable small and not a problem, this
should be avoided, since it > [name=Joel Dierkes] Dunno about this part. Should
we use `if typing.TYPE_CHECKING:` and quoted types or avoid them?

**Be as specific as possible**

  - If the types of the keys and values of a dict are known, specify
  them. (E.g. Use `Dict[str, str]` over `Dict`).
  - If the types of all items in a list are known, specify them
    (E.g. `List[int]` over `List`)
  - ...

**Avoid `Dicts`**

  - Strongly typed structures should be preferred, since they are more
  descriptive and easier to use in the future.
  - "Was the id key in the `Dict` named `_id` or `id`?" is a question that
    should not occur.

**Avoid the `Optional` type**

  - The `Optional` type highlights that a value _might_ be `None`. As a result
  the value _has_ to be checked for `None` on every usage (`if value: `).
  - While sometimes this cannot be avoided, the `Optional` type should be used
    sparely. Most of the times a proper initialization of the variable is
    enough to get rid of it. If it makes sense that a type can be
    "non-existent", it is appropriate to use the `Option` type.

**Avoid the `Union` type**

  - The `Union` type indicates, that one of multiple different types may be
  returned. This can be confusing and resolves in the need of testing, which
  type is returned.
  - Split the function or variable, which used the `Union` type, in multiple
    ones. This resolves in more readable code and is unambiguous. This also
    follows the `A function does one, and only one thing` rule.

**Use
[`typing.NoReturn`](https://docs.python.org/3/library/typing.html#typing.NoReturn)**

  - It is used to indicate a never terminating function (e.g. `while
  True:`). Use this annotation to indicate these kind of functions.

**Use
[`collections.abc.Iterator`](https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator)
over
[`collections.abc.Generator`](https://docs.python.org/3/library/collections.abc.html#collections.abc.Generator)**

  - `collections.abc.Generator[YieldType, SendType, ReturnType]` takes three Type Vars:
  The Type that gets yielded, the type that gets send back to the yield, and the
  return type of the function. If a function does only yield values, but does
  not take back values from the yield and also does not return anything with the
  `return` keyword, the type is `collections.abc.Generator[YieldType, None, None]`. This
  is equivalent to `collections.abc.Iterator[YieldType]`. We favor the `Iterator`
  approach over the `Generator` one because it's more understandable and easier
  to read.

### Common Types

To ensure a consistent use of type annotations in Rucio, here is a list of
common variables with their corresponding type:

| Code section | Variable | Type | Description |
| ------------ | -------- | ------------------------------ | ----------------------- |
| * | session | sqlalchemy.orm.session.Session | The sqlalchemy session. |
| DID | scope | `str` | The scope of a DID.  |
| DID | name | `str` | The name of a DID.  |
| DID | account | `str` | The account name.  |
| DID | did_type | `str` | The DID type.  |
---
id: sprint_planning
title: Sprint planning process in Rucio
---

Work planning in Rucio is organised in two main ways:
- **Release roadmaps**: each roadmap corresponds to a major release, as outlined in the [Release Policy](started/releasepolicy.md).
- **Sprint cycles**: each sprint cycle lasts 2 weeks.

The main objectives of sprint cycles are to:
- Break down each roadmap into 2-week sprint iterations
    - This should result in smaller, more actionable items
- Have more visibility of unplanned tasks that occur during a major release cycle, and that might not be part of the initial roadmap
- More granular understanding of what the members of the team are currently focusing on in the shorter term.

As the Rucio development team is distributed,
the sprint planning process in Rucio is **completely asynchronous, with no meetings**,
resulting in minimal overhead.

## Duration
Given a two-week period, the sprint:
- Begins on the Monday of the first week
- Ends on the Friday of the second week

## Availability
Sprint participants might have other responsibilities to handle, besides the development of Rucio. Because of this, they should determine their availability (i.e. total number of days) before they can plan which issues to work on for a sprint.

Code review is an important, continuous, task to be done by all developers. Availability should be reduced accordingly to cover for that.

For example, for a full-time developer devoting 100% of 10 working days to Rucio development, perhaps only 8 days should be reported, to cover time for reviews, meetings, etc.

## Issues

### Priority
In a sprint, issues are categorised by their priority:
- priority: issue is part of the release roadmap objectives
- non-priority: issue is not related to the release roadmap objectives

### Size estimation
Each issue is assigned a size based on how many days of work the responsible developer estimates the issue to take.
After/during the roadmap planning, the size estimates should be set for all priority issues.
For non-priority issues, the estimates are set when the issue is added to the project, thus ideally all issues in the project should have a size estimate.

Given a sprint period, a developer should plan issues totaling their available days for that week (see the [Availability](#availability) section.)

The following labels are available for size estimation:
- **XS**: less than a day of work
- **S**: about a day of work
- **M**: less than 3 days of work
- **L**: about one week (5 days) of work. This issue **should** be broken down into individual sub-issues
- **XL**: more than one week (5 days) of work. This issue **must** be broken down into individual sub-issues


## Moderating a sprint
Each sprint is moderated by one of the developers. This role rotates on a voluntary basis.

The moderator is in charge of:
- Beginning the sprint:
    - Validating that the GitHub project board has been correctly populated
- Providing brief status updates during the Rucio weekly meetings
- Ending the sprint:
    - Reviewing the retrospective comments left by the developers
    - Validating that the GitHub project board has been correctly updated

## Participating in a sprint
As a sprint participant, at the beginning of the sprint you should:
- Respond to the initial sprint start thread with your availability (in days), and the issues with their estimates
- Update the GitHub project board accordingly

And at the end you should:
- Respond to the sprint end thread with your review and comments
- Update the GitHub project board accordingly

## FAQ

### If I am done with my tasks for this sprint and the sprint is not over, should I add more issues to the sprint?
You can add new issues and mark them with an `unplanned` tag on the sprint board.
In general, we should plan sprints in a way that ensures that the work is neither delayed nor completed too much in advance. If this happens regularly, it might be an indication that we are not estimating sizes properly.

### An issue is less/more work than I thought, should I change the estimate?
The size estimate should not be changed once the issue has been added to the sprint with that estimate.
In the sprint review phase, you can share a reflection on why you found the real effort to not match the estimated effort.

### I won't be able to finish all the issues in the sprint. Should I remove them?
It's important to leave all the issues in the sprint, in order to reflect on whether we planned correctly or not.
We can estimate the remaining effort for the unfinished issues in case we want to move them to a subsequent sprint,
but they should not be removed from their sprint.

### The current sprint is over and I have an unfinished item that I would like to continue in the next sprint. What should I do?
You can move the unfinished item to the next sprint, updating its size according to how much work is estimated to be left.

For example, if you would like to carry over an unfinished item of size M, and you estimate that it will need 1 more day of work,
you can move it to the next sprint and change its size to S.---
id: webui_frontend_vscode_dev_env
title: Setting up a WebUI Developer Environment using Visual Studio Code
---

## Different Usecases

There are different usecases when developing for the WebUI, this leads to
varying requirements for the development environment. This guide will cover two
usecases:

1. UI development only (i.e. using Storybook).
2. Full WebUI development, requiring the connection to a Rucio server.

In both cases, the proposed development environment is run fully local. Visual
Studio Code is more than just an editor, and we attempt to make use of its "Dev
Container"-tooling as much as possible.

## UI Only

Remember that the WebUI frontend is composed of *Components*, which are
developed with the help of a tool called [Storybook](https://storybook.js.org/).
Storybook is, in its own words, a "frontend workshop for building UI components
and pages in isolation". To us, it is a viewer which allows us to inspect
individual components and interactively manipulate the inputs to these
components. An example would be viewing a clickable button and manipulating the
"disabled" attribute, noticing that the colour changes from blue to gray, etc.
Storybook is also very helpful when testing accessibility and design.

For a developer whose only intent is to add to or edit these components (found
under `webui/src/component-library`), it is helpful to run a docker container
with NPM and TypeScript.

Open the editor commands with `Ctrl`-`Shift`-`P` and run `Dev containers: reopen
in dev container`. Select `Node.js & Typescript` in the follow-up menu. Go with
the default values in the following menus.

You can close the remote connection via the editor commands. From now on, you
can reopen the same dev container by opening the folder locally and selecting
`reopen in dev container` from the editor commands. You can also directly open
the folder in the dev container, for example using `File` > `Open Recent` and
then selecting an element such as `~/foo/bar [Dev Container]`.

It makes sense to install some extensions for a better dev experience:
* Tailwind CSS Intellisense
* ESLint
* Jest

These extensions are specific to the project at hand. Of course, extensions such
as Github Copilot and GitLens are still helpful.

An further extension that is extremely important is *Run on Save* by
emeraldwalk. This will need to be configured: enter the settings with `Ctrl`-`,`
and search `Emeraldwalk: Runonsave`. Click on `Edit in settings.json`. Add the
following configuration:

```json
"emeraldwalk.runonsave": {
    "commands": [
        {
            "match": "\\.tsx$",
            "cmd": "npm run build-tailwind"
        }
    ]
}
```
to ensure that the `build-tailwind` command will be run each time you save a
`*.tsx` file.

You can now run Storybook using `npm run storybook`.

## Connecting to a Rucio instance

When testing the full WebUI stack, it will be necessary to connect to a Rucio
backend. This can be done by running a Rucio instance locally.

We begin by starting all the Docker containers required.
1. Start the dev container as described above.
2. Clone Rucio from the [Github Repository](https://github.com/rucio/rucio)
3. Open the folder in VS Code.
4. Open the editor commands and run `Docker Compose Up` and select
  `docker-compose-storage.yml`.

:::note

Starting the docker containers using `docker-compose-storage.yml` will also
create several RSEs, DIDs, etc. on the rucio instance which are quite helpful
when running tests.

:::

We must now connect the dev container to the rucio container network.
1. Figure out the docker name of the dev container. Run `docker ps` and search
  for name which does not contain the string `rucio`
2. Connect using `docker network connect dev_default <container name>`.
   `dev_default` is the name of the network shared by the docker containers spun
   up using `docker-compose-storage.yml` from earlier.

Finally, we must change the `RUCIO_AUTH_HOST` and `RUCIO_HOST` in the
`.env.development.local` file at the root level of the WebUI repository to point
to the rucio container. By default, this should be `https://dev-rucio-1:443`.

Test the setup by logging in using the credentials username `ddmlab`/ password
`secret`/ account `root` under the VO `Default`.
---
id: webui_frontend
title: Developing the WebUI Frontend
---

## Notes on external frameworks
The Rucio WebUI frontend uses two main frameworks: [TailwindCSS](https://tailwindcss.com/)
and [React](https://react.dev/learn). Both have excellent documentations, which I will
not attempt to replace with these notes. Instead, I will extend them and concretise them
with the focus on the Rucio WebUI frontend and the design that it entails.

Nonetheless, I will do a quick rundown of the most important and most frequently used
aspects of both frameworks in order to save a new developer time. In the end, I will also
introduce [StorybookJS](https://storybook.js.org/), which is our tool to visualise the
individual tools of our website.

### React

We use React as our frontend JavaScript framework. It is essentially a tool for building UI
components.

#### Components

The building-blocks of our webpages are React Components. The webpages themselves are React
Components themselves. Individual components can be composed with each other to produce
more complex components, in the sense that a button component can be used to populate a
widget which is then used in a page.

:::note

The Rucio WebUI is written in TypeScript. In these docs, JavaScript and TypeScript might
be used interchangeably.

:::

These components JavaScript functions which return HTML code. We use `.tsx` files, which mean
that HTML markup and TypeScript code can be used in the same file. This can take the following
shape:


```tsx
export const H1: React.FC<JSX.IntrinsicElements["h1"]> = (
    { ...props }
) => {
    const { children, className, ...otherprops } = props;
    return (
        <h1
            className={
                // handled later
            }
            {...otherprops}
        >
            {children}
        </h1>
    )
}
```

Several things are going on here. The overall structure, however, is that of an
arrow function `() => {}` returning an HTML tag of the shape
`<h1>something</h1>` is defined and assigned to `const H1`. The arrow function
is typed, this will be handled later.

:::tip

In TSX, HTML markup is inserted into TypeScript after a `return` statement.
TypeScript is inserted into HTML when wrapped in `{}`. VS Code does proper
syntax highlighting, which will help out.

:::

This function can now be imported and called in the following way:

```tsx
<H1>Hello world!</H1>
// more or less equal to
<h1>Hello world!</h1>
```

When using React components, take note of the following:
- Custom components must start with a capital letter.
- You must return only one top-level HTML tag. You may nest other tags within this one.
- React components can nest other react components. They must ultimately resolve to HTML, though.

In our components, we usually make sure to typecast the functions as React Function Components.
These can in turn be cast to take the attributes of a specific HTML tag, which is chosen
to be the top-level HTML tag returned.

In the following example, we define a type `TextInputProps` which combines the
attributes of the `<input>`-tag (all of which are optional) with the
`onEnterkey?` attribute (the `?` signifies that this is an optional attribute). The
resulting component can take any of these attributes.

Note that the `...props` syntax simply means that any parameters `<TextInput foo bar/>`
passed to the component that are not `onEnterkey` are swallowed by the `props` object.


```tsx
type TextInputProps = JSX.IntrinsicElements["input"] & {
    onEnterkey?: (event: any) => void,
}

export const TextInput: (
    React.FC<TextInputProps>
) = (
    {
        onEnterkey,
        ...props
    }
) => {
    // shown in next code example
}
```

Typing our components this way means that we can easily extend standard HTML tags, while keeping
all of their original functionality (i.e. the way they deal with attributes passed to them) *without*
the need to implement each attribute ourselves.

:::info

These attributes are referred to as *content attributes* and it is a very good idea to get
acquainted with the different attributes that can be assigned to the different HTML tags.
We will cover some examples in the next section. You can find more information here:

- [HTML Specification](https://html.spec.whatwg.org/)
- [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML/Attributes)
- [W3Schools](https://www.w3schools.com/tags/ref_attributes.asp)

:::

Finally, you can pass these attributes into the component state (usually done
with components that render pages), or pass them into the sub-components that
make up the component. The latter is what happens in the following code block,
which  is taken from `TextInput` again:

```tsx
// unpack props props to be handled individually and otherprops
const { children, onKeyDown, ...otherprops } = props
return (
    <input
        onKeyDown={(e) => {
            if (e.key === "Enter") {
                // `?.()`-syntax used because onEnterkey could be undefined
                onEnterkey?.(e)
            }
            // could also be undefined
            onKeyDown?.(e)
        }}
        {...otherprops} // unpack otherprops here
    >
        {
            // pass the content between <input></input> in here
            children
        }
    </input>
)
```

In the previous example, a native HTML tag has been enriched by the `onKeyDown` function
by a special `onEnterkey` functionality. In all other ways, the new component is the same
as the included, standard `<input>` tag.



#### Attributes

:::tip

React will rename some of the component attributes. An example of this is the
`className`-attribute, which replaces the native HTML keyword `class`. This is
done to prevent namespace clashes.

:::

##### className

A more detailed description of how we style our components will follow in the
Tailwind-section. The important concept is that we assign classes to each component,
and these classes are then used as selectors to apply certain styles to the the component.
Classes are assigned to the `className` attribute as space-separated strings.

```tsx
<p
    className="text-xl font-serif"
>
    Hi
</p>
```

We often make use of the `twMerge` function. This function swallows all string arguments
and returns a single class string. Importantly, classes added later may override those added
before them, which is not the case when we do not use `twMerge`. An example of this
is seen in the following code-block.

```tsx
<div
    className={twMerge(
        "flex flex-col sm:flex-row",
        "list-none font-bold",
        "bg-white dark:bg-gray-900",
        "border-0 dark:border-2 dark:rounded-md dark:bg-clip-content",
        className ?? "", // optionally add classes to override what came before
    )}
>
```

It is a good idea to use `twMerge` wherever the class definition contains conditionals,
overrides, or is so long that it would span multiple lines.

##### PageProps

The highest-level components produced in the frontend are the pages. Remember
that a component is a function that returns HTML. The input of these functions
can be quite complex, especially for the highest-level components, which combine
a lot of data from different sources.

In the following example, we define an interface for the `ListDID` page. Note
that input to the page is given in the form of a `ViewModel` (`DIDViewModel`,
`DIDMetaViewModel`), while the functions are for the page to communicate to
deeper-lying components.

```tsx
export interface ListDIDPageProps {
    comdom: UseComDOM<DIDViewModel>, // this ViewModel comes in a stream, more later.
    didQuery: (query: string, type: DIDType) => void,
    didMetaQuery: (scope: string, name: string) => void,
    didMetaQueryResponse: DIDMetaViewModel,
}

export const ListDID = (
    props: ListDIDPageProps
) => {
    // ...
}
```

We do not want the frontend to handle any fetches or streams itself, hence the
functions `didQuery` and `didMetaQuery` (which start a stream and query
metadata, respectively) are defined outside of the component and only passed to
it. It also means that the page can be run even without a backend attached to
it, we simply replace these functions with dummies (and the ViewModels with fake
data).

##### aria

ARIA stands for [Accessible Rich Internet Applications](https://www.w3.org/TR/html-aria/)
and defines the standard for how to make a web application accessible to users that
are somehow impaired. One way of making the website more accessible is the conscious inclusion of
ARIA-attributes.

A simple but common example would be adding an ARIA-label to an element which would
otherwise only be described by an icon/glyph. Note that screen-readers give priority
to semantic `<label>`-elements over `aria-label`. In the following example, however,
adding such an element would not be practical.

```tsx
<Button
    icon={<HiChevronDoubleLeft />}
    aria-label="First Page"
    // ...
/>
```

:::caution

Do not overuse the ARIA-attributes. By using [semantic
HTML](https://developer.mozilla.org/en-US/docs/Glossary/Semantics), many of these attributes
will already be filled in correctly.

:::


#### Hooks
Hooks are reusable functions. They all start with `use`.

##### useState
Add a state variable to the component, i.e.
```tsx
const [state, setState] = useState<TypeOfState>(initialState)
```

##### useEffect
The signature of this hook is `useEffect(setup, dependencies?)`, where `setup`
is the function run when any value in the `dependencies` list changes. The
values in the `dependencies` list can be thought of as *reactive*, because if
any one of them changes, the `setup` function is called as a *reaction*.

Importantly, when the `dependencies` list is empty, the `setup` function is only
run *once* after the initial render. Since `useEffect` is only run on the
client, this is a very useful way of running code (seemingly) on page load, but
client-side. An example of this would be fetching additional metadata. An
example of this can be seen here:

```tsx
useEffect(() => {
    didMetaQueryBase(params.scope, params.name).then(setDIDMeta)
}, [])
```

:::info

In this case, the function `didMetaQueryBase` returns a value of type
`Promise<DIDMetaViewModel>`.  Learn about `Promise` in TypeScript, and how to
deal with it,
[here](https://basarat.gitbook.io/typescript/future-javascript/promise)

:::

:::caution

Note that passing no dependency array at all will lead to the Effect running
after every single render of the component!

:::

##### useComDOM

This is a custom hook returning a `UseComDOM<T>` (Communication Document Object
Model) object. It serves as a wrapper for streamed data, also providing
functions to inspect and control the stream.  It is a generic which is bound to
type `T`, which itself is usually a ViewModel.  The `useComDOM`-hook is most
strongly associated with the StreamedTables, whose documentation can be found
[here](./streamedtables.md).

##### useResponsiveHook

The website is designed to be responsive, this means that there are several
breakpoints at which the assigned styles can change during window resizing. In
most cases, this is by Tailwind, our CSS framework. In some cases, you want
access to the window width class (small, medium, etc.) from within your code in
order to manipulate components programmatically. An example of this is the
visibility of columns in StreamedTables, which is defined in a `TableStyling`
object.

This custom hook implements a window resize event listener and exposes the
following object:

```tsx
return {
    sm: windowSize[0] > 640,
    md: windowSize[0] > 768,
    lg: windowSize[0] > 1024,
    xl: windowSize[0] > 1280,
    xxl: windowSize[0] > 1536,
} as ResponsiveHook
```

The width classes in pixels correspond to those defined by
[Tailwind](#responsive-design).





### Tailwind
Tailwind is the CSS framework which we use to style our components. Instead of writing
CSS directly, we assign classes to components via the `className` attribute. These
classes act as selectors for the stylesheet to selectively assign styles to specific components.
The main concept is that Tailwind consists of many utility classes which, by themselves, only
contribute a small amount of style information (e.g. the background colour, the display mode, etc.).
Many Tailwind classes map directly to a CSS attribute. In adding multiple classes, we
can iteratively build complex styles from these basic building blocks.

:::caution

The philosophy is to build all styles from these common basic building blocks,
and to avoid creating a new custom styles grouping these basic classes into more complex ones.
While it might seem smart to collect a frequently-reused set of classes into one larger one,
this is actually a Tailwind-antipattern in most cases.

:::

An example of this in action would be the custom `Button` created for the WebUI project.
In the simplified version shown in the following code-block, the `twMerge`-function introduced
in the section on the `className`-attribute was used to compile multiple strings into a
single one that could be used by the component. Importantly, we can use code (such
as the ternary operator) to specify branching logic that returns us the style. In this
case, branching logic is used to style the button depending on whether it is disabled or enabled.

```tsx
<button
    disabled={props.disabled}
    className={twMerge(
        "py-1 px-3 h-8 rounded", // style the button shape
        "bg-blue-500 hover:bg-blue-600 text-white", // style button colours
        props.disabled ? (
            // show this style if disabled
            "cursor-not-allowed bg-gray-500 hover:bg-gray-500 text-gray-200"
            : "cursor-pointer" // show this style if enabled
        ),
        "font-bold", // style text
        className ?? "" // optionally override all this
    )}
>
```

We will cover how we conventionally structure the contents of `className` and `twMerge`.
We will also discuss dark mode and responsive design as examples of metaclasses, but
we strongly encourage you to work through the Tailwind documentation, which is full of
examples and very helpful. In addition, it is a good idea to look at external resources
to fully understand the flexbox and other display models in HTML.

#### Structuring Class Names, Overriding

When adding Tailwind classes to a component, it is useful to ask the following questions in
order:

1. What are the dimensions of this component?
1. What space do you want around the element's border (margin), and how much
space do you want between the element's border and its content (padding)?
1. Considering the border of the component. Should it be rounded, should the
border be visible, how thick/which colour shall the border line have?
1. What is the background colour of this component?
1. If the component directly contains text, how shall this text be styled?
1. If the component is a container for several child components, how shall these be arranged? (Consider reading up on `flex`, `grid`, etc.)
1. If the component is within a flexbox, how shall it respond to resizing? (grow, shrink, etc.)
1. Which style of cursor should be displayed when the cursor enters the component?
1. Do you need to override any of these styles depending on external props?
Remember that you can use branching logic within the classname definitions and
that later classes override previous ones.
1. Consider dark mode and override previous classes with the `dark:`-prefix.
1. Consider different window sizes and create a responsive component by using
the size-prefixes `sm:`, `md:`, `lg:`, etc.
1. Consider other necessary overrides with any of the other
Tailwind-pseudoclasses.

In general, we attempt to structure the contents of `twMerge` this way, but it
might make more sense for you to group your classnames in a different manner.

As touched on in the above listing, Tailwind has the concept of *pseudoclasses*
to optionally only enable a certain selector when a precondition is given. A
metaclass will override the vanilla class (even without `twMerge`). For example,
a component with classes `bg-blue-500 hover:bg-blue-600` will turn a darker
shade of blue when the mouse cursor comes to hover over it.

:::tip

Also look at the
[reference](https://tailwindcss.com/docs/hover-focus-and-other-states#pseudo-class-reference)
for Tailwind-pseudoclasses.

:::

#### Responsive Design

Most of the pseudoclasses are straightforward, nevertheless, we want to give a
short introduction to responsive design with Tailwind. The way Tailwind handles
the pseudoclasses responding to changes in page width might be unintuitive at
first, since Tailwind is designed "mobile first". In essence, a prefix adds a
*minimum* size, above which the style is applied. For example, the string
`w-full md:w-24` will apply the `w-full`-class when the page width is below the
threshold for `md`.

The table below serves as a reference for the size thresholds used in Tailwind.
These sizings are also used throughout the whole project.

| Prefix | Minimum size \[px\] |
| :----- | ------------------: |
| `sm` | 640 |
| `md` | 768 |
| `lg` | 1024 |

:::tip

Use the `useResponsiveHook` to do conditional resizing (such as in
StreamedTables, where Tailwind is difficult to use). It combines an
EventListener and all these thresholds into a reusable hook.

:::

#### Compiling Tailwind

Tailwind compiles the minimal stylesheet required to use all the classes defined
in the project.  The stylesheet is then automatically loaded by the relevant
NextJS components and made available in every page of the WebUI. You can find
the compiled stylesheet under `src/component-library/outputtailwind.css`, where
it lives at the top-level folder of the `component-library` with the other
components. Tailwind can be configured via the
`src/component-library/tailwind.css` and the `tailwind.config.js` file in the
root folder, see the Tailwind docs for more information on this.

When you want to recompile Tailwind, you can run `npm run build-tailwind`, which
takes `tailwind.css` and outputs `outputtailwind.css`. If you followed the
tutorial laid out [here](./webui_frontend_vscode_dev_env.md#ui-only), you will
have configured VS Code to automatically recompile Tailwind each time you save a
relevant file. We strongly recommend this.

Any changes to the `outputtailwind.css`-file will immediately become visible on
the StorybookJS-viewer or on a NextJS dev server. This allows for rapid
prototyping and quick changes.

:::info

You can define additional regex matchers for the Tailwind compiler to use to
match classNames. These are defined in `tailwind.config.js` under
`module.exports.content.files.extract`.

:::

### Storybook

StorybookJS is calls itself a "frontend workshop for building UI components and
pages in isolation".  Importantly, it allows us to view our React components,
interact with them and also play around with their inputs. This allows us to
have direct access from the smallest components (such as text, buttons, inputs)
up to the most complex pages. A further usecase is for demonstrations, as well
as testing dark mode and responsive design.

Each component in the `src/component-library`-folder has an associated *story*,
i.e.

```
$ ls src/component-library/Pages/RSE
> ListRSE.tsx
> ListRSE.stories.tsx
> PageRSE.tsx
> PageRSE.stories.tsx
```

Inside each of these stories lies code which returns the associated component
wrapped within a Storybook Function. This function is irrelevant to us.
Importantly, we can pass arguments to the component, essentially inserting mock
or fake data. These inputs serve as defaults for Storybook, one can also
manipulate these inputs from within the Storybook web interface. We can also
pass in mock functions if this is required, these cannot be manipulated from the
web interface.

:::tip

Use the `create-component-story` tool (found within the `tools`-folder) to
rapidly create a new dummy component together with its story. You might need to
change the value of `title` in the default export.

:::

An example of how arguments are passed into a storybook function can be seen in
the following code block.  Fixtures and the `mockUseComDOM`-function will be
explained below.

```tsx
PageDID.args = {
    didMeta: fixtureDIDMetaViewModel(),
    fromDidList: "yosearch",
    // Parent DIDs [FILE]
    didParentsComDOM: mockUseComDOM(
        // create array of length 100, populate with fake DIDViewModel objects
        Array.from({length: 100}, (_, i) => fixtureDIDViewModel())
    ),
    //...
}

```

##### Fixtures

We make use of many kinds of fixtures to insert mock data into our stories. The
fixtures can be found in `test/fixtures/table-fixtures.ts`, where each ViewModel
used in the frontend has a function that returns a mock version of it. A bunch
of utility functions can be found in this file, which can help in creating the
mock fixture.

##### mockUseComDOM

The `table-fixtures.ts`-file contains a fixture which deserves to be mentioned
by itself: the `mockUseComDOM`-function, which takes a list of datapoints of
type `T` and then mocks the `UseComDOM<T>` object. This ComDOM-object can then
be passed on to StreamedTables.  Since the relevant attributes and methods that
the StreamedTable accesses are all fully mocked, this function allows you to
show StreamedTables with mock data and interfacing even when no backend is
connected.

:::caution

Note that these random fixtures do not work well with the NextJS Dev Server,
which will raise Hydration Errors. This is because the page rendered on the
server does not match the first render of the page on the client (because both
use random fixture generators). However, you can choose to ignore the Hydration
Errors for the time being.

:::

<!-- ## Design decisions

## Components

## Folder structure -->
---
id: streamedtables
title: Using the WebUI StreamedTable Component
---

## Data presentation in the WebUI
An issue we encountered in the process of developing the WebUI which is
particularly relevant in the context of writing a Rucio frontend is streaming.
The "old" web frontend (which we want to replace) had the issue that it would
freeze on starting larger queries, with no indication as to the progress made or
potential errors encountered. Even when everything worked well, these "freezes"
could last for up to several minutes in which the page was effectively unusable.

The reason for this was the fact that any responses from the Rucio server (which
supports streaming) were stored in a WebUI intermediary, before being forwarded
to the client in one batch. Since Rucio responses can be huge (and often, there
is no prior indication of the size which a query will take), this could lead to
freezing of unknown duration.

The solution would be to pass the stream from Rucio through the WebUI
backend/core to the frontend. This required special software engineering on the
backend, which shall not be covered here. The frontend accesses the streams via
a system called the `ComDOM`, short for `Communication Document Object Model`.
It accesses the streams provided by the backend API layer (and NOT the Rucio
server itself) and wraps the data with further tooling (to start/pause/stop the
stream, inspect the state, etc.).

:::note

Streamed data is unavoidable in the WebUI. The UI accesses this data in the form
of the `ComDOM`.

:::

The generic component when dealing with streamed data is the `StreamedTable`,
which is a custom HTML table framework based on [Tanstack
Table](https://tanstack.com/table/v8/docs/introduction) and optimized for
use with the streams fed in via the ComDOM. A StreamedTable is defined by a set
of react props passed into the component (which will be described below). The
framework is completed by a plethora of ready-to-use components which solve the
majority of required usecases when it comes to streamed tables.

![StreamedTable Example](/img/webui/streamedtable.png)

## StreamedTable

The StreamedTable is a fully typed react component based on the HTML `table` element. The props it takes are defined in the codeblock below.

```typescript
type StreamedTableProps<T extends BaseViewModel> = JSX.IntrinsicElements["table"] & {
  tablecomdom: UseComDOM<T>
  tablecolumns: any[]
  tablestyling?: TableStyling
  tableselecting?: TableSelecting<T>
}
```

Explanation

1. The table expects a stream of elements of type `T`, where `T` extends the
`BaseViewModel`. In practice, this means that an entity is wrapped in an object
providing status and an optional errormessage.

2. The data shown by the table is passed into the `tablecomdom` prop as an
`UseComDOM` object. This object also provides functions to inspect and control
the stream. The generic `UseComDOM` type is bound to type `T`.

3. The table structure (i.e. the way data is structured into columns) is defined
by the `tablecolumns` prop. This is essentially an array of column definition
objects (which are taken from Tanstack Table), but there are exceptions, which
make typing this prop difficult. Most of your time building a streamedtable is
spent writing this definition.

4. The first optional prop, `tablestyling` is a structure containing style
commands for the table. The definition of the `TableStyling` type is given
below.

5. `tableselecting` optionally defines how the user can select (single- or
multi-select) elements from the table. It also controls the "breakout"
functionality. This is discussed later.

6. `JSX.IntrinsicElements["table"]` means that the StreamedTable component will
return a react component with the HTML `<table>` tag as its top-level component,
which in turn means that any valid HTML table-properties can be passed to the
StreamedTable, and these will be passed on to the top-level component. This
specifically also includes styling the top-level `<table>` via `className` but
also ARIA-properties.

### TableColumns
:::tip

When creating a new component or editing a new one, it is very useful to view the table in Storybook.

:::

The definition of the tablecolumns can be taken from the
[Tanstack Table Documentation](https://tanstack.com/table/v8/docs/guide/column-defs).
A simple example shall be discussed here nonetheless. The following code snippet
has been adapted from `PageDIDMetadata`, with custom styles and most custom
components removed for clarity.

```tsx
// imports and type definitions
import { createColumnHelper } from "@tanstack/react-table"
type DIDKeyValuePairViewModel = {
    // from DIDKeyValuePair entity
    key: string;
    value: string;

    // from BaseViewModel
    status: 'success' | 'error' | 'pending';
    message?: string;
}

// the streamedtable receives a stream of `DIDKeyValuePairViewModel`
// each element is converted into a row
// initialise the columnHelper to this type
const columnHelper = createColumnHelper<DIDKeyValuePairViewModel>()


// define the tablecolumns array
const tablecolumns: any[] = [
    // Accessor columns have an underlying data model which means they can be
    // sorted, filtered, grouped, etc.
    columnHelper.accessor("key", { // this column accesses the "key" attribute
        id: "key",
        // function to render each body cell of the column, return JSX object
        // "info" contains the value but also other useful context -> read docs
        cell: info => {
            return (
                <span>
                    {info.getValue()}
                </span>
            )
        },
        // function to render header cell of column, return JSX object
        // "info": context useful for the creation of a header cell -> read docs
        header: info => {
            return (
                // this is a custom component that can be placed into the header
                // they are discussed further below
                <TableFilterString
                    column={info.column}
                    name="Key"
                />
            )
        }
    }
    ),
    columnHelper.accessor("value", { // column accesses the "value" attribute
        id: "value",
        cell: info => {
            return (
                <span>{info.getValue()}</span>
            )
        },
        header: info => {
            return (
                // must not use a custom component, a simple <h3> tag is fine!
                <h3>Value</h3>
            )
        }
    })
]
```

Passing this column definition into a StreamedTable together with the ComDOM supplying the data like so

```tsx
<StreamedTable<DIDKeyValuePairViewModel>
    tablecomdom={comdom}
    tablecolumns={tablecolumns}
/>
```

will return a fully functional StreamedTable. This can be expanded upon to
create much complex tables with multiple columns.

:::info

The StreamedTable is a wrapper for Tanstack Table. In particular, the column
definitions have been taken over unchanged. This means that the documentation
and examples provided by Tanstack are also valid in the StreamedTables, check
them out.

:::

### Common components for column headers

A number of reusable components have been added to the StreamedTable package
which can be used to implement frequently-needed functionality such as sorting
and filtering. An example of this has already been given in the code above, in
which a `TableFilterString` component used.

These components are added in the column header definitions within the
`tablecolumns` definition, i.e.

```tsx
columnHelper.accessor("foo", {
    header: info => <TableCommonComponent bar={bar} />,
    ...
})
```

The remainder of this section will introduce each of the common table
components.


#### TableSortUpDown
This component toggles the Tanstack Table sorting mechanism on mouseclick and
cycles between no sorting, sorting in ascending order and sorting in descending
order. Examples of column entries which can be sorted are numeric data and
dates.

![TableSortUpDown Dirty](/img/webui/sort-updown-dirty.png) ![TableFilterBoolean Clean](/img/webui/sort-updown-clean.png)

It takes the following props:

```tsx
props: JSX.IntrinsicElements["div"] & {
    name: string
    column: Column<any, any>
    element?: JSX.Element
    stack?: boolean
    nocollapse?: boolean
}
```
Explanation:
* `name` is the string which will be used as the column header with a default
  style if no `element` is defined
* `column` is the table column the header belongs to.
* `element` is the JSX element which overrides `name`.
* `stack` determines whether the icon signifying the sorting state and the name
  element are stacked vertically instead of horizontally. The example picture
  above uses vertical stacking.
* `nocollapse` will force the `name` (or `element`) to remain visible even on
  small screen widths (below 768px).
* any props valid for a `div` element will also work.

#### TableFilterDiscrete

On mouseclick, it cycles through a list of keys and only displays the rows in
which the column entry matches the key. Used when the column data is restricted
to a small number of options, such as the Rucio enums.

![TableFilterDiscrete Dirty](/img/webui/filter-discrete-dirty.png) ![TableFilterDiscrete Clean](/img/webui/filter-discrete-clean.png)

```tsx
type TableFilterDiscrete<T> = JSX.IntrinsicElements["div"] & {
    name: string, // see TableSortUpDown
    keys: T[],
    renderFunc: (key: T | undefined) => JSX.Element,
    column: Column<any, T>, // see TableSortUpDown
    stack?: boolean // see TableSortUpDown
}
```
Explanation:
* `keys` is a list of type `T` where `T` is the type of the column entries.
  Usually a string enum.
* `renderFunc` is the function used to render the icon to the side or below the
  text (depending on whether `state` is set).

#### TableFilterBoolean
A special form of `TableFilterDiscrete` in which the values are booleans. This
means that no `renderFunc` needs to be supplied.

![TableFilterBoolean in Action](/img/webui/filter-boolean-all.png)

```tsx
// see TableFilterDiscrete
type TableFilterBoolean = JSX.IntrinsicElements["div"] & {
    name: string,
    column: Column<any, boolean>,
    stack?: boolean
}
```

#### TableFilterString
A filter in which the column entries are strings.

![TableFilterString Dirty](/img/webui/filter-string-dirty.png) ![TableFilterString Clean](/img/webui/filter-string-clean.png)

```tsx
type TableFilterString = JSX.IntrinsicElements["form"] & {
    column: Column<any, string>, // see TableFilterDiscrete
    name: string, // see TableFilterDiscrete
    placeholder?: string,
}
```
Explanation
* `placeholder` is the placeholder string for the textbox
* this component is not derived from `div` but from `form`. Any attributes valid
  in `form` are also valid here.

:::tip

These common StreamedTable components are very helpful in reducing the
complexity of the column definitions you write. They also ensure a common design
language throughout the project. We encourage you to use them as much as
possible.

:::

### Common components for column cells
The selection here is quite limited right now, but shall be extended in order to
keep a common design language. It is also much easier to keep the project
accessible if the total number of unique components is kept low.

* `TableInternalLink` for links within the rucio webui
* `TableExternalLink` for links pointing outside of the rucio webui.

### TableStyling

:::info

As with the rest of the Rucio WebUI, the StreamedTable is styled via [Tailwind
CSS](https://tailwindcss.com/). This means that styles are constructed by
assigning various classes to an element. Tailwind will then compile a minimal
set of CSS.

:::

This optional StreamedTable attribute itself only contains optional members. It
is used to add to or override table styling settings.

```tsx
type TableStyling = Partial<{
    visibility?: Record<string, boolean>
    tableHeadRowStyle?: string
    tableBodyRowStyle?: string
    pageSize?: number
    tableFooterStack?: boolean
}>
```
Explanation:
* `visibility` maps a column's visibility (boolean) to the column id (string).
* `tableHeadRowStyle`: the classes which shall get passed to the `tr` in the
  `thead`.
* `tableBodyRowStyle`: the classes which shall get passed to each `tr` in the
  `tbody`.
* `pageSize` is how many rows are shown per page at maximum.
* `tableFooterStack` will stack table footer components in order to save
  horizontal space.

### Setting column widths

Column widths are set automatically, but they can be styled individually. This
is done by adding a string of class names to the column definition in the
following way:

```tsx
columnHelper.accessor("foo", {
    meta: {
        style: "w-28 md:w-56" // see tailwind documentation for an explanation
    },
    ...
})
```
The Tailwind compiler has been configured to consider these styles as well when
putting together the minimal set of CSS. Internally, the styles defined here are
applied to the header cell. Since the StreamedTable `table` element uses
`table-layout: fixed`, the column widths of the first row will be applied to
each of the following rows as well.

### TableSelecting
This optional table attribute handles selecting rows and the "breakout". The
breakout is an infopanel placed under the table which shows columns for a single
selected row (these columns would otherwise be hidden due to lack of horizontal
space), see the image below for an example.

```tsx
type TableSelecting<T> = { // where T is the type of the table rows
    handleChange: (data: T[]) => void,
    enableRowSelection: boolean,
    enableMultiRowSelection?: boolean,
    breakOut?: {
        breakoutVisibility: boolean,
        keys: Record<string, string>, // column id, displayname
    }
}
```
Explanation:
* `handleChange` is the function which is run each time the selection state
  changes.
* `enableRowSelection`: whether to enable row selection
* `enableMultiRowSelection`: whether to allow row multi-select (only valid if
  row selection is enabled)
* `breakout.breakoutVisibility`: whether to allow showing the breakout when a
  row is selected. Often coupled to the horizontal window size.
* `breakout.keys`: matching column id to the name under which it is to be
  displayed. The column ids determine which columns are going to be added to the
  breakout. Usually, these columns end up being the columns that were hidden due
  to lack of horizontal space.

![Table Breakout](/img/webui/breakout.png)

### NormalTable
The StreamedTable design is part of the design language of the Rucio WebUI, and
we intend for all tables (even if they are non-streaming) to follow this design.
It follows that we created the `NormalTable`, which keeps the same design as the
StreamedTable but without the streaming capabilities.

```tsx
type NormalTableProps<T> = JSX.IntrinsicElements["table"] & {
    tablecolumns: any[] // see StreamedTable
    tablestyling?: TableStyling // see StreamedTable
    tableselecting?: TableSelecting<T> // see StreamedTable
    tabledata: T[]
}
```
Explanation:
* `tabledata`: instead of passing a comdom which wraps the streamed data, we pas
  the data directly in the form of an array.

## StreamedTable in the context of NextJS
The StreamedTable relies on streams, which are rapped in the ComDOM. Until now,
we have taken the existence of a functioning ComDOM as given. In this section,
we will discuss how to properly embed the StreamedTable in the wider context of
the NextJS web application.

:::info

The frontend uses the NextJS app router. [Look at their
docs](https://nextjs.org/docs) to learn more.

:::

#### QueryClientProvider

On a layout-file affecting the route containing the StreamedTable, include the
following code:

```tsx
'use client';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query'

const queryClient = new QueryClient();

export default function RootLayout({children}) {
    return (
        <QueryClientProvider client={queryClient}>
            {children}
        </QueryClientProvider>
    )
}
```
where `children` is a `React.ReactNode` containing the StreamedTable.

#### Route

On the route which returns the page including the StreamedTable, you must
initialise the ComDOM. You can optionally auto-start the ComDOM query or wait
for the user to trigger it manually -- this will not be covered in-depth here,
since this is more closely related to the ComDOM itself than to the
StreamedTable. An example, taken and adapted from the `ListSubscription` route,
would be:

```tsx
'use client';
import { ListSubscription as ListSubscriptionStory } from "@/component-library/Pages/Subscriptions/ListSubscription";
import { useEffect } from "react";
import useComDOM from "@/lib/infrastructure/hooks/useComDOM";
import { SubscriptionRuleStatesViewModel } from "@/lib/infrastructure/data/view-model/subscriptions";

export default function ListSubscription({ params }: { params: { account: string }}) {
    // initialise ComDOM
    const ComDOM = useComDOM<SubscriptionRuleStatesViewModel>(
        "subscription-rule-states-query",
        [],
        false,
        Infinity,
        50,
        true
    )
    // auto-start query, GET data from /api/list-subscription, use param account
    useEffect(() => {
        const runQuery = async () => {
            await ComDOM.start({
                url: new URL("http://localhost:3000/api/list-subscription"),
                method: "GET",
                headers: new Headers({
                    'Content-Type': 'application/json'
                } as HeadersInit),
                params: {
                    "account": params.account,
                }
            })
        }
        runQuery()
    }, [])
    // * return a react component
    // * in this case the StreamedTable is wrapped by a Storybook Page
    // * we pass the ComDOM to this page so that it may be passed further down
    // to the StreamedTable
    return (
        <div>
            <ListSubscriptionStory
                accountname={params.account}
                comdom={ComDOM}
            />
        </div>
    )
}
```
---
id: setting_up_intellij_dev_env
title: Setting up a Rucio Developer environment using IntelliJ Ultimate
---

This tutorial works on Linux, but it was confirmed, in the past, to **not**
work on Windows. Windows support could have improved in later IntelliJ
versions. If you manage to make it work, please tell us and update the
documentation.

## Prerequisites

- An IntelliJ Ultimate licence is required. The `Free` versions doesn't have the
needed features. Some people may qualify for a free licence.
Refer to their website for more information.
I assume it's easy to adapt the following tutorial for `PyCharm Professional`,
but I don't have a licence to test it.

- If not already done, install the official `Python` plugin by following their
[official documentation](https://www.jetbrains.com/help/idea/plugin-overview.html#e6e8b3a2)

- Your local user running IntelliJ must be able to run docker commands
without the need for becoming `root` with sudo/su. Running the following
command and re-logining should be enough:

      sudo gpasswd -a $USER docker

## Open the rucio project in IntelliJ

Open the existing rucio project in intellij. This should be trivial. However,
if you need help with it, refer to the
[official documentation](https://www.jetbrains.com/help/idea/import-project-or-module-wizard.html)

## Add docker-compose remote interpreters

Once the `rucio` project opened in IntelliJ, navigate to
`File/Project Structure` and add a new python SDK as shown in the following
picture:

![Add new Python SDK](/img/intellij/add_python_sdk.png)

In the new window, as shown on the next picture:
- select `Docker Compose` in the left menu.
- in the `Configuration Files` field, navigate to the
`./etc/docker/dev/docker-compose.yaml` within the project
- in the `Service` field select `rucio`

![Remote interpreter for rucio service](/img/intellij/rucio_docker_compose.png)

Repeat the operation, but select `rucioclient` in the `Service` field.

Now you should have two remote interpreters configured. IntelliJ will start
indexing the projects (see progress bar in the bottom right corner). Wait
for indexing to finish.

It's important for the default interpreter to be the one with
Service = `rucioclient`. This should be the case if you added them in the order
mentioned previously.

## Mark the `bin` and `lib` folders as Source folders

Open `File/Project Structure` again and navigate to `Project Settings/Modules`
in the left menu. Mark the `bin` and `lib` folders as `Source` folders. For
that, right click on them and select the folder type. As shown in the following
screenshot:

![Source folders](/img/intellij/source_folders.png)

## Start and initialize the docker-compose env

Follow our [documentation](/operator/setting_up_demo/#using-the-environment-including-storage)
to manually start the docker-compose environment and initialize the database
and storage.

## Debugging a daemon or core function

With the docker-compose env up in running, navigate to a test file. For example,
open the file `lib/tests/test_config.py` and refer to the following video for
an example how to debug a test:

<video width="100%" controls>
  <source src="/documentation/img/intellij/debugging.mp4"/>
</video>

## Debugging the server

The procedure configured until now works very well for developing and testing
daemons and core code. However, it doesn't allow to debug the server code.
For that, one has to run the server separately.

Add a new `Flask server` run configuration in the menu `Run/Edit Configurations`

- Target type: `Module name`
- Target: `rucio.web.rest.flaskapi.v1.main`
- Additional options:
`--cert=/etc/grid-security/hostcert.pem --key=/etc/grid-security/hostkey.pem --host=0.0.0.0 --port=443`
- Environment variables: `LC_ALL=en_US.utf-8`
- Use specified interpreter: select the `rucio` docker-compose remote
  interpreter.

This will allow to debug the code on server side.
---
id: requirements
title: Requirements
sidebar_label: Requirements
---

Rucio relies on several dependencies, some of them being a hard requirement,
others being optional depending on the used Rucio functionality.

To install Rucio, we generally recommend a Kubernetes-based deployment based on
our [helm-charts](https://github.com/rucio/helm-charts). A direct deployment
using Rucio [docker containers](https://hub.docker.com/u/rucio), or Rucio
[pip](https://pypi.org/project/rucio/) packages is also possible.

## Python

Rucio server, daemons and clients are written in Python and thus depend on an
installed Python interpreter.

Rucio release                        | Supported python versions
------------------------------------ | -----------------------------------------------
from [32 LTS](/release-notes/32.0.0) | `>=3.9, <=3.10`

## Database

Database   | Supported version | Note
-----------| ----------------- | ---------
SQLite     | 3+                | For testing only
MySQL      | 8                 | No support for SSH public key authentication
PostgreSQL | 12 and higher     |
Oracle     | 19c               |
---
id: what_is_rucio
title: What is Rucio?
sidebar_position: 2
---

Rucio enables centralized management of large volumes of data backed by many
heterogeneous storage backends.

Data is physically distributed over a large number of storage servers, potentially
each relying on different storage technologies (SSD/Disk/Tape/Object storage) and,
frequently, managed by different teams of system administrators.

Rucio builds on top of this heterogeneous infrastructure and provides an interface
which allows users to interact with the storage backends in a unified way.
The smallest operational unit in Rucio is a file. Rucio enables users to upload,
download, and declaratively manage groups of such files.

Declarative management is the power of Rucio, as it allows the user to define
high-level rules such as "Keep 3 copies, on 2 different continents".
If one copy is lost, it will be automatically re-constructed on a different storage
server to enforce the configured rules.

## History

Rucio was developed as a replacement for the Don Quijote (DQ2) data management
system. Even if DQ2 has demonstrated very large scale data management capabilities,
the **ATLAS Distributed Data Management System** used for **HEP
experiments at CERN** had reached its limits in terms of scalability. The
primary concerns were

- the requirement of a large number of support staff to operate.
- difficulty in interfacing with new technologies

To address these very scaling requirements for HEP experiments, **Rucio** as a
Distributed Data Management System, was developed. Drawing benefits from
advances in Cloud & Big Data computations, it relies on a conceptual data model
to ensure system stability. Dataflow autonomy and automation are the key design
principles guiding the development of Rucio. To reduce the operational overheads
of the support staff, it employs an automation framework and also accounts for
newer use cases & user requirements in high energy physics and beyond.

## What can Rucio do

The capabilities of Rucio are:

- Storage of detector data, simulator data, and user data
- Unified interfacing of heterogeneous network & storage infrastructure
- Support for newer protocols in Storage & Network using plugins
- Data Recovery
- Adaptive Replication
- Quota management

## What Rucio doesn't do

Rucio doesn't automatically create the storage backends. The storage servers must
be created and configured, in advance, with one of the supported access protocols
(webdav/s3/sftp/xrootd/...), then configured in Rucio.
---
id: daemons
title: Rucio Daemons
sidebar_label: Rucio Daemons
---

Rucio relies on several daemons (processes) to perform different logic.
Most of the daemons connect to the DB to read some data, perform some computation,
and then write some data back into the DB.

Usually one daemon will create some work for another daemon and vice-versa.
In Rucio realm, daemons communicate to others by the DB.
The following table represents a high level view of the responsibility of each of the daemons.

## Daemons

Name | Domain|  Purpose |  Details
-----------| ----------------- | -------| ----- |
rucio-abacus-account | Accounting | Account usage | [Details](bin/rucio-abacus-account.md)
rucio-abacus-collection-replica | Accounting |  Updates collection replicas | [Details](bin/rucio-abacus-collection-replica.md)
rucio-abacus-rse | Accounting | Updates RSE counters | [Details](bin/rucio-abacus-rse.md)
rucio-atropos | Replica | End the life of the rules according to the Lifetime Model | [Details](bin/rucio-atropos.md)
rucio-auditor | Replica | Find inconsistencies on storage, for example, dark data discovery | [Details](bin/rucio-auditor.md)
rucio-automatix | Replica | Used for testing: injects random data in RSEs to check liveness | [Details](bin/rucio-automatix.md)
rucio-bb8 | Replica | Rebalance data across RSEs | [Details](bin/rucio-bb8.md)
rucio-cache-client | Replica | Populates information of replicas on volatile storage  | [Details](bin/rucio-cache-client.md)
rucio-cache-consumer | Replica | Adds and deletes cache replicas to the Rucio catalog | [Details](bin/rucio-cache-consumer.md)
rucio-conveyor-finisher | Transfer | Updates Rucio internal state after the file transfer has finished | [Details](bin/rucio-conveyor-finisher.md)
rucio-conveyor-poller | Transfer | Polls updates from the transfer tool to check the transfer state | [Details](bin/rucio-conveyor-poller.md)
rucio-conveyor-preparer | Transfer | Prepares data transfers | [Details](bin/rucio-conveyor-preparer.md)
rucio-conveyor-receiver | Transfer | Sister of poller, instead of polling for updates, it reads transfer tools notifications to check transfer state | [Details](bin/rucio-conveyor-receiver.md)
rucio-conveyor-stager | Transfer | Issues staging (bring online) requests to tape RSEs  | [Details](bin/rucio-conveyor-stager.md)
rucio-conveyor-submitter | Transfer | Submit transfer requests to the transfer tool (prepares the transfer as well if the conveyor-preparer is not enabled) | [Details](bin/rucio-conveyor-submitter.md)
rucio-conveyor-throttler | Transfer | Queues transfer requests inside Rucio, applying limits, ex: only one transfer at a time, etc ... | [Details](bin/rucio-conveyor-throttler.md)
rucio-dark-reaper | Deletion | Deletes quarantined replicas | [Details](bin/rucio-dark-reaper.md)
rucio-dumper | Consistency | Dumps file lists. The rucio-auditor consumes these dumps to discover dark data | [Details](bin/rucio-dumper.md)
rucio-follower | Telemetry | Aggregates events affecting DIDs | [Details](bin/rucio-follower.md)
rucio-hermes | Telemetry | Sends Rucio messages to external services (InfluxDB, OpenSearch, ActiveMQ, ...) | [Details](bin/rucio-hermes.md)
rucio-judge-cleaner | Rule | Cleans expired replication rules | [Details](bin/rucio-judge-cleaner.md)
rucio-judge-evaluator | Rule | Creates and evaluates replication rules based on their state (OK/REPL/STUCK)  | [Details](bin/rucio-judge-evaluator.md)
rucio-judge-injector | Rule | Asynchronously injects replication rules  | [Details](bin/rucio-judge-injector.md)
rucio-judge-repairer | Rule | Repairs stuck replication rules (STATE=STUCK) | [Details](bin/rucio-judge-repairer.md)
rucio-kronos | Telemetry | Consumes Rucio tracing messages, updates access time of replicas and access count of DIDs | [Details](bin/rucio-kronos.md)
rucio-minos | Replica | Reads list of physical file names (PFNs) declared bad and classifies them in: temporary unavailable  and permanently unavailable (to be recovered by the necromancer daemon)  | [Details](bin/rucio-minos.md)
rucio-minos-temporary-expiration | Replica | Moves back TEMPORARY_UNAVAILABLE replicas into AVAILABLE state  | [Details](bin/rucio-minos-temporary-expiration.md)
rucio-necromancer | Deletion | Works on permanently unavailable replicas, it tries to recover the data from other valid replicas if any, else declares the replica as lost  | [Details](bin/rucio-necromancer.md)
rucio-oauth-manager | Auth/Authz | Deletes expired access tokens (in case there is a valid refresh token, expired access tokens will be kept until refresh_token expires as well.) and deletion of expired OAuth session parameters | [Details](bin/rucio-oauth-manager.md)
rucio-reaper | Deletion | Deletes replicas that don't have locks anymore, i.e. they have a tombstone set | [Details](bin/rucio-reaper.md)
rucio-suspicious-replica-recoverer | Replica | Declares suspicious replicas as bad if they are found available on other RSEs, so necromancer will work on them | [Details](bin/rucio-replica-recoverer.md)
rucio-rse-decommissioner | Deletion | Decommissions an RSE. The actions to perform are specified in decommissioning profiles (delete all data, move replicas, etc ...)  | [Details](bin/rucio-rse-decommissioner.md)
rucio-storage-consistency-actions | Consistency | Applies corrective actions as a result of a consistency check on an RSE | [Details](bin/rucio-storage-consistency-actions.md)
rucio-transmogrifier | Rule | Creates replication rules for DIDs matching a subscription | [Details](bin/rucio-transmogrifier.md)
rucio-undertaker | Deletion | Manages expired DIDs, deleting them (does not delete replicas)  | [Details](bin/rucio-undertaker.md)

## FAQ


### Conveyor daemons
It is important to know the following:
* The throttler daemon will need the preparer to work.
* The preparer is a daemon that optimizes transfer requests, while recommended to install, it's not mandatory.
* The submitter is the only daemon needed to submit transfers and can do a subset of what the preparer can do.
* To update the state of requests, the conveyor poller (polls for changes) or conveyor receiver (listens for changes)  are needed to understand the new state.
* The finisher analyzes this new state and updates the state.

### What happens when a rule is stuck?
The judge repairer will analyze why the transfer is stuck and try to unstuck it, eventually resubmitting the request.

### What happens when new data is added to an existing dataset that already has replicas?
The judge evaluator will keep track of new data added to datasets that are already replicated to trigger the necessary transfer requests to ensure all new data is copied to the RSEs.

### What is the purpose of the minos daemons?
An human operator can declare some datasets as temporarily unavailable due to maintenance, outages, etc ...The operator will set an expiration time on the temporary unavailable status. When the expiration time is reached, the minos-temporary-expiration will put the replicas back in available state.

### What is the relationship between auditor, rucio-dumper and dark-reaper?
The dumper will create a dump of all the files in an RSE that will be passed to the auditor. The auditor will check for inconsistencies and mark missing data as dark data (quarantined replicas).
Dark reaper is the one deleting this dark data to free up space from the quarantined replicas table.

### How is data deleted?
When replicas are healthy, the judge-cleaner will set a tombstone on replicas where the lifetime has expired. These replicas are taken by the reaper and they are deleted.
Sometimes, replicas can become unhealthy. A dump is created by the dumper daemon. The auditor checks these dumps and declares replicas as suspicious.

## How is a replica declared bad?
- An operator can declare a replica as bad issuing rucio cli commands.
```
rucio-admin replicas declare-bad [-h] --reason REASON [--inputfile [INPUTFILE]] [--allow-collection] [--lfns [LFNS]] [--scope [SCOPE]] [--rse [RSE]] [listbadfiles ...]
```

These bad replicas are taken by the necromancer daemon and then deleted if they cannot be recovered from other RSEs.
- the suspicious-replica-recoverer is a daemon that will analyze different counters (transfer errors, download errors, etc ...) to mark replicas as being suspicious to have an issue. After hitting
a certain limit (configurable by `--nattempts` flag), the replica is marked as bad and eventually consumed by the necromancer.

## What is the purpose of undertaker?
A dataset is "never" deleted, however, when the dataset is known to be bad, there is no point having it in the catalog. The undertaker daemon takes care to remove these datasets.
An operator will set an expiration date in the past of the DIDs and this daemon will delete the dataset from the DB. If there were any replicas attached, the replicas will be deleted as well.
---
id: main_components
title: Main Components
---

Rucio is based on a distributed system architecture & can be sectioned into four
major layers:

## Clients

The clients layer consists of components such as the command line clients (CLI),
Python clients, and the Javascript-based web user interface and configuration.

## Server

The server layer serves the purpose of authentication & provides a common API
for interaction with clients & other external application, as also the Web UI.

## Core

This layer consists of all the Rucio-level abstractions that are explained at
length in the Concepts section.

## Daemons

The daemons layer takes care of all the asynchronous & continuous workflows in
the background.

A diagrammatic representation of the 4 layers is as shown below

![image](/img/architecture.png)

To learn more about each of these layers in detail, download our [peer reviewed
scientific paper](https://link.springer.com/article/10.1007/s41781-019-0026-3).
---
id: additional_layers_and_resources
title: Additional Layers and Resources
sidebar_position: 4
---

In addition to the four [main layers](started/main_components/main_components.md), we have the storage
resources & transfer tools, as well as the underlying persistent layers. These
are represented in the architecture diagram by the different *queuing systems*,
*transactional* relational databases, & *analytics* storage on non-relational
databases.

- Storage & Transfer Tools layer

The Storage layer is responsible for the various interactions with different
grid middleware tools & storage systems. The transfer tool interface is a
feature of Rucio that enables the daemons to submit, query, and cancel transfers
generically & independently from the actual transfer service being used.

This layer is not a software that resides in a datacenter but is more
representative of the various abstractions in a storage system. It can be
configured dynamically & centrally to suite requirements.

- Persistence Layer

This is the layer responsible for keeping all the data & application states for
all daemons. Also known as the **catalog**, it requires a transactional
database.

The persistence layer supports many different types of transactions relational
database management systems such as SQLite, MySQL, PostgreSQL, or Oracle. Both
upgrading & downgrading of the database schema are supported.
---
title: Rucio Storage Element
---

A Rucio Storage Element (RSE) is the logical abstraction of a storage system for
physical files. It is the smallest unit of storage space addressable within
Rucio. It has a unique identifier and a set of meta attributes describing
properties such as supported protocols (https, srm, s3, \...), host/port
address, quality of service, storage type (disk, tape, \...), physical space
properties (used-, available-, non-pledged space), and geographical zone.

Rucio Storage Elements can be grouped in many logical ways, e.g. the UK RSEs,
the Tier-1 RSEs, or the 'good' RSEs. One can reference groups of RSEs by
metadata attributes or by explicit enumeration of RSEs. See the section about
[RSE Expressions](rse_expressions.md) for more information.

RSE tags are expanded at transfer time to enumerate target sites.  Post-facto
changes to the sites in an RSE tag list will not affect currently replicated
files.

## Rucio Cache RSE

A cache is storage service which keeps additional copies of files to reduce
response time and bandwidth usage. In Rucio, a cache is an RSE, tagged as
volatile. The control of the cache content is usually handled by an external
process or applications (e.g. the Workflow management systems) and not by
Rucio. Thus, as Rucio doesn't control all file movements on these RSEs, the
application populating the cache must register and unregister these file
replicas in Rucio. The information about replica location on volatile RSEs can
have a lifetime. Replicas registered on volatile RSEs are excluded from the
Rucio replica management system (replication rules, quota, replication locks)
described in the section [Replica management](replica_management.md). Explicit
transfer requests can be made to Rucio in order to populate the cache.

## Distances between RSEs

When configuring transfers between RSEs, distances must be defined for that link.
Distances are unidirectional. To allow transfers in both directions, a distance
has to be defined separately in each direction. Refer to the section
[Transfers Overview](operator_transfers/transfers_overview.md) for more details.
---
id: notifications
title: Notifications
sidebar_label: Notifications
---

External applications can require synchronisation on events relative to
data availability and can subscribe to particular events, e.g., dataset
state changes, replication rule state changes, etc. Rucio publishes
messages via the [STOMP](https://stomp.github.io) protocol (to e.g.
[ActiveMQ](https://activemq.apache.org)) when these events happen.
---
title: Rucio account
---

A Rucio account is the unit of assigning privileges in Rucio. It can represent
individual users (such as user1, user2, user3, \...), a group of users (such as
group1, group2, group3, \...) or a centralized production activity such as
service accounts for data generation (datagen) or the workflow management
systems (wfms). A Rucio account is identified by a string.

All interactions with Rucio are always conducted by a Rucio account. A Rucio
user is authenticated by credentials, such as X509 certificates,
username/password, SSH, OIDC, or tokens. Credentials can map to one or more
accounts (N:M mapping). The Rucio authentication system checks if the used
credentials are authorized to use the supplied Rucio account.  The figure below
gives an example of the mapping between credentials and Rucio accounts:

```mermaid
graph LR
    g["i"]

    subgraph TB sgcred[Credentials]
        b["X509 <br> (barisits)"]
        v["X509 <br> (vgaronne)"]
        g["X509 <br> (graemes)"]
    end

    subgraph TB sgaccount[Rucio Accounts]
        account_b[barisits]
        account_v[vgaronne]
        account_p[prod]
        account_h[higgs]
    end

    b --> account_b
    v --> account_v
    v --> account_h
    g --> account_p
    g --> account_h
```
---
id: deletion_overview
title: Deletion Overview
---

Deletion in Rucio can be performed in broadly two ways.

- Rule based deletion:

    Each rule has a corresponding lifetime associated with it (default is None). The attribute `expires_at` of the rule is the time when the lifetime is set plus the lifetime associated with the rule.
    Additionally each rule can be `locked` for additional protection. If a rule is locked the rule is protected from expiration.
    The `Judge-cleaner` daemon is responsible for handling expired rules. If an expired rule is not locked it sets `tombstones` on all replicas not covered by the rule anymore (or any other rule). These replicas then become eligible for deletion.

- DID based deletion:

    Additionally, each DID also has a lifetime associated with it (default is None). The metadata `expires_at` of the DID is calculated similarly to a rule.
    The `Undertaker` daemon is responsible for handling expired DIDs. The daemon checks if none of the associated rules to the DID are locked, if not it removes all rules of the DID (see section above). The DID itself is then removed from the catalog.

**The DID expiration overrules the rule expiration. But the locked rules are protected.**

After the tombstone is set for replicas, the actual data deletion is done by the `Reaper` daemon. The reaper physically deletes the tombstoned replicas from storage.
The deletion service supports two different modes: greedy and non-greedy.

- Greedy

    The reaper daemon gets all the replicas with tombstone in the RSE and immediately deletes all replicas.

- Non-greedy

    The reaper daemon first checks if the free space is needed in the RSE. The needed free space is the difference of minimum free space (attribute set for RSE) and actual free space in RSE. Deletion only occurs once free space is needed.
    Deletions are processed by Least Recently Used (LRU) algorithm, thus oldest accessed (tombstoned) replicas are deleted first.


```mermaid
flowchart TB

    A((Undertaker))-->1[Get expired dids]

    1-->2[Get all rules for did]
    2-->D{Are Rules Locked?}

    D--"yes"-->f([Finished])
    D--"no"--> sub1[Remove Rules]

    subgraph sg[ ]
        sub1[Remove Rules] --> sub2["`Set tombstones
                        on replicas`"]
        sub2 --> sub3[Remove DID]
        sub3 --> sub4{"`Does the DID
                        have children?`"}
        sub4 --yes-->sub5["`Remove Child
                             DID and Replicas`"]
    end

    sub4 --"no"--> f
    sub5--> f
```


```mermaid
    graph TD

    R((Reaper)) --> RSEs[Get all RSEs]
    RSEs --> D1{RSE.availability_delete}
    D1--"False"--> f([Finished])
    D1--"True"--> Greedy_RSE{Greedy RSE?}

    %% Non-greedy RSE Logic
    Greedy_RSE--"no"--> MinFreeSpace{Min free space <= Actual Free Space}
    MinFreeSpace--"no"--> f
    MinFreeSpace--"yes"--> i[List replicas with tombstones] --> id4[Apply LRU algorithm to replicas]
    id4 --> RemoveReplicas[Remove replicas]

    %% Greedy RSE Logic
    Greedy_RSE--"yes" --> j[List replicas with tombstones] --> RemoveReplicas --> f


```

---
title: Permission model
---

Rucio assigns permissions to accounts. Permissions are boolean flags
designating whether an account may perform a certain action (read,
write, delete) on a resource (RSE, account, replica, etc.).

Rucio comes with a generic permission policy including a typical set of
permissions. This policy can be replaced with a more fitting permission
file representing the policies of the community using Rucio.
---
title: Replica management with replication rules
---

Replica management is based on replication rules defined on data identifiers
(files, datasets, containers). A replication rule is owned by an account and
defines the minimum number of replicas to be available on a list of RSEs,
denoted by an [RSE Expression](rse_expressions.md).  Accounts are allowed to set
multiple rules[^1]. Rules may optionally have a limited lifetime and can be
added, removed or modified at any time.

An example listing of replication rules is given below:

- prod: 1x replica @ CERN, no lifetime
- barisits: 1x replica @ US-T2, until 2019-01-01
- vgaronne: 2x replica @ T1, no lifetime

A rule engine validates the rules and creates transfer primitives to fulfil all
rules, e.g. transfer a file from RSE A to RSE B. The rule engine is triggered
when a new rule is defined on an existing data identifier, or when a file is
added to a dataset with existing rules.  The rule engine will only create the
minimum set of necessary transfer primitives to satisfy all rules.

Notifications can be provided for rules and their underlying transfer
requests. All transfer requests are transient.

The deletion service supports two different modes: greedy and non-greedy. Greedy
means that the service tries to immediately delete all replicas which are not
protected by a replication rule. Non-greedy deletion is triggered when storage
policy dictates that space must be freed. The deletion service will look for
replicas on that RSE which can be deleted without violating any replication
rule. The deletion service will use a Least Recently Used (LRU) algorithm to
select replicas for deletion. The deletion service will also immediately delete
all replicas of any file which is declared obsolete.

Some examples of replication rules are listed
[here](replication_rules_examples.md).

## Rule grouping and replica storage
The following two parameters determine the way the files to be replicated
are assigned to suitable RSEs:
- The **rule grouping**: `ALL`, `DATASET`, `NONE`.
- The [DID type](file_dataset_container.md): `FILE`, `DATASET`, `CONTAINER`.

The table below describes the resulting replication logic
depending on the combination of rule grouping (header row) and DID type (left column).


|               	| **ALL**                                               	| **DATASET**                                                                                     	| **NONE**        	|
|---------------	|-------------------------------------------------------	|-------------------------------------------------------------------------------------------------	|-----------------	|
| **FILE**      	| All files must be on the same RSE                     	| N/A                                                                                             	| No restrictions 	|
| **DATASET**   	| All files must be on the same RSE                     	| N/A                                                                                             	| No restrictions 	|
| **CONTAINER** 	| All files must be on the same RSE 	| All files in a dataset must be on the same RSE, but different datasets can be on different RSEs 	| No restrictions 	|


## Footnotes

[^1]: The system may reject rules if these violate other policies, e.g., only
    specific accounts are allowed to request replication rules for tape systems.
---
title: RSE Expressions
---

An RSE Expression allows to select a set of RSEs, for example to
create replication rules. The RSE Expression consists of one or more
terms. A term can be a single RSE name or a condition over the RSE
attributes. The RSE Expression Parser resolves each term to a set of
RSEs. Terms can be connected by operators to form more complex
expressions. For example, users can write RSE expressions to address all
Tier 2 RSEs, all the RSEs in certain cloud, all Tier 2 RSEs not in
certain clouds, etc.

## Simple RSE Expressions

Rucio allows to test RSE Expressions, using the command list-rses.
The most simple RSE Expression is the one containing the name of a
particular RSE.

1. The following expression returns all RSEs:

  ```bash
  jbogadog@lxplus0058:~$ rucio list-rses --rses '*'

  IFIC-LCG2_LOCALGROUPDISK
  IFAE_PRODDISK
  PIC_SCRATCHDISK
  EELA-UNLP_SCRATCHDISK
  CERN-PROD_TZDISK
  BNL-OSG2_MCTAPE
  BNL-OSG2_DATADISK
  IN2P3-CC_MCTAPE
  CERN-PROD_DERIVED
  CERN-PROD_DATADISK
  EELA-UNLP_DATADISK
  UAM-LCG2_SCRATCHDISK
  IFIC-LCG2_DATADISK LIP-COIMBRA_LOCALGROUPDISK
  ```

2. Whereas the next expression only returns a set containing a single
   RSE:

  ```bash
  jbogadog@lxplus0058:~$ rucio list-rses --rses

  EELA-UNLP_SCRATCHDISK
  ELA-UNLP_SCRATCHDISK
  ```

3. Another simple RSE Expression allows to list the set of all the RSEs
   in a particular site:

  ```bash
  jbogadog@lxplus0058:~$ rucio list-rses --rses site=EELA-UNLP

  EELA-UNLP_PRODDISK
  EELA-UNLP_DATADISK
  EELA-UNLP_SCRATCHDISK
  ```

4. Or all the RSEs who's type is `SCRATCHDISK`:

  ```bash
  jbogadog@lxplus0058:~$ rucio list-rses --rses type=SCRATCHDISK

  UNI-SIEGEN-HEP_SCRATCHDISK
  NCG-INGRID-PT_SCRATCHDISK
  EELA-UNLP_SCRATCHDISK
  INFN-T1_SCRATCHDISK
  FMPHI-UNIBA_SCRATCHDISK
  INFN-FRASCATI_SCRATCHDISK
  ```

5. Or all the Spanish sites:

  ```bash
  jbogadog@lxplus0058:~$ rucio list-rses --rses SPAINSITES

  IFIC-LCG2_LOCALGROUPDISK
  IFAE_PRODDISK
  PIC_SCRATCHDISK
  EELA-UNLP_SCRATCHDISK
  EELA-UNLP_DATADISK
  UAM-LCG2_SCRATCHDISK
  IFIC-LCG2_DATADISK
  LIP-COIMBRA_LOCALGROUPDISK
  ```

6. Also numerical comparisons with `<` and `>` are possible:

  ```bash
  jbogadog@lxplus0058:~$ rucio list-rses --rses "freespace>3000"

  CERN-PROD_TZDISK
  BNL-OSG2_MCTAPE
  BNL-OSG2_DATADISK
  IN2P3-CC_MCTAPE
  CERN-PROD_DERIVED
  CERN-PROD_DATADISK
  ```

Note that if the RSE Expression returns an empty set, Rucio returns an error as
an RSE Expression must resolve to __at least one__ RSE. Thus, an error does not
necessarily mean that the syntax of the expression is wrong, it might just
result into an empty list.

In `3.` and `4.`, the RSE Expression refers to an attribute in the RSE that must
be equal to a given value to match the expression. While in `2.` and `5.`, the
expression matches an RSE if the attribute is True. In `6.` a numerical term is
used to resolve all RSEs with more than 3000 TB free space. It is possible to
see the list of attributes for a particular RSE with Rucio:

```bash
jbogadog@lxplus0100:~$ rucio list-rse-attributes EELA-UNLP_SCRATCHDISK

ftstesting: https://fts3-pilot.cern.ch:8446

ALL: True
ESTIER2S: True
physgroup: None
spacetoken: ATLASSCRATCHDISK

fts: https://fts3.cern.ch:8446,https://lcgfts3.gridpp.rl.ac.uk:8446,https://fts.usatlas.bnl.gov:8446

site: EELA-UNLP
EELA-UNLP_SCRATCHDISK: True
datapolicyt0disk: False
cloud: ES
SPAINSITES: True
datapolicyt0taskoutput: False

fts_testing: https://fts3-pilot.cern.ch:8446
tier: 3
datapolicyt0tape: False
type: SCRATCHDISK
istape: False
```

Most of the RSEs share the same set of attributes, and is possible to create RSE
Expressions based on all of them.

## Operators

Operators are used to connect terms in order to get more complex RSE
Expressions/terms. The syntactic functionality of the Rucio RSE Expressions
Parser allows the basic operations defined in mathematical set theory, Union,
Intersection and Complement. Using an operator on two sets of RSEs will
construct a new set based on the given sets.

The symbols A and B in this table stand for a term.

| Operator | Meaning    | Interpretation | Example                    |
|----------|------------|----------------|----------------------------|
| A\|B     | UNION      | A union B      | tier=1\|tier=2             |
| A&B      | INTERSECT  | A intersect B  | tier=1&country=us          |
| A\\B     | COMPLEMENT | A complement B | cloud=ES\\type=SCRATCHDISK |

## Composing RSE Expressions

Using the operators described above, it's possible to create expressions to
select whatever RSE you need to put your data in. Use the following list of
examples to build your own RSE Expressions.

All Tier 2 sites in DE cloud:

```bash
jbogadog@lxplus0100:~$ rucio list-rses --rses 'tier=2&cloud=DE'
PRAGUELCG2_PPSLOCALGROUPDISK
FMPHI-UNIBA_LOCALGROUPDISK
...
UNI-FREIBURG_DATADISK
DESY-HH_PRODDISK
```

Note the use of the single quotes. Single quotes are needed to avoid the shell
interpret the `&`, the `\|` or the `\\` as commands.

All tier 1 but not the ones in country=us:

```bash
jbogadog@lxplus0100:~$ rucio list-rses --rses 'tier=1\country=us'

INFN-T1_MCTAPE
BNL-OSG2_DATATAPE
BNL-OSG2_DDMTEST
NIKHEF-ELPROD_PHYS-SUSY
```

However, take care of the subtle differences. While the first expression exclude
United States' sites, the second doesn't:

```bash
jbogadog@lxplus0100:~$ rucio list-rses --rses 'tier=1\country=us'|wc -l
115

jbogadog@lxplus0100:~$ rucio list-rses --rses 'tier=1\country=US'|wc -l
117
```

The filters are processed from left to right. Is possible to use parenthesis to
force the order of operation. See the following example to get all the
`SCRATCHDISK`s in IT or FR clouds:

```bash
jbogadog@lxplus0100:~$ rucio list-rses --rses \
  'cloud=IT|cloud=FR&type=SCRATCHDISK' | wc -l
30

jbogadog@lxplus0100:~$ rucio list-rses --rses \
  '(cloud=IT|cloud=FR)&type=SCRATCHDISK' | wc -l
30

jbogadog@lxplus0100:~$ rucio list-rses --rses \
  'type=SCRATCHDISK&(cloud=IT|cloud=FR)' | wc -l
30

jbogadog@lxplus0100:~$ rucio list-rses --rses \
  'type=SCRATCHDISK&cloud=IT|cloud=FR' | wc -l
92
```

While the first three operations are equivalent, the last return sites in cloud
`FR` but not only the `SCRATCHDISK`s but the `GROUPDISK`s and `DATADISK`s too,
among other types.
---
title: Replication rule examples
---

Replica management is based on replication rules defined on data identifiers. A
replication rule gets resolved and issues replica locks on the physical
replicas.

A replication rule consists (besides other parameters) of a factor representing
the numbers of replicas wanted and a Rucio Storage Element Expression that
allows to select a set of probable RSEs to store the replicas.

The [__RSE Expression__](rse_expressions.md) gets resolved into a set of RSEs,
which are possible destination RSEs for the number of replicas the user wants to
create.

Is possible to find detailed information and examples about how to write RSE
Expressions [__here__](rse_expressions.md).

## Examples

### I want to have 2 replicas of first_dataset and second_dataset on Tier 1 RSEs

The number 2 *second_dataset* is the number of copies expected. At the end, the
RSE Expression select all the Tier 1 RSEs as possible targets to store the
replicas.:

```bash
username@host:~$ rucio add-rule scope:first_dataset scope:second_dataset 2 'tier=1'
```

To see all the possible targets, **rucio list-rses** command can be used:

```bash
username@host:~$ rucio list-rses --rses 'tier=1'
```

### I want to have 2 replicas on whatever T2 RSEs in the UK but not in Glasgow

```bash
username@host:~$ rucio add-rule scope:first_dataset scope:second_dataset 2 'tier=2&country=uk\site=GLASGOW'
```
---
id: metadata_attributes
title: Metadata attributes
---

Meta-data associated with a dataset/file is represented using
attribute/value pairs. Meta-data attributes are classified into four
categories:

- `System-defined attributes`: size (bytes), checksums (adler32, md5),
   creationtime, modificationtime, status, length (datasets/containers)
- `Physics attributes`: GUID, number of events, project, datatype, run_number,
  stream_name, prod_step, version, campaign, lumiblocknr
- `Workflow management attributes`: storing information like which task
  (task_id) or job (panda_id) produced the file
- `Data management attributes`: necessary for the organisation of data on the
  grid (see Replica Management section)

For datasets, it is possible that the value of a meta-data attribute is
a function of the meta-data of its constituents, e.g. the total size is
the sum of the sizes of the constituents. In this case it is not
possible to assign a value to it.
---
title: Accounting and quota
---

Accounting is the measure of how much resource, e.g. storage, an account
has used as a consequence of its actions. Quota is a policy limit which
the system applies to an account.

Rucio accounts are only accounted for the files they set replication
rules on. The accounting is based on the replicas an account requested,
not on the actual amount of physical replicas in the system. Thus if two
different users set a replication rule for the same file on the same RSE
both users are accounted for this file, although there is only one
physical copy of it.
---
title: Files, Datasets, and Containers
---

As data is physically stored in files, files are also the smallest operational
unit of data in Rucio. Sub-file operations are currently not possible. Rucio
enables users to identify and access on any arbitrary set of files.

Files can be grouped into datasets (a named set of files) and datasets can be
grouped into containers (a named set of datasets or, recursively,
containers). All three types of names refer to data so the term 'data
identifier' (DID) is used to represent any set of files, datasets or container
identifiers. A data identifier is just the name of a single file, dataset or
container.

## Data identifiers and scope

Files, datasets and containers follow an identical naming scheme which is
composed of two strings: the scope and a name. The combination of both is called
a data identifier (DID). Thus for files, the Logical File Name (LFN), a term
commonly used in DataGrid terminology to identify files is equivalent to the DID
in Rucio.

The scope string partitions the namespace into several sub namespaces.  The
primary use case for this is to easily separate centrally created data from
individual user data.

By default, accounts have read access to all scopes and write access only to
their own scope. Privileged accounts have write access to multiple scopes, e.g.,
the Workload Management System is allowed to write into official data scopes.

Files, datasets and containers are uniquely identified over all time.  This
means that a data identifier, once used, can never be reused to refer to
anything else at all, not even if the data it referred to has been deleted from
the system.

## File, dataset, and container status

### File status

The following status attributes are supported for files:

- `availability`: LOST/DELETED/AVAILABLE

A file is LOST if there are no known replicas of the file in Rucio, while at the
same time at least one account requested a replica; a file is DELETED if no
account requested a replica; otherwise the file is AVAILABLE. This is a derived
attribute.

- `suppressed`: True/False

This is a user settable flag. It indicates that the owner of the scope no longer
needs the name to be present in the scope. Files that are suppressed (by
default) do not show up in search and list operations on the scope. Note however
that this flag will be ignored when explicitly listing contents of
datasets/containers.

### Dataset/Container status

The dataset/container status is reflected by a set of attributes:

- `is_open`: True/False

If a dataset/container is open, content can be added to it.  Datasets/containers
are created open and once closed, they cannot be opened again[^1].

- `monotonic`: True/False

If the monotonic attribute is set, content cannot be removed from an open
dataset/container. Datasets/containers are, by default, created
non-monotonic. Once set to monotonic, this cannot be reversed.

- `complete`: True/False

A dataset/container where all files have replicas available is complete.  Any
dataset/container which contains files without replicas is incomplete. This is a
derived attribute.

## Footnotes

[^1]: Datasets from which files have been lost can be repaired when replacement
    files are available, even if Open=False. The replacements need not be binary
    identical to the lost files.
---
id: replica_workflow
title: Typical Replica Workflow
---

This section gives an overview of what happens within Rucio, for a typical
replica workflow. Two workflows are described: When a replica is uploaded to
Rucio via a client and when a replica is created by a site to site transfer due
to the creation of a [replication rule](replica_management.md).

## Replica paths on storage

Rucio has two basic paradigms in deciding the path for a replica on a specific
storage system. **Deterministic** and **Non-deterministic** paths. If we assume
a file whose data identifier is `user.jdoe:test.file.1`, thus the scope is
`user.jdoe` and the name is `test.file.1`. In Rucio a deterministically created
path is a path which can be generated solely knowing the scope and name of a
data identifier (Ignoring the static prefix of the storage endpoint). For a
non-deterministic path additional information describing the file is necessary,
such as meta-data, the dataset the file belongs to, etc.

Rucio supports pluggable algorithms for both deterministic and non-deterministic
algorithms. This section explains a few of them.

## Deterministic algorithm based on hashes

The hash deterministic algorithm is an algorithm commonly used in Rucio. The
advantage of this algorithm is that, due to the characteristics of cryptographic
hash functions, the files are evenly distributed to directories. This can be an
important characteristic for storage systems whose access performance degrades
based on the number of files in a directory.

For a data identifier, e.g. `user.jdoe:test.file.1` a md5-hashsum is calculated
`077c8119053bebb168d125034bff64ac`. The generated path is then based on the
first four characters of the hashsum. e.g. `/user/jdoe/07/7c/test.file.1`.

## Deterministic algorithm based on naming convention

If a specific naming convention is enforced on the filenames, a possible
deterministic algorithm can be based on it.

For the data identifier `user.jdoe:test.file.1` the first part of the filename
(`test`) is extracted and used to generate the path: `/test/user.jdoe/file.1`

## Non-Deterministic algorithm based on parent dataset

If the file is part of a dataset, e.g. `data:dataset1234` the dataset can be
used in the path of the filename. This is useful for e.g. tape storage systems,
to keep the files belonging to the same dataset on the same tape.

For the data identifier `user.jdoe:test.file.1` which is part of the dataset
`data:dataset1234` the generated path is:
`/data/dataset1234/user.jdoe/test.file.1`

## Replica is uploaded with the command line client

This is a typical workflow when a user uploads multiple files, which are part of
a dataset, via the command line client.

1. The dataset `test.dataset` is being registered at the server.  All files, or
   datasets are associated to a [scope](/started/concepts/file_dataset_container.md), if not
   specifically mentioned the client will assume the default scope of the user,
   such as `user.jdoe`. Thus the full data identifier for the dataset is
   `user.jdoe:test.dataset`.

1. The client queries the RSE information from the server. This not only gives a
   list of prioritized write protocols to use but also the information if the
   RSE is a deterministic or non-deterministic one.

1. The file replica is registered as `COPYING` on the RSE.

1. Based on the identified naming algorithm of the RSE and the list of
   prioritized write protocols, the file URL is calculated.  e.g. using the hash
   algorithm from above:
   `https://storageserver.organization.org/VO/data/user/jdoe/07/7c/test.file.1`

1. The file upload is done with the first prioritized protocol. If the upload
   fails, step 4 is repeated with the second prioritized protocol, etc.

1. Once the upload is successfully finished, the replica state is changed to
   `AVAILABLE`.

1. Step 3-6 are repeated (done in parallel) with all other files part of the
   uploaded dataset.

## Replica is created by a replication rule

This is a typical workflow if a file already exists in Rucio but the user wants
to replicate it to a different RSE.

1. The user creates a replication rule for the dataset `user.jdoe:test.dataset`
   at the server.

1. The Rucio server creates internal requests for each single file in the
   dataset and puts them in a queue to be read by the data transfer service.

1. The data transfer submitter picks these requests up and queries the
   destination RSE information for each file.

1. Based on the identified naming algorithm of the destination RSE it creates
   the destination URLs and creates the file replicas in `COPYING` state.

1. The transfer service then submits the transfer job to the connected transfer
   tool (e.g. FTS)

1. Once the transfers are finished the transfer tool notifies Rucio about the
   transfer success and the transfer services mark the replicas as `AVAILABLE`.
---
title: Subscriptions
---

Rucio Subscriptions exist for the purpose of making data placement
decisions before the actual data has been created. Subscriptions
generate rules for new datasets based on matching metadata at
registration time. Subscriptions are owned by an account and can only
generate rules for that account. Policies may have a lifetime, after
which they will expire.

An example of a subscription is given below:

| Attribute | Value                                               |
|-----------|-----------------------------------------------------|
| Owner     | tzero                                               |
| match     | project=data11 7TeV, dataType=RAW, stream=physics\* |
| rule      | 1\@CERNTAPE, 1\@T1TAPE                              |
| lifetime  | 2012-01-01 00:00                                    |
---
id: before_you_get_started
title: Before you get started
sidebar_position: 1
---

A great starting point for those absolutely new to Rucio, this section aims to
helps you understand the basics of our Data Management system. The motivations
behind this project, what we are looking to achieve, & what we have done so far
are some of the points covered in this section.

- [What is Rucio?](what_is_rucio.md)
- [Main Components](main_components/main_components.md)
- [Additional layers and Resources](additional_layers_and_resources.md)
---
id: releasepolicy
title: Release Policy
sidebar_label: Release Policy
sidebar_position: 8
---

Rucio follows a release policy, based on [semantic versioning](https://semver.org),
with **major** (named) releases. Approximately
every 4 months we produce a major release with a version number like **x.0.0**
(with x > 0). A major release marks the start of a release line. This release
line is maintained with minor/patch releases published every two weeks,
containing bug fixes or minor enhancements,
with version numbers like **35.y.z** (with y &ge; 0, z &ge; 0). Versions within
one release line are always backwards compatible, thus they do not include
database schema changes, API modifications, or other backward-compatibility
breaking changes.

Previous to the 32 release line, Rucio used a different versioning theme.

## Support period

A release line is only maintained with patch releases until the start of the
next release line, thus approximately 4 months. Typically once a year we will
designate a release line a **Long-term Support** (LTS) release line. This
release line will be supported with **security** and **critical** patches for
approximately two years. It is foreseen to have an overlap of at least 12 months
between two LTS release lines, to give communities a comfortable time window to
deploy the new LTS release.

## Client &harr; Server compatibility

For differences between the versions of a Rucio client and a Rucio server we guarantee
compatibility between a server and an older client up until the second LTS release line
preceding the server.

For example, a 37 Rucio server guarantees client support until the second LTS release line
preceding its version, thus the 32 LTS release line. Therefore it guarantees support
of clients of the 32, 33, 34, 35, 36 and 37 release lines.

A 33 Rucio server guarantees client support of the 1.29, 1.30, 1.31, 32, and 33 release
lines, while a 32 server guarantees support of the 1.26, 1.27, 1.28, 1.29, 1.30, 1.31, and
32 release lines.

Older clients will most likely work without problems as well; however, the release policy does not
guarantee it.

Please be aware that we can not guarantee compatibility of a newer client with an older
server.

| Version         | Code name                                 | Release date   | Supported until      |
| --------------- | ----------------------------------------- | -------------- | -------------------- |
| 39              | Grand Theft Donkey                        | _2025-11_      | _2026-03_            |
| 38 LTS          | Donkirk                                   | _2025-07_      | _at least 2027-07_   |
| **37**          | **Dungeons & Donkeys**                    | **2025-04**    | _2025-07_            |
| 36              | Donkey Unchained                          | 2024-12        | 2025-04              |
| **35 LTS**      | **Donkey and the Data Factory**           | **2024-07**    | _at least 2026-07_   |
| 34              | Donkey Potter and the Data Cache          | 2024-03        | 2024-07              |
| 33              | Eternal Sunshine of the Donkey's Mind     | 2023-12        | 2024-03              |
| **32 LTS**      | **The Good, The Bad and the Donkey**      | **2023-08**    | **2025-08**          |
| 1.31            | Donkeys of the Caribbean                  | 2023-03        | 2023-07              |
| 1.30            | The Donkeynator                           | 2022-11        | 2023-03              |
| 1.29 LTS        | Into the Donkeyverse                      | 2022-07        | 2024-08              |
| 1.28            | Teenage Mutant Ninja Donkeys              | 2022-03        | 2022-07              |
| 1.27            | Batdonkey v Superdonkey                   | 2021-11        | 2022-03              |
| 1.26 LTS        | Donkey League of La Mancha                | 2021-07        | 2023-07              |
| 1.25            | Rat-Donkey                                | 2021-02        | 2021-07              |
| 1.24            | Aquadonkey                                | 2020-11        | 2021-02              |
| 1.23 LTS        | The incredible Donkey                     | 2020-07        | 2022-07              |
| 1.22            | Green Donkey                              | 2020-02        | 2020-06              |
| 1.21            | Donkeys of the Galaxy                     | 2019-11        | 2020-02              |
| 1.20 LTS        | Wonder Donkey                             | 2019-06        | 2021-07              |
| 1.19            | Fantastic Donkeys                         | 2019-02        | 2019-06              |
| 1.18            | Invisible Donkey                          | 2018-09        | 2019-02              |
| 1.17            | Donkey Surfer                             | 2018-06        | 2018-09              |
| 1.16            | Doctor Donkey                             | 2018-04        | 2018-06              |
| 1.15            | Daredonkey                                | 2018-02        | 2018-04              |
| 1.14            | Professor D                               | 2017-11        | 2018-02              |
| 1.13            | Donkerine                                 | 2017-09        | 2017-11              |
| 1.12            | Captain Donkey                            | 2017-07        | 2017-09              |
| 1.11            | Batdonkey                                 | 2017-05        | 2017-07              |
| 1.10            | Irondonkey                                | 2017-02        | 2017-05              |
| 1.9             | Superdonkey                               | 2016-10        | 2017-02              |
| 1.8             | Spiderdonkey                              | 2016-09        | 2016-10              |
| 1.7             | Donkey One                                | 2016-08        | 2016-09              |
| 1.6             | The Donkey awakens                        | 2016-05        | 2016-08              |
| 1.5             | Return of the Donkey                      | 2016-04        | 2016-05              |
| 1.4             | The Donkey strikes back                   | 2016-02        | 2016-04              |
| 1.3             |                                           | 2016-01        | 2016-02              |
| 1.2             |                                           | 2015-10        | 2016-01              |
| 1.1             |                                           | 2015-08        | 2016-10              |
| 1.0             |                                           | 2015-07        | 2015-08              |
| (0.3)           |                                           | 2015-03        | 2015-07              |
| (0.2)           |                                           | 2014-10        | 2015-03              |
| (0.1.7)         |                                           | 2014-01        | 2014-10              |

## Secondary Rucio Software Policy

### WebUI

The [Rucio WebUI](
https://rucio.github.io/documentation/developer/webui/webui_frontend) is an
initiative to modernize the Rucio user interface by leveraging the latest web
technologies, built with Next.js, TailwindCSS, and React.js.

The versioning of the Rucio WebUI is closely aligned with the Rucio Server's
release versioning. Both follow semantic versioning principles, ensuring
consistency and compatibility.

- **Major Version**: The major version number of the WebUI matches that of the
  Rucio server it is compatible with. For example, Rucio Server version
  **37.x.x** is compatible with Rucio WebUI version **37.x.x**.
- **Minor and Patch Versions**: These versions follow semantic versioning:
   - **Minor Version**: Incremented for new features and improvements that are
     backward-compatible.
   - **Patch Version**: Incremented for backward-compatible bug fixes.

### JupyterLab extension

The Rucio Jupyterlab extension follows it's own release policy described below.

The plugin's main dependencies are the Rucio REST API, the JupyterLab environment
and JavaScript (Node.js and React.js). Up to date, the extension has been
tested with several combinations of Rucio major versions (v1.30 onwards) and
JupyterLab (v&GreaterEqual;3) environments, with the latter indicating the
extension version to install (which follows the semantic versioning schema).

- For `JupyterLab v4.x`, use v&GreaterEqual;1.0.0.
- For `JupyterLab v3.x` use the latest supported version (v0.10.0).

Please refer to the Rucio JupyterLab extension [project](
https://github.com/rucio/jupyterlab-extension) for more details on
installation, requirements and release policy.
---
id: rucio_advisory_board
title: Rucio Advisory Board
---

## Mandate and responsibilities of the board

The primary function of the Rucio advisory board (RAB) is to provide expertise from
representatives of Rucio Science Communities (RSC) and to advise the Rucio project
leader. Long-term priorities and plans of communities should be discussed by the RAB
in order to advise on the alignment of Rucio project objectives and plans. This should
also lead to the identification of common objectives to form common development
efforts. The resource and person-power situation within the Rucio project, the
discussion and identification of funding streams and collaboration on funded projects
are also within the scope of the RAB. The Rucio project leader communicates the advice
given by the advisory board to the development team.

The board provides non-binding strategic advice and is informal in nature.

## Composition

The RAB is composed of a group of representatives of their respective RSC. The goal is
to complement the expertise of the development team, and thus representatives should
have knowledge about the long-term computing strategy of their community and ideally
have responsibility for their project programme and budget.

Membership in the RAB is by invitation of the Rucio project leader. However,
suggestions for expanding the board should ideally come from the RAB itself.
Representatives serve a 2-year, renewable, term. Invitations to join the board are
made to the legal representative of the RSC, typically the spokesperson or coordinator
charged with the computing portfolio of the community. The RSC should in return
nominate one representative from their community to join the board. Nominations are
discussed with the Rucio project leader who ultimately decides on the acceptance.

The Rucio project leader is an ex-officio member of the board and chairs the
board meetings.

### Composition of the Rucio Advisory Board

| Community                           | Representative           | Term                 |
| ----------------------------------- | ------------------------ | -------------------- |
| [ATLAS](https://atlas.cern)         | David South, DESY        | 2023-Apr to 2027-Mar |
| [Belle II](https://www.belle2.org)  | Cedric Serfon, BNL       | 2023-Jun to 2027-May |
| [CMS](https://cms.cern)             | Eric Vaandering, FNAL    | 2025-Feb to 2027-Jan |
| [CTAO](https://www.ctao.org)        | Mieke Bouwhuis, CTAO     | 2025-May to 2027-May |
| [DUNE](https://www.dunescience.org) | Mike Kirby, BNL          | 2023-Feb to 2027-Jan |
| [ESCAPE](https://projectescape.eu)  | Giovanni Guerrieri, CERN | 2025-Feb to 2027-Jan |
| [SKAO](https://www.skao.int/)       | Rosie Bolton, SKAO       | 2024-Jun to 2026-May |

### Former Rucio Advisory Board representatives

| Community                           | Representative        | Term                 |
| ----------------------------------- | --------------------- | -------------------- |
| [ATLAS](https://atlas.cern)         | David Cameron, U Oslo | 2023-Feb to 2023-Apr |
| [Belle II](https://www.belle2.org)  | Paul Laycock, BNL     | 2023-Feb to 2023-Jun |
| [CMS](https://cms.cern)             | Katy Ellis, STFC      | 2023-Feb to 2025-Jan |
| [ESCAPE](https://projectescape.eu)  | Xavier Espinal, CERN  | 2023-Feb to 2025-Jan |


### Eligibility criteria

There are no strict eligibility criteria for an RSC to be viable for having a
representative on the RAB. However the goal of the RAB is to have all science
communities, who utilize Rucio in production and who are significantly involved in the
development of the system, represented on the board. Such significance involves, but
is not limited to, long-term contributions of source-code, leadership roles as Rucio
component-leads, participation in hackathons and Rucio workshops, community support on
the Mattermost channel, etc.

## Organisation

The RAB meets as needed, nominally twice a year. Meetings generally take place
at CERN, with remote participation possibilities. The RAB members agree to
select a secretary for each meeting who will be in charge of keeping minutes.
Minutes are kept internal to the RAB.

Should a representative be exceptionally impeded to join a board meeting, they can
nominate a replacement representative, from their RSC, to join the meeting. The
replacement must be approved by the Rucio project leader.

Guests can be invited to parts, or the entirety, of a board meeting. Invitations are
issued by the Rucio project leader and the RAB members must be informed, in advance,
of the attendance of guests.

## Mandate versions

| Version                                                                         | Creation    |
| ------------------------------------------------------------------------------- | ----------- |
| [v1.2](https://rucio.cern.ch/documentation/files/Rucio_Advisory_Board_v1.2.pdf) | Feb 5, 2025 |
| [v1.1](https://rucio.cern.ch/documentation/files/Rucio_Advisory_Board_v1.1.pdf) | May 8, 2024 |
| [v1.0](https://rucio.cern.ch/documentation/files/Rucio_Advisory_Board_v1.0.pdf) | Nov 7, 2022 |
---
id: about_our_contributors
title: About Our Contributors
---

Below is a list of contributors who have contributed to the
source code & their employers.

Should you wish to contribute to the Rucio source code or the documentation,
please ensure you go through the guidelines listed in the [__contribution guide
for developers__](contributing) and look into the [documentation GitHub
repository](https://github.com/rucio/documentation) before making a submission.

# Individual contributors to the source code

---
- Mario Lassnig [mario.lassnig@cern.ch](mailto:mario.lassnig@cern.ch), 2012-2021
- Vincent Garonne [vgaronne@gmail.com](mailto:vgaronne@gmail.com), 2012-2019
- Angelos Molfetas [Angelos.Molfetas@cern.c](mailto:Angelos.Molfetas@cern.ch), 2012
- Martin Barisits [martin.barisits@cern.ch](mailto:martin.barisits@cern.ch), 2012-2021
- Thomas Beermann [thomas.beermann@cern.ch](mailto:thomas.beermann@cern.ch), 2012-2021
- Ralph Vigne [ralph.vigne@cern.ch](mailto:ralph.vigne@cern.ch), 2012-2016
- Graeme Stewart [graeme.andrew.stewart@cern.ch](mailto:graeme.andrew.stewart@cern.ch), 2012
- Yun-Pin Sun [winter0128@gmail.com](mailto:winter0128@gmail.com), 2012-2013
- Cedric Serfon [cedric.serfon@cern.ch](mailto:cedric.serfon@cern.ch), 2012-2021
- Luis Rodrigues [lfrodrigues@gmail.com](mailto:lfrodrigues@gmail.com), 2013
- WeiJen Chang [e4523744@gmail.com](mailto:e4523744@gmail.com), 2013-2014
- Gancho Dimitrov [gancho.dimitrov@cern.ch](mailto:gancho.dimitrov@cern.ch), 2013
- Wen Guan [wguan.icedew@gmail.co](mailto:wguan.icedew@gmail.com), 2014-2017
- David Cameron [d.g.cameron@gmail.com](mailto:d.g.cameron@gmail.com), 2014-2016
- Tomáš Kouba [tomas.kouba@cern.ch](mailto:tomas.kouba@cern.ch), 2014-2015
- Cheng-Hsi Chao [cheng-hsi.chao@cern.ch](mailto:cheng-hsi.chao@cern.ch), 2014
- Evangelia Liotiri [evangelia.liotiri@cern.ch](mailto:evangelia.liotiri@cern.ch), 2014-2015
- Joaquín Bogado [jbogado@linti.unlp.edu.ar](mailto:jbogado@linti.unlp.edu.ar), 2014-2018
- Fernando López [fernando.e.lopez@gmail.com](mailto:fernando.e.lopez@gmail.com), 2015-2016
- Sylvain Blunier [sylvain.blunier@cern.ch](mailto:sylvain.blunier@cern.ch), 2016
- Tomas Javurek [tomas.javurek@cern.ch](mailto:tomas.javurek@cern.ch), 2016-2020
- Brian Bockelman [bbockelm@cse.unl.edu](mailto:bbockelm@cse.unl.edu), 2016-2018
- Tobias Wegner [twegner@cern.ch](mailto:twegner@cern.ch), 2017-2020
- Frank Berghaus [frank.berghaus@cern.ch)](mailto:frank.berghaus@cern.ch), 2017
- Vitjan Zavrtanik [vitjan.zavrtanik@cern.ch](mailto:vitjan.zavrtanik@cern.ch), 2017
- Stefan Prenner [stefan.prenner@cern.ch](mailto:stefan.prenner@cern.ch), 2017-2018
- Nicolo Magini [Nicolo.Magini@cern.ch](mailto:Nicolo.Magini@cern.ch), 2017-2018
- Oliver Freyermuth [o.freyermuth@googlemail.co](mailto:o.freyermuth@googlemail.com), 2017
- Eric Vaandering [ericvaandering@gmail.co](mailto:ericvaandering@gmail.com), 2018
- Dimitrios Christidis [dimitrios.christidis@cern.ch](mailto:dimitrios.christidis@cern.ch), 2018-2021
- Igor Mandrichenko [ivm@fnal.gov](mailto:ivm@fnal.gov), 2018
- Shreyansh Khajanchi [shreyansh_k@live.com](mailto:shreyansh_k@live.com), 2018
- Robert Illingworth [illingwo@fnal.gov](mailto:illingwo@fnal.gov), 2018
- Hannes Hansen [hannes.jakob.hansen@cern.c](mailto:hannes.jakob.hansen@cern.ch), 2018-2019
- James Perry [j.perry@epcc.ed.ac.uk](mailto:j.perry@epcc.ed.ac.uk), 2019-2021
- Vivek Nigam [viveknigam.nigam3@gmail.com](mailto:viveknigam.nigam3@gmail.com), 2019 - 2020
- Kaustubh Hiware [hiwarekaustubh@gmail.co](mailto:hiwarekaustubh@gmail.com), 2019
- Florido Paganelli [florido.paganelli@hep.lu.se](mailto:florido.paganelli@hep.lu.se), 2019
- Boris Bauermeister [Boris.Bauermeister@gmail.com](mailto:Boris.Bauermeister@gmail.com) 2019
- Ruturaj Gujar [ruturaj.gujar23@gmail.com](mailto:ruturaj.gujar23@gmail.com) 2019
- Andrew Lister [andrew.lister@stfc.ac.uk](mailto:andrew.lister@stfc.ac.uk), 2019
- Aristeidis Fkiaras [aristeidis.fkiaras@cern.ch](mailto:aristeidis.fkiaras@cern.ch), 2019
- Benedikt Ziemons [benedikt.ziemons@cern.ch](mailto:benedikt.ziemons@cern.ch), 2020-2021
- Muhammad Aditya Hilmy [mhilmy@hey.com](mailto:mhilmy@hey.com), 2020
- Eli Chadwick [eli.chadwick@stfc.ac.uk](mailto:eli.chadwick@stfc.ac.uk), 2020
- Patrick Austin [:patrick.austin@stfc.ac.uk](mailto:patrick.austin@stfc.ac.uk), 2020
- Rob Barnsley [R.Barnsley@skatelescope.org](mailto:R.Barnsley@skatelescope.org), 2020
- Alan Malta Rodrigues [alan.malta@cern.ch](mailto:alan.malta@cern.ch), 2020
- Rizart Dona [rizart.dona@gmail.com](mailto:rizart.dona@gmail.com), 2021-2022
- Aksel Lunde Aase [aksel.lunde.aase@gmail.co](mailto:aksel.lunde.aase@gmail.com), 2022
- Anton Schwarz [anton.schwarz@cern.ch](mailto:anton.schwarz@cern.ch), 2022
- Johannes Lange [johannes.lange@uni-hamburg.de](mailto:johannes.lange@uni-hamburg.de), 2022-2023
- Domenic Gosein [domenic.gosein@cern.ch](mailto:domenic.gosein@cern.ch), 2023
- Eraldo Junior [esilvaju@cern.ch](mailto:esilvaju@cern.ch), 2021-2024
- Dimitris Xenakis [d.xenakis@ieee.org](mailto:d.xenakis@ieee.org), 2024
- Fabio Luchetti [fabio.luchetti@cern.ch](mailto:fabio.luchetti@cern.ch), 2021
- Giovanni Guerrieri [giovanni.guerrieri@cern.ch](mailto:giovanni.guerrieri@cern.ch), 2024

# Organisations employing contributors

---

- European Organisation for Nuclear Research (Switzerland)
- University of Oslo (Norway)
- University of Wisconsin Madison (USA)
- National University of La Plata (Argentina)
- Albert Ludwigs Universität Freiburg (Germany)
- University of Nebraska Lincoln (USA)
- Bergische Universität Wuppertal (Germany)
- University of Victoria (Canada)
- INFN e Universita Genova (Italy)
- University of Bonn (Germany)
- Fermi National Accelerator Laboratory (USA)
- Leopold-Franzens Universität Innsbruck (Austria)
- Academia Sinica (Taiwan)
- University of Edinburgh (UK)
- Birla Institute of Technology, Mesra (India)
- Indian Institute of Technology, Kharagpur (India)
- Stockholm University, Stockholm (Sweden)
- Dwarkadas J. Sanghvi College of Engineering (India)
- Science and Technology Facilities Council (UK)
- Brookhaven National Laboratory (USA)
- Institut Teknologi Bandung (Indonesia)
- Universität Hamburg (Germany)
- Brazilian Center for Research in Physics (Brazil)
---
id: component_leads
title: Component development leads
sidebar_label: Component leads
---

The component lead is responsible for the planning and development of patches
and features for the respective component. This does not mean that the component
lead is the only person developing the component, but he/she should direct the
efforts and be the point of contact in case of problems.

**Rucio Project Leader**: [Martin Barisits](https://github.com/bari12)

|Component | Lead (+Deputies) | Description|
|--------- | ---------------- | -----------|
|[Authentication & Authorisation](https://github.com/rucio/rucio/issues?q=label%3A%22Authentication+%26+Authorisation%22+is%3Aissue+is%3Aopen) | [Dimitrios Christidis](https://github.com/dchristidis), [Mario Lassnig](https://github.com/mlassnig) | Client-server authentication, Rucio tokens, auth methods (kerberos, ssh, userpass, OIDC, ...)|
|[Clients](https://github.com/rucio/rucio/issues?q=label%3A%22Clients%22+is%3Aissue+is%3Aopen) | [Maggie Voetberg](https://github.com/voetberg), [Mario Lassnig](https://github.com/mlassnig) | General Python Clients and Command Line interface related development, rucio download, rucio upload; Specific component related client functionality is part of the other components|
|[Consistency checks](https://github.com/rucio/rucio/issues?q=label%3A%22Consistency+checks%22+is%3Aissue+is%3Aopen) | [Fabio Luchetti](https://github.com/faluchet), [Guilherme Lima](https://github.com/mrguilima)| Dark data detection daemon: Auditor|
|[Core & Internals](https://github.com/rucio/rucio/issues?q=label%3A%22Core+%26+Internals%22+is%3Aissue+is%3Aopen) | [Martin Barisits](https://github.com/bari12), [Mario Lassnig](https://github.com/mlassnig) | Core functionality not specifically part of other components|
|[Database](https://github.com/rucio/rucio/issues?q=label%3A%22Database%22+is%3Aissue+is%3Aopen) | [Dimitrios Xenakis](https://github.com/Geogouz), [Mario Lassnig](https://github.com/mlassnig), [Martin Barisits](https://github.com/bari12) | Database (Compatibility and optimisation) and SQLAlchemy framework specific issues|
|[Dataset deletion](https://github.com/rucio/rucio/issues?q=label%3A%22Dataset+deletion%22+is%3Aissue+is%3Aopen) | [Alexander Richards](https://github.com/alexanderrichards), [Martin Barisits](https://github.com/bari12) | Dataset deletion daemon: Undertaker|
|[Deletion](https://github.com/rucio/rucio/issues?q=label%3A%22Deletion%22+is%3Aissue+is%3Aopen) | [Hugo Gonzalez Labrador](https://github.com/labkode), [Cedric Serfon](https://github.com/cserf) | File deletion daemon: Reaper|
|[DIRAC](https://github.com/rucio/rucio/issues?q=label%3A%22Dirac%22+is%3Aissue+is%3Aopen) | [Cedric Serfon](https://github.com/cserf) | Rucio integration with [Dirac](https://github.com/DIRACGrid/)|
|[Docker & Kubernetes](https://github.com/rucio/rucio/issues?q=is%3Aopen+is%3Aissue+label%3A%22Docker+%26+Kubernetes%22) | [Riccardo Di Maio](https://github.com/rdimaio), [Mario Lassnig](https://github.com/mlassnig), [Eric Vaandering](https://github.com/ericvaandering), [Hugo Gonzalez Labrador](https://github.com/labkode) | Docker & Kubernetes deployment|
|[Documentation](https://github.com/rucio/rucio/issues?q=label%3A%22Documentation%22+is%3Aissue+is%3Aopen) | [Martin Barisits](https://github.com/bari12), [Maggie Voetberg](https://github.com/voetberg) | General documentation|
|[Jupyterlab extension](https://github.com/rucio/jupyterlab-extension/issues) | [Francesc Torradeflot](https://github.com/ftorradeflot), [Enrique Garcia Garcia](https://github.com/garciagenrique) | Rucio Jupyterlab extension|
|[Life time model](https://github.com/rucio/rucio/issues?q=label%3A%22Life+time+model%22+is%3Aissue+is%3Aopen) | [Dimitrios Christidis](https://github.com/dchristidis) | Life time model processing and exceptions|
|[Messaging](https://github.com/rucio/rucio/issues?q=label%3A%22Messaging%22+is%3Aissue+is%3Aopen) | [Alexander Richards](https://github.com/alexanderrichards), [Mario Lassnig](https://github.com/mlassnig) | Messaging daemon: Hermes, Creation of eMails and ActiveMQ messages|
|[Metadata](https://github.com/rucio/rucio/issues?q=label%3A%22Metadata%22+is%3Aissue+is%3Aopen) | [Rob Barnsley](https://github.com/robbarnsley), [Dimitrios Xenakis](https://github.com/Geogouz) | Metadata workflows|
|[Monitoring & Traces](https://github.com/rucio/rucio/issues?q=label%3A%22Monitoring+%26+Traces%22+is%3Aissue+is%3Aopen) | [Mayank Sharma](https://github.com/maany), [Mario Lassnig](https://github.com/mlassnig) | All things internal monitoring and traces|
|[Multi VO](https://github.com/rucio/rucio/issues?q=is%3Aopen+is%3Aissue+label%3A%22Multi+VO%22) | [Tim Noble](https://github.com/Thysk) |  Issues and developments related to Rucio Multi VO mode|
|[Policies](https://github.com/rucio/rucio/issues?q=is%3Aopen+is%3Aissue+label%3APolicies) | [James Perry](https://github.com/jamesp-epcc) |  All things related to separate settings/config/policies into community specific things|
|[Probes & Alarms](https://github.com/rucio/rucio/issues?q=label%3A%22Probes+%26+Alarms%22+is%3Aissue+is%3Aopen) | [Dimitrios Christidis](https://github.com/dchristidis), [Eric Vaandering](https://github.com/ericvaandering) | Probes and alarms for Nagios|
|[Protocols](https://github.com/rucio/rucio/issues?q=is%3Aopen+is%3Aissue+label%3AProtocols) | [Maggie Voetberg](https://github.com/voetberg), [Mario Lassnig](https://github.com/mlassnig) |  Protocols (Upload, Download, Deletion)|
|[Rebalancing](https://github.com/rucio/rucio/issues?q=label%3A%22Rebalancing%22+is%3Aissue+is%3Aopen) | [Cedric Serfon](https://github.com/cserf) | Data rebalancing daemon: BB8|
|[Recovery](https://github.com/rucio/rucio/issues?q=label%3A%22Recovery%22+is%3Aissue+is%3Aopen) | [Cedric Serfon](https://github.com/cserf) | Data recovery daemon: Necromancer and suspicious replica recovery |
|[Release management](https://github.com/rucio/rucio/issues?q=label%3A%22Release+management%22+is%3Aissue+is%3Aopen) | [Martin Barisits](https://github.com/bari12), [Dimitrios Christidis](https://github.com/dchristidis) | Packaging of new versions, package configuration|
|[Replicas](https://github.com/rucio/rucio/issues?q=is%3Aopen+is%3Aissue+label%3AReplicas) | [Riccardo Di Maio](https://github.com/rdimaio), [Alexander Richards](https://github.com/alexanderrichards) | Replicas related workflows (list_replicas, add_replica, ...)|
|[REST & API](https://github.com/rucio/rucio/issues?q=is%3Aopen+is%3Aissue+label%3A%22REST+%26+API%22) | [Maggie Voetberg](https://github.com/voetberg), [Martin Barisits](https://github.com/bari12) | Web-Framework (Web.py & Flask)|
|[Rules](https://github.com/rucio/rucio/issues?q=label%3A%22Rules%22+is%3Aissue+is%3Aopen) | [Riccardo Di Maio](https://github.com/rdimaio), [Martin Barisits](https://github.com/bari12) | Replication rules and rule daemons: Judge|
|[Subscriptions](https://github.com/rucio/rucio/issues?q=label%3A%22Subscriptions%22+is%3Aissue+is%3Aopen) | [Cedric Serfon](https://github.com/cserf) | Subscription daemon: Transmogrifier|
|[Testing](https://github.com/rucio/rucio/issues?q=label%3A%22Testing%22+is%3Aissue+is%3Aopen) | [Mayank Sharma](https://github.com/maany), [Riccardo Di Maio](https://github.com/rdimaio) | Regression and Unit tests, automatic tests of submissions|
|[Transfers](https://github.com/rucio/rucio/issues?q=label%3A%22Transfers%22+is%3Aissue+is%3Aopen) | [Riccardo Di Maio](https://github.com/rdimaio), [Hugo Gonzalez Labrador](https://github.com/labkode) | Transfer daemons: conveyor and functional transfer tests |
|[WebUI](https://github.com/rucio/rucio/issues?q=label%3A%22WebUI%22+is%3Aissue+is%3Aopen) | [Mayank Sharma](https://github.com/maany), [Eraldo Silva Junior](https://github.com/esilvaju) | Web user interface: Rucio Web UI|
---
id: project_organisation
title: Project Organisation
---

Rucio is organised as a community-driven, open-source, project.
An open development team, comprised of technical experts rooted in the scientific
community, drives the development based on expertise, technical best-practices and
input from their respective communities. This openness is core to the identity and
the success of the project.

To organise the daily development work as well as the long-term strategic objectives
of the software, the project consists of several entities described on this page.

![Project organisation](/img/project_organisation.svg)

A list of members of the current "Core Team" can be seen [here](https://rucio.cern.ch/team.html).

## Project leader

The project leader has the overall responsibility of steering the project. This
includes coordinating, planning, and assessing the development activity of the
Rucio developers.

## Component leads

The [component leads](component_leads.md) take formal responsibility in planning
and developing contributions for their respective components. They are the
core experts and the point-of-contact in case of issues as well as to guide new
developments within their components.

## Rucio Advisory Board

The primary function of the Rucio advisory board (RAB) is to provide expertise from
representatives of Rucio communities and to advise the Rucio project leader.
Long-term priorities and plans of communities should be discussed by the RAB in order
to advise on the alignment of Rucio project objectives and plans. This should also
lead to the identification of common objectives to form common development efforts.
The resource and person-power situation within the Rucio project, the discussion and
identification of funding streams and collaboration on funded projects are also
within the scope of the RAB. The Rucio project lead communicates the advice given by
the advisory board to the development team.

Detailed mandate and responsibility of the board can be found [here](rucio_advisory_board.md).

## Special Interest Groups

Rucio Special Interest Groups (SIG) serve the purpose to offer a forum for interested users,
operators, and developers to discuss and plan the evolution of a specific part of Rucio.
The topic of a SIG needs to be well-defined and the community interest on the topic
needs to be above a threshold to justify the creation of a SIG, instead of covering the
topic just within the weekly Rucio meeting. A SIG topic can involve one or multiple
Rucio components, or even the entire system. SIGs are open to any interested community
member.

Detailed information for Rucio SIGs can be found [here](special_interest_groups.md).

## Contributors / Developers

Rucio could not exist without the numerous contributors who spent their valuable time
to improve the software. A list of contributors can be seen [here](about_our_contributors).
---
id: sig_tokens
title: SIG Tokens
---

## Objective

The objective of this SIG is to collect feedback from the wider Rucio community
about the evolution of OIDC/oAuth2 token workflows in Rucio. This feedback should
first be compiled into an, ongoing, design document and then drive the development
of token functionality in Rucio.

## Expected End-Date

March 2026, in line with the [WLCG Token Transition](https://zenodo.org/record/7014668)
timeline. By then token functionality in Rucio will have reached a level of maturity
and the further development can be coordinated without the SIG.

## Means to achieve the objective

- Mattermost channel: [#tokens](https://mattermost.web.cern.ch/rucio/channels/tokens)
- eMail list: rucio-sig-tokens@cern.ch
- Ad hoc meetings
  - [SIG Tokens Kickoff](https://indico.cern.ch/event/1316668/), Weekly Rucio Meeting, September 2023
- A regularly updated design document

## Convener

[Dimitrios Christidis](https://github.com/dchristidis)

## Design document

| Version                                                                         | Creation     |
| ------------------------------------------------------------------------------- | ------------ |
| [v0.1](https://rucio.cern.ch/documentation/files/Rucio_Tokens_v0.1.pdf)         | Sep 25, 2023 |
---
id: developing_with_rucio
title: Developing with Rucio
sidebar_position: 5
---

## Rucio Clients

Rucio includes a client class to remove some of the complexity of dealing with
raw HTTP requests against the RESTful API.

All client methods are accessible through the high-level class Client.  Below is
one example of using Rucio Client class:

```python
from rucio.client import Client
rucio_client = Client()
rucio_client.ping()
```

The methods are separated per resource type. The API in full can be viewed
[here](pathname:///html/site/client.html).

### Errors and Exceptions

In the case of an error, Rucio returns a Python Exception with the appropriate
Traceback, a detailed error string, and a unique error number. If the error
occurred on the server side, it will be propagated to the client. The command
line clients will exit back to the shell with the POSIX `errno` of
the unique Rucio error number. The full and up to date list can be found in the
[Exception
definition](https://github.com/rucio/rucio/blob/master/lib/rucio/common/exception.py).

## RESTful APIs

Each resource can be accessed or modified using specially formed URLs and the
standard HTTP methods:

```text
GET to read
POST to create
PUT to update
DELETE to remove
```

We require that all requests are done over SSL. The API supports JSON
formats. Rucio uses [OAuth](http://oauth.net/) to authenticate all API
requests. The method is to get an authentication token, and use it for the rest
of the requests. Descriptions of the actions you may perform on each resource
can be found in the REST API documentation.

### Date format

All dates returned are in UTC and are strings in the following format (RFC 1123,
ex RFC 822): `Mon, 13 May 2013 10:23:03 UTC`

In code format, which can be used in all programming languages that support
strftime or strptime:

```text
%a, %d %b %Y %H:%M:%S UTC
```

### SSL only

We require that all requests(except for the ping) are done over SSL.

### Response formats

The currently-available response format for all REST endpoints is the
string-based format JavaScript Object Notation ([JSON](http://www.json.org/)).
The server answer can be one of the following content-type in the http Header:

```text
Content-type: application/json
Content-Type: application/x-json-stream
```

In the last case, it corresponds to JSON objects delimited by newlines(streaming
JSON for large answer), e.g.:

```json
{ "id": 1, "foo": "bar" }
{ "id": 2, "foo": "baz" }
...
```

### Error handling

Errors are returned using standard HTTP error code syntax.  Additional info is
included in the header or the body of the return call, JSON-formatted with the
parameters:

```text
ExceptionClass
ExceptionMessage
```

Where `ExceptionClass` refers to Rucio Exceptions.
---
title: Setting Up the Rucio Client
sidebar_position: 1
---

## Install via pip

When `pip` is available, the distribution can be downloaded from the
Rucio PyPI server and installed in one step:

```bash
pip install rucio-clients
```

This command will download the latest version of Rucio and install it to your
system. The dependencies are listed in the file
[`requirements.client.txt`](https://github.com/rucio/rucio/blob/master/requirements/requirements.client.txt)
and will be pulled in as necessary.

## Upgrade via pip

To upgrade via pip:

```bash
pip install --upgrade rucio-clients
```

## Installing using setup.py

Otherwise, you can install from the distribution using the `setup.py`
script:

```bash
python setup.py install
```
---
title: Configuring the Client
sidebar_position: 2
---

## Environmental variables

Please note that the underlying libraries that the Rucio client uses (e.g. GFAL) may be further affected by environmental variables not mentioned in this page.

### ATLAS_SITE_NAME

See [SITE_NAME](#SITE_NAME).

### CONDA_PREFIX

See [RUCIO_HOME](#RUCIO_HOME).

### OSG_SITE_NAME

See [SITE_NAME](#SITE_NAME).

### RUCIO_ACCOUNT

Which account to use to authenticate to Rucio.
Corresponds to the `--account` command-line option and the `clients.account` configuration-file option.

### RUCIO_AUTH_TYPE

Which mechanism to use to authenticate to Rucio.
Corresponds to the `--auth-strategy` command-line option and the `clients.auth_type` configuration-file option.
Valid options are `gss`, `oidc`, `saml`, `ssh`, `userpass`, `x509`, and `x509_proxy`.

### RUCIO_CLIENT_MODE

Force the Rucio client to communicate to the Rucio server.
Only matters when the `database` section exists in the configuration file.
Any non-empty value is treated as true.

### RUCIO_CONFIG

The path to the main Rucio configuration file.
Defaults to `$RUCIO_HOME/rucio.cfg`.

### RUCIO_HOME

The directory where the Rucio configuration files are located.
Defaults to `/opt/rucio/`.

### RUCIO_LATITUDE

The geographical location where the client is run.
Used to potentially prefer replicas on RSEs which are closer geographically.
The value is a floating-point number.
Defaults to `0`.
Both `RUCIO_LATITUDE` and `RUCIO_LONGITUDE` must be properly set to have effect.

### RUCIO_LOGGING_FORMAT

Configure the content and presentation of log entries.
Refer to the [Python logging documentation](https://docs.python.org/3/library/logging.html#logrecord-attributes).

### RUCIO_LONGITUDE

See [RUCIO_LATITUDE](#RUCIO_LATITUDE).

### RUCIO_VO

Which Virtual Organisation (VO) to use to authenticate to Rucio.
It matters for multi-VO Rucio instances.
Corresponds to the `--vo` command-line option and the `client.vo` configuration-file option.
Defaults to `def`.

### SITE_NAME

The name of the site on which the Rucio client is run.
It matters for sites which have different RSE protocols for WAN and LAN.
If the `site` RSE attribute matches `SITE_NAME`, then the LAN domain is preferred.
The site name of the site is also put in the Rucio traces.
Defaults to `ROAMING`.

### VIRTUAL_ENV

See [RUCIO_HOME](#RUCIO_HOME).

### X509_CERT_DIR

The path to the directory or certificate bundle to use to verify the Rucio servers.
Corresponds to the `--ca-certificate` command-line option and the `client.ca_cert` configuration-file option.
Defaults to using the Mozilla certificate collection (Certifi).
On Red Hat Enterprise Linux (and derivatives), depending on the method of installation, the system trust may be the default instead.

### BitTorrent

The following environmental variables affect the experimental BitTorrent feature:

* `QBITTORRENT_LISTEN_PORT`
* `QBITTORRENT_TRACKER_PORT`
* `QBITTORRENT_UI_CERT`
* `QBITTORRENT_UI_KEY`
* `QBITTORRENT_UI_PASSWORD`
* `QBITTORRENT_UI_PORT`
* `QBITTORRENT_UI_USERNAME`

### Traces

The following environmental variables offer an advanced mechanism to populate the content of the Rucio traces:

* `RUCIO_TRACE_APPID`
* `RUCIO_TRACE_DATASET`
* `RUCIO_TRACE_DATASETSCOPE`
* `RUCIO_TRACE_EVENTTYPE`
* `RUCIO_TRACE_PQ`
* `RUCIO_TRACE_TASKID`
* `RUCIO_TRACE_USRDN`
---
id: using_the_client
title: Using the Client
sidebar_position: 3
---

Rucio provides several commands for the end-user. See [executables](bin/rucio.md).
The command line client is called ``rucio``.


## Getting help

To get an overview of the available ``rucio`` subcommands and flags, run:

```bash
rucio --help
```

## Enable command line autocompletion

If you would like to automatically complete ``rucio`` commands, install the
[argcomplete](https://pypi.org/project/argcomplete/) package and run:

```bash
eval "$(register-python-argcomplete rucio)"
```

Next, type ``rucio `` (note the trailing space) and press the <kbd>Tab</kbd>
key to see all available options. To use the autocompletion feature, type enough
letters of a subcommand or flag to uniquely define it and then press
<kbd>Tab</kbd>.

## Getting user information

The first thing you might try is to check who you are:

```bash
$ rucio whoami
status     : ACTIVE
account    : jdoe
account_type : SERVICE
created_at : 2014-01-17T07:52:18
updated_at : 2014-01-17T07:52:18
suspended_at : None
deleted_at : None
email      : jdoe@blahblah.com
```

You can switch between different accounts by setting the `RUCIO_ACCOUNT`
variable:

```bash
$ export RUCIO_ACCOUNT=root
$ rucio whoami
status     : ACTIVE
account    : jdoe
account_type : SERVICE
created_at : 2014-01-17T07:51:59
updated_at : 2014-01-17T07:51:59
suspended_at : None
deleted_at : None
email      : root@blahblah.com
```

If you try to authenticate with an account that is not mapped with your
credentials:

```bash
$ export RUCIO_ACCOUNT=janedoe
$ rucio whoami
cannot get auth_token
2018-01-30 16:50:08,554 ERROR   Cannot authenticate.
Details: x509 authentication failed
2018-01-30 16:50:08,554 ERROR   Please verify that your proxy is \
  still valid and renew it if needed.
```

The VO to authenticate against is set in the configuration file, if you're
running a *multi-VO* instance of Rucio. However you can specify a different VO
as a CLI argument if your credentials map to an account there too:

```bash
$ rucio whoami
status     : ACTIVE
account    : jdoe
account_type : SERVICE
created_at : 2014-01-17T07:52:18
updated_at : 2014-01-17T07:52:18
suspended_at : None
deleted_at : None
email      : jdoe@normalvo.com
```

```bash
$ rucio --vo abc --account root whoami
status     : ACTIVE
account    : root
account_type : SERVICE
created_at : 2014-01-17T07:51:59
updated_at : 2014-01-17T07:51:59
suspended_at : None
deleted_at : None
email      : root@abc.com
```

## Open ID Connect authentication examples

There are 3 CLI login methods. Two were introduced in order to avoid typing the
password in the Rucio CLI. The default Identity Provider `(IdP)/issuer` is
configured on the side of Rucio server. In case multiple IdPs are supported,
user can specify which one he desires to use by `--oidc-issuer=\<IdP nickname\>`
option (where IdP nickname is the key under which issuers are configured on
Rucio server side in the *idpsecrets.json* file). In the following examples we
assume that user does not want to use the rucio account name specified in the
*rucio.cfg* file on the client side (if so `-a` parameter can be omitted).  If
*auth_type* is specified to be "oidc" in the *rucio.cfg* file, `-S` can be
omitted as well.  Furthermore, we use the same default issuer as configured on
Rucio server side.

1. Login via user's browser + fetch code:

  ```bash
  rucio -a=\<rucio_account_name\> -S=OIDC -v whoami
  ```

1. Login via user's browser + polling Rucio auth server:

  ```bash
  rucio -a=\<rucio_account_name\> -S=OIDC --oidc-polling -v whoami
  ```

1. Automatic login:

  ```bash
  rucio -a=\<rucio_account_name\> \
    -S=OIDC --oidc-user=\<idp_username\> \
    --oidc-password=\<idp_password\> \
    --oidc-auto \
    -v \
    whoami
  ```

We strongly discourage this approach, typing your password in CLI does not
comply with OAuth2/OIDC standard !

Options for automatic token refresh: Assuming the rucio-oauth-manager daemon is
running on the Rucio server side, one can also grant Rucio a refresh token and
specify the time for which Rucio should act on behalf of the user (in hours)
using the `--refresh-lifetime` option:

```bash
rucio -a=\<rucio_account_name\> \
  -S=OIDC \
  --oidc-scope="openid profile offline_access" \
  --oidc-refresh-lifetime=24 \
  -v \
  whoami
```

If Rucio Server is granted a user both valid access and refresh tokens, it is
also possible to configure Rucio Client to ask Rucio Server for token
refresh. Assuming user used one of the 3 CLI authentication methods above +
requested offline_access in the scope, rucio.cfg file can be configured with the
following parameters in the `[client]` section:

```bash
[client]
auth_oidc_refresh_active = true
auth_oidc_refresh_before_exp = 20
```

`auth_oidc_refresh_active` is false by default. If set to true, the Rucio Client
will be following up token expiration timestamp. As soon as the current time
gets to `auth_oidc_refresh_before_exp` minutes (20 min default) before token
expiration, Rucio Client will ask Rucio Server for token refresh with every
command. If the token has been refreshed in the recent 5 min already once, the
same one will be returned (protection on the Rucio Server side). If the
presented token has been refreshed automatically on the Rucio Server side by a
oauth_manager daemon run, it will return this existing new token. If the
presented token is invalid/expired/does not have refresh token in the DB, no
refresh will be attempted.

Example of rucio.cfg file configuration with automatic token refresh:

```cfg
[client]

rucio_host = https://\<rucio_host\>:443
auth_host = https://\<rucio_auth_host\>:443
auth_type = oidc
account = \<rucio_account_name\>
oidc_audience = rucio
oidc_scope = openid profile offline_access
oidc_issuer = wlcg
auth_oidc_refresh_active = true
auth_oidc_refresh_before_exp = 20
```

Then, you should be able to do simply:

```bash
rucio -v whoami
```

and follow the instruction for first log-in with your browser. New token will be
requested before the current expires if a user types a rucio command within
`auth_oidc_refresh_before_exp` minutes before the expiry.  Note: If user does
not use Rucio Client within `auth_oidc_refresh_before_exp` minutes before token
expires, it will be necessary to re-authenticate asking for a new offline_access
token.

If a user wishes to authenticate with Rucio using a JSON web token not issued
via the Rucio login mechanisms (CLI, WebUI), one has to make sure that:

- The token scope claim is no less than the minimum scope (e.g. 'openid
  profile') required by the Rucio Auth server (configured there in the rucio.cfg
  file).
- same as above is true for the use of audience claim
- token issuer is known to Rucio Authentication server
- the identity of the token (`SUB=\<user sub claim\>, ISS=\<issuer url\>`) is
  assigned to an existing Rucio account (pre-provisioned)

If so, one can directly present the token to the Rucio REST endpoint in the
`X-Rucio-Auth-Token` header, e.g.:

```python
python
import requests
s=requests.session()
your_token=\<your JWT access token string\>
headers={'X-Rucio-Auth-Token': your_token}
address='https://\<Rucio Auth Server Name\>/accounts/guenther'
result=s.get(address, headers=headers, verify=False)
result.text
u'{
  "status": "ACTIVE",
  "account": "guenther",
  "account_type": "USER",
  "created_at": "2019-11-13T13:01:58",
  "suspended_at": null,
  "updated_at": "2019-11-13T13:01:58",
  "deleted_at": null,
  "email": "jaroslav.guenther@gmail.com"
}'
```

There is also an option to specify a `auth_token_file_path` in the `client`
section of the rucio.cfg file. Rucio Client will then store and search for
user's token saved in such file:

```cfg
[client]
auth_token_file_path = /path/to/token/file
```

## Querying basic information about RSEs

You can query the list of available RSEs:

```bash
$ rucio list-rses
SITE1_DISK
SITE1_TAPE
SITE2_DISK
SITE2_SCRATCH
SITE3_TAPE
```

If the RSEs are tagged with attributes you can build RSE expressions and query
the sites matching these expressions:

```bash
$ rucio list-rses --rses "tier=1&disk=1"
SITE1_DISK
SITE2_DISK
```

## Querying information about DIDs

To list all the possible scopes:

```bash
$ rucio list-scopes
mc
data
user.jdoe
user.janedoe
```

You can query the DIDs matching a certain pattern. It always requires to specify
the scope in which you want to search:

```bash
$ rucio list-dids user.jdoe:*
+-------------------------------------------+--------------+
| SCOPE:NAME                                | [DID TYPE]   |
|-------------------------------------------+--------------|
| user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
| user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
| user.jdoe:user.jdoe.test.dataset.1        | DATASET      |
| user.jdoe:user.jdoe.test.dataset.2        | DATASET      |
| user.jdoe:test.file.1                     | FILE         |
| user.jdoe:test.file.2                     | FILE         |
| user.jdoe:test.file.3                     | FILE         |
|-------------------------------------------+--------------|
```

You can filter by key/value, e.g.:

```bash
$ rucio list-dids --filter type=CONTAINER
+-------------------------------------------+--------------+
| SCOPE:NAME                                | [DID TYPE]   |
|-------------------------------------------+--------------|
| user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
| user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
|-------------------------------------------+--------------|
```

If you want to resolve a collection (CONTAINER or DATASET) into the list of its
constituents:

```bash
$ rucio list-content user.jdoe:user.jdoe.test.container.1234.1
+------------------------------------+--------------+
| SCOPE:NAME                         | [DID TYPE]   |
|------------------------------------+--------------|
| user.jdoe:user.jdoe.test.dataset.1 | DATASET      |
| user.jdoe:user.jdoe.test.dataset.2 | DATASET      |
+------------------------------------+--------------+
```

You can resolve also the collections (CONTAINER or DATASET) into the list of
files:

```bash
$ rucio list-files user.jdoe:user.jdoe.test.container.1234.1
+-----------------------+---------+-------------+------------+----------+
| SCOPE:NAME            | GUID    | ADLER32     | FILESIZE   | EVENTS   |
|-----------------------+---------+-------------+------------+----------|
| user.jdoe:test.file.1 | 9DF3... | ad:56fb0723 | 39.247 kB  |          |
| user.jdoe:test.file.2 | 67E8... | ad:e3e573b5 | 636.075 kB |          |
| user.jdoe:test.file.3 | 32CD... | ad:22849380 | 641.427 kB |          |
+-----------------------+---------+-------------+------------+----------+
Total files : 3
Total size : 1.316 MB:
```

## Rules operations

You can create a new rule like this:

```bash
$ rucio add-rules --lifetime 1209600 \
  user.jdoe:user.jdoe.test.container.1234.1 1 \
  "tier=1&disk=1"
a12e5664555a4f12b3cc6991db5accf9
```

The command returns the rule_id of the rule.

You can list the rules for a particular DID:

```bash
$ rucio list-rules user.jdoe:user.jdoe.test.container.1234.1
ID    ACCOUNT  SCOPE:NAME  STATE[OK/REPL/STUCK]  RSE_EXPRESSION  COPIES  EXPIRES
----  -------  ----------  --------------------  --------------  ------  -------
a...  jdoe     user....    OK[3/0/0]             tier=1&disk=1   1       2018...
b...  janedoe  user....    REPLICATING[4/1/1     tier=1&tape=1   2
4...  mc       user....    OK[3/0/0]             tier=1&tape=1   2
```

The state indicates how many locks (physical replicas of the files) are OK,
Replicating or Stuck

## Accessing files

The command to download DIDs locally is called rucio download. It supports
various sets of option. You can invoke it like this:

```bash
# rucio download user.jdoe:user.jdoe.test.container.1234.1
2018-02-02 15:13:08,450 INFO    Thread 1/3 : Starting the download of user.jdoe:test.file.2
2018-02-02 15:13:08,451 INFO    Thread 2/3 : Starting the download of user.jdoe:test.file.3
2018-02-02 15:13:08,451 INFO    Thread 3/3 : Starting the download of user.jdoe:test.file.1
2018-02-02 15:13:08,503 INFO    Thread 1/3 : File user.jdoe:test.file.2 trying \
  from SITE1_DISK
2018-02-02 15:13:08,549 INFO    Thread 2/3 : File user.jdoe:test.file.3 trying \
  from SITE2_DISK
2018-02-02 15:13:08,551 INFO    Thread 3/3 : File user.jdoe:test.file.1 trying \
  from SITE1_DISK
2018-02-02 15:13:10,399 INFO    Thread 3/3 : File user.jdoe:test.file.1 \
  successfully downloaded from SITE1_DISK
2018-02-02 15:13:10,415 INFO    Thread 2/3 : File user.jdoe:test.file.3 \
  successfully downloaded from SITE2_DISK
2018-02-02 15:13:10,420 INFO    Thread 3/3 : File user.jdoe:test.file.1 \
  successfully downloaded. 39.247 kB in 1.85 seconds = 0.02 MBps
2018-02-02 15:13:10,537 INFO    Thread 2/3 : File user.jdoe:test.file.3 \
  successfully downloaded. 641.427 kB in 1.87 seconds = 0.34 MBps
2018-02-02 15:13:10,614 INFO    Thread 1/3 : File user.jdoe:test.file.2 \
  successfully downloaded from SITE1_DISK
2018-02-02 15:13:10,633 INFO    Thread 1/3 : File user.jdoe:test.file.2 \
  successfully downloaded. 636.075 kB in 2.11 seconds = 0.3 MBps
----------------------------------
Download summary
----------------------------------------
DID user.jdoe:user.jdoe.test.container.1234.1
Total files :                                 3
Downloaded files :                            3
Files already found locally :                 0
Files that cannot be downloaded :             0
```
---
id: using_the_admin_client
title: "Using the Admin Client"
sidebar_position: 4
---

Rucio provides a CLI for administrative tasks. The get methods can be executed
by any user, but the set methods require some admin privileges. See the
[rucio-admin help page](bin/rucio-admin.md).
The command line client for administrative tasks is called ``rucio-admin``.

## Getting help

To get an overview of the available ``rucio-admin`` subcommands and flags, run:

```bash
rucio-admin --help
```

## Enable command line autocompletion

If you would like to automatically complete ``rucio-admin`` commands, install
the [argcomplete](https://pypi.org/project/argcomplete/) package and run:

```bash
eval "$(register-python-argcomplete rucio-admin)"
```

Next, type ``rucio-admin `` (note the trailing space) and press the
<kbd>Tab</kbd> key to see all available options. To use the autocompletion
feature, type enough letters of a subcommand or flag to uniquely define it
and then press <kbd>Tab</kbd>.

## Account and identity methods

To create a new account:

```bash
  $ rucio-admin account add --type USER --email jdoe@blahblih.com jdoe
```

You can choose different types in the list USER, GROUP, SERVICE. Different
policies/permissions can be set depending on the account type.  Once the account
is created, you need to create and attach an identity to this account:

```bash
  $ rucio-admin identity add --type X509 \
      --id "CN=jdoe,OU=Users,OU=Organic Units,DC=blih,DC=blah" \
      --email jdoe@blahblih.com --account jdoe
```

The list of possible identity types is X509, GSS, USERPASS, SSH:

```bash
  $ rucio-admin account list-identities jdoe
  Identity: CN=jdoe,OU=Users,OU=Organic Units,DC=blih,DC=blah,        type: X509
```

You can set attributes to the users:

```bash
  $ rucio-admin account add-attribute --key country --value xyz jdoe
```

And list these attributes:

```bash
  $ rucio-admin account list-attributes jdoe
  +---------+-------+
  | Key     | Value |
  |---------+-------|
  | country | xyz   |
  +---------+-------+
```

You can also list all the accounts matching a certain attribute using the filter
option:

```bash
  $ rucio-admin account list --filters "country=xyz"
  jdoe
```

To set the quota for one account on a given RSE:

```bash
  $ rucio-admin account set-limits jdoe SITE2_SCRATCH 10000000000000
  Set account limit for account jdoe on RSE SITE2_SCRATCH: 10.000 TB
  $ rucio-admin account get-limits dcameron SITE2_SCRATCH
  Quota on SITE2_SCRATCH for jdoe : 10 TB
```

## Scope methods

To create a new scope:

```bash
  $ rucio-admin scope add --account jdoe --scope user.jdoe
```

Only the owner of the scope or privileged users can write into the scope.

To list all the scopes:

```bash
  $ rucio-admin scope list
  user.janedoe
  user.jdoe
```

## RSE methods

To create a new RSE:

```bash
  $ rucio-admin rse add SITE2_SCRATCH
```

To add a RSE attribute:

```bash
  $ rucio-admin rse set-attribute --rse SITE2_SCRATCH --key country --value xyz
```

To check an RSE attribute:

```bash
  $ rucio-admin rse get-attribute SITE2_SCRATCH
  country: xyz
```

## Replica methods

To declare bad (i.e. corrupted or lost replicas):

```bash
  $ rucio-admin replicas declare-bad --reason "File corrupted" https//path/to/lost/file
```
---
id: sig_metadata
title: SIG Metadata
---

## Objective

The purpose of this SIG is to reach out to communities who are either already using
Rucio's metadata functionality or interested in using it, to ascertain if the
functionality currently provided in this area is sufficient for their current and
expected future use cases. This information will be distilled into the form of a
small report, where, if missing functionality is identified, the report will serve
to aggregate any similar requests and prioritise them, with the goal of delivering
a tentative roadmap to guide development.

## Expected End-Date

End of ~~2023~~ ~~2024~~ 2026

## Means to achieve the objective

- Mattermost channel: [#metadata](https://mattermost.web.cern.ch/rucio/channels/metadata)
- eMail list: rucio-sig-metadata@cern.ch
- Events and ad-hoc meetings with interested communities either over Zoom or asynchronously
  over Mattermost.
  - [SIG Metadata Kickoff](https://indico.cern.ch/event/1051395/), Weekly Rucio Meeting, July 2021
  - [Metadata tests in Belle II](https://indico.cern.ch/event/1068644/), Weekly Rucio Meeting, September 2021
  - [Astronomy & Metadata Panel](https://indico.cern.ch/event/1037922/), 4th Rucio Community Workshop, September 2021
  - [Recent metadata developments & SIG Metadata status](https://indico.cern.ch/event/1160579/), Weekly Rucio Meeting, June 2022
  - [SIG Metadata Update](https://indico.cern.ch/event/1185600/contributions/5120129/), 5th Rucio Community Workshop, November 2022
  - [Rucio metadata extension roadmap](https://indico.cern.ch/event/1343110/), 7th Rucio Community Workshop, September 2024
  - [AI Driven Metadata Generation Applied to Copernicus Data](https://www.hipeac.net/2025/barcelona/#/program/?q=cern), Workshop at High Performance, Edge And Cloud computing (HiPEAC) conference, January 2025

## Convener

[Rob Barnsley](https://github.com/robbarnsley)

## Documents

| Document                                                                                                                                                        | Creation       |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|
| [Rucio SIG Metadata interim report](https://rucio.cern.ch/documentation/files/Rucio_SIG_Metadata_report_2024-03-25.pdf)                                         | Mar 25, 2024   |
| [Presentation on Rucio metadata extension roadmap](https://indico.cern.ch/event/1343110/contributions/6099490/attachments/2937898/5160700/DaFab%20D.Xenakis.pdf)| Sep 30, 2024   |
---
id: index
title: "Welcome to Rucio's documentation"
sidebar_label: "Welcome"
slug: /
---

Rucio is a project that provides services and associated libraries for allowing
scientific collaborations to manage large volumes of data spread across
facilities at multiple institutions and organisations. Rucio was originally
developed to meet the requirements of the high-energy physics experiment
[ATLAS](https://atlas.cern/), and now is continuously extended to support the
LHC experiments and other diverse scientific communities.

Rucio offers advanced features, is highly scalable, and modular. It is a data
management solution that covers the needs of different communities in the
scientific domain (e.g., HEP, astronomy, biology).

Below are some resources to help you get you started on your journey.

## Getting Started

What exactly is Rucio? What were the motivations behind developing such a
system? Who uses it? What powers these systems? Answers to all these questions
and more can be found by browsing through the sub-sections of this topic.

- [What is Rucio](started/what_is_rucio.md)
- [Main Components](started/main_components/main_components.md)
- [Additional Layers and Resources](started/additional_layers_and_resources.md)
- [About Rucio Daemons](started/main_components/daemons.md)

## Client

The rucio client enables users to interact with the system and access the
distributed data. The client can upload, download, manage and delete everything
from single files up to Petabyte sized datasets.

- [Setting Up the Rucio Client](user/setting_up_the_rucio_client.md)
- [Using the Client](user/using_the_client.md)
- [Using the Admin Client](user/using_the_admin_client.md)

## Administration

This section of the documentation deals with some of the material that an
operator or administrator of a Rucio environment would require. For example, how
to install a server or some quick tips for working with the administrative
CLI. Take a deep dive, but not before you ensure you've read through the
pre-requisites section under each of the topics!

- [Setting up a Rucio demo environment](operator/setting_up_demo.md)
- [Installing Rucio Server](operator/installing_server.md)
- [Installing Rucio Daemons](operator/installing_daemons.md)
- [Monitoring](operator/monitoring.md)
- [Database](operator/database)
- [Configuration parameters](operator/configuration_parameters.md)

## Developer Documentation

Whether you want to develop with Rucio or contribute to the project, the
Developer documentation will help you get started. Peruse some common REST API &
Client API references that are directly derived from Rucio's python
libraries. We also have a contribution guide for those who wish to pitch in.

- [Client API Documentation](pathname:///html/site/client.html)
- [REST API Documentation](pathname:///html/rest_api_doc.html)
- [Contributing guide](contributing)

## Contributing to the Documentation

Documentation is always a work in progress and we welcome both, qualitative and
technical contributions, from the community. Make sure you look into the
[documentations GitHub repository](https://github.com/rucio/documentation) and
understand the pre-requisites before you submit your first PR!

## About Us

Learn more about the brilliant minds pioneering the development and maintenance
of Rucio in this section. Should you wish to get in touch with us, we've also
included several ways of doing so in the **Contact Us** section.

- [Project Organisation](project_organisation.md)
- [About Our Contributors](about_our_contributors.md)
- [Contact Us](contact_us.md)
---
id: contributing
title: Contributing Guide
sidebar_label: Contributing Guide
---

## Thank you for participating

The following is a set of rules for contributing to **Rucio** and its
packages. Use your best judgment, and feel free to propose changes to this
document.

If you have questions, you can reach the core development team on our
[__Mattermost__](mattermost.md) channel, or send an email to our
development mailing list [__rucio-dev@cern.ch__](mailto:rucio-dev@cern.ch).

## What should I know before I get started

A contribution can be either be a **patch** or **feature**:

* **Patches** include bugfixes and minor changes to the code and are included in
  patches that are usually released every two weeks.
* **Features** include major developments or potentially disruptive changes and
  are included in feature releases made multiple times a year.

The [__repository__](https://github.com/rucio/rucio/) consists of different
branches:

* the **master** branch includes the development for the next major version.
* the **release-…** branches include the patch/minor development of the
  releases.

Release branches only exist for the currently maintained release
versions. Hotfix branches are created on demand. Please communicate to the Rucio
maintainers, if you wish to hotfix a previous release.

Generally all [__pull requests__](https://github.com/rucio/rucio/pulls) are to
be created against the Rucio **master** branch. Features will end up in the
upstream **master** only and patches are cherry-picked to the maintained
releases if applicable. Release-specific changes are excluded from that rule and
might be needed if e.g. cherry-picking to the last release was not successful.

The following figure might help you with an overview:

![Branching Strategy Graph](/img/branching_strategy.svg)

## How can I Contribute

### 1. Prerequisite

* Ensure you add your name (and organisation) to our list of [__contributors__](about_our_contributors.md).

* Fork the [__repository__](https://github.com/rucio/rucio/) on
  Github.

* Clone the repository to your development machine and configure it:

  ```bash
  git clone https://github.com/<YOUR_USER>/rucio/
  cd rucio
  git remote add upstream https://github.com/rucio/rucio.git
  ```

* **Optional: Install Git Hooks**

  The `prepare-commit-msg` hook can be installed by executing the script:

  ```bash
  ./tools/configure_git.sh
  ```

  Also, the [`pre-commit` python](https://pre-commit.com/) package is configured
  for this repository. The `pre-commit` hook checks the syntax and format of the
  files before committing. This saves time in the development process, since
  minor errors are noticed before submission.

  To install the package and activate the hooks for the project:

  ```bash
  pip install pre-commit
  pre-commit install
  ```

  If you only want to run the hooks on a push, run:

  ```bash
  pre-commit install --hook-type pre-push
  ```

  More information [please view the pre-commit documentation](https://pre-commit.com/#confining-hooks-to-run-at-certain-stages)

### 2. Create an Issue

Please ensure that an [__issue__](https://github.com/rucio/rucio/issues/new)
exists before submitting your contribution as a pull request. The issue should
contain the motivation, modification and expected results (discussions usually
happen there). No pull request will be merged without an associated issue
(release notes are generated from issues). Each issue gets a **unique issue
number**.

### 3. Create a local working branch

Create a local branch that corresponds to the issue. To easily
identify the purpose of branches different keywords must be used:

* Patch branches must be named **patch-[issue number]-[short description]**
* Feature branches must be named **feature-[issue number]-[short description]**

If you create these branches by hand please check the spelling because otherwise
the test automation might misidentify your branch. There are utility scripts to
fetch master and create these branches for you:

```bash
./tools/create-patch-branch <unique issue number> '<short_change_message>'
./tools/create-feature-branch <unique issue number> '<short_change_message>'
```

### 4. Commit your changes

Commit your change. The commit command must include a specific message format:

```bash
git commit -m "<component>: <change_message> #<issue number>"
```

Valid component names are listed in the [__label
list__](https://github.com/rucio/rucio/labels) and are usually specified on the
issue of the change.

Add additional explanations to the body of the commit, such as motivation for
certain decisions and background information. [Here are some general rules.](https://cbea.ms/git-commit/).

If you add a [__github-recognised
keyword__](https://help.github.com/articles/closing-issues-using-keywords/) then
the associated issue can be closed automatically once the pull request is
merged, e.g.:

```bash
<component>: <change_message> Fix #<issue number>
```

Using multiple commits is allowed as long as they achieve an independent,
well-defined, change and are well-described. Otherwise multiple commits should
be squashed.

### 5. Push changes and create a Pull Request

Push the commit to your forked repository and create the pull request. Try to
keep the Pull Request simple, it should achieve the single objective described
in the issue. Multiple enhancements/fixes should be split into multiple Pull
Requests.

While using the [__github
interface__](https://help.github.com/articles/creating-a-pull-request/) is the
default interface to create pull requests, you could also use GitHub’s
command-line wrapper [__hub__](https://hub.github.com) or the [__GitHub
CLI__](https://cli.github.com/).

The format of the pull request title must be:

```bash
<component>: <short_change_message> #<issue number>
```

### 6. Watch the Pull Request for reviews

Watch the pull request for comments and reviews. For any pull requests update,
please try to squash/amend your commits to avoid “in-between” commits.

## Automatic Testing

Every submitted pull request will automatically be run through automated testing
through continuous integration.
This testing includes multiple [suites of testing](https://github.com/rucio/rucio/tree/master/.github/workflows),
all of which are required to pass.
Please enable testing on your fork of the main repository to see the status of your tests as you develop.


###  Writing Tests
For every feature added, there should be a set of corresponding tests that verify
its functionality and integration with the rest of the codebase.

* Use fixtures (found in the tests/conftest.py) or temporary object factories
(tests/temp_factories.py) instead of making bare instances of rucio objects.
* Only write tests deterministically.
 Randomness produces [flaky tests](https://docs.pytest.org/en/7.1.x/explanation/flaky.html).
* Only write tests that are "stand alone" -
tests should be entirely self-contained besides for the before-mentioned fixtures and factories.
* If a test requires a configuration file changed,
[use a fixture to modify a mock-configuration file.](https://github.com/rucio/rucio/blob/master/tests/conftest.py#L510)
* If a test can interfere with another test
(use the same database table, interact with a queue), mark it as `noparallel`.
* If a test is specific to a VO, mark it as such using a [`skip_non_{vo}`](https://github.com/rucio/rucio/blob/master/lib/rucio/tests/common.py) fixture,
or mark it as `skip_multivo` if the test only is intended to work in single-vo settings.

### Local automatic testing

There is also a local shell script to run the same autotests:
`tools/run_autotests.sh`. For manual local testing within containers, please see
[__the docker
README__](https://github.com/rucio/rucio/blob/master/etc/docker/dev/README.rst).

**WARNING:** Because of the nature of using the same scripts as continuous
integration, some containers may be left running after a test run or when
aborting the test run. This is especially the case for running this script
without podman.

By default the tool uses 3 worker processes to run all tests that are defined in
`etc/docker/test/matrix.yml`. Feel free to modify the matrix to your needs, but
be sure to not unintentionally commit your changes to it. The tests run at most
6 hours - after that a TimeoutError will be raised, causing the script to
fail. Running the autotests like this can be parameterized with environment
variables as follows:

* `USE_PODMAN` 0/1 (default: depends on whether the docker command points to
  podman)

    Use podman and therefore pods to run the tests.

* `PARALLEL_AUTOTESTS` 0/1 (default: 1)

    1 enables multiple processes to run autotests and 0 disables it.  When
    enabled, logs of the running autotests will be written to the `.autotest`
    directory created in the working directory. Otherwise the log output will be
    written to the console (stderr).

    *Note that when tests are not running in parallel mode, the test run will
    always fail fast.*

* `PARALLEL_AUTOTESTS_PROCNUM` (1,) (default: 3)

    Specifies the number of processes to run and therefore the concurrently run
    autotests. 3 will usually result in more than 8 GB RAM usage and a fair
    amount of load on the PC.

* `PARALLEL_AUTOTESTS_FAILFAST` 0/1 (default: 0)

    Will abort the parallel run of autotests as soon as possible after at least
    one autotest failed. Enabling this will leave containers running in case of
    a failure even on podman.

* `COPY_AUTOTEST_LOGS` 0/1 (default: 0)

    Copies `/var/log` from the rucio container into the `.autotest` directory
    after the test run. Each test case will have it’s specific naming as with
    the logs from the parallel run above.

## Human Review

Anyone is welcome to review merge requests and make comments!

The Rucio development team can approve, request changes, or close pull
requests. Merging of approved pull requests is done by the Rucio development
lead.

## Coding Style

We use flake8 and pylint to sanitize our code. Please do the same before
submitting a pull request.

[A more indepth set of coding style guidelines can be found here.](./developer/style_guide.md)
---
id: contact_us
title: Contact us
---

We know getting started can be difficult, which is why
our developers are always happy to help out! Reach out
to us on any of the below channels for any assistance
with Rucio.

## Chat with us

We have a dedicated Mattermost team for questions, support, news, development
updates and all things Rucio and you can be a part of it too!
Click [here](mattermost.md)

## Weekly Rucio meeting

The Rucio community gathers for a meeting every Thursday at 3 PM CEST to discuss
community news, operational issues and questions, as well as ongoing & further
contributions by the development team. The meeting is open to the public
and you can view the schedule [here](https://indico.cern.ch/category/10588/).

## Rucio news mailing list

We have a dedicated news mailing list for general announcements (\<10 eMails per
year) for the Rucio community. You can subscribe to [rucio-news@cern.ch](https://e-groups.cern.ch/e-groups/Egroup.do?egroupId=10586148)
directly with a CERN account, for external subscribers please send an eMail
to [rucio-news-subscribe@cern.ch](mailto:rucio-news-subscribe@cern.ch?subject=Subscribe).

## Email us

We'd love to hear from you! Get in contact with us
directly via [email](mailto:rucio-contact@cern.ch)

## Social Media

Follow us on [Mastodon](https://fosstodon.org/@rucio) to stay updated
with the latest on Rucio!
---
id: sig_qualityofservice
title: SIG Quality of Service
---

## Objective

The purpose of this SIG is to allow a RSE handle in an automatic manner storage
with two different qualitiy of services based on access latency. Initial work in
this area differentiates between disk and tape within the same RSE. New data files
are initially pinned on disk either for the lifetime of the rule or up to the
maximum set by the storage site. After the pin expires the local storage system is
free to stage the file to tape for further access. When Rucio needs to access such
files again, the bring on line command is given to the RSE and the file is
transferred from tape to disk if needed. Currently this is being tested with dCache.
In the future, it can be extended to other storage systems where appropriate.
At the end of this SIG a report will be written documenting the outcome/benefit of this
activity.

## End-Date

End of 2023

## Means to achieve the objective

- Mattermost channel: [#QoS](https://mattermost.web.cern.ch/rucio/channels/QoS)
- eMail list: rucio-sig-qos@cern.ch
- biweekly meetings with interested communities either over Zoom or asynchronously
  over Mattermost.
  - [Meeting series in Indico](https://indico.cern.ch/category/14213/)
  - [Meeting notes](https://codimd.web.cern.ch/MfDv9yRMQmOrwGEwhlGXpQ#)

## Convener

[Doug Benjamin](mailto:douglas.benjamin@cern.ch)
---
id: transfers_poller_receiver
title: Transfers Poller/Receiver
---

`conveyor-poller` and `conveyor-receiver` are daemons responsible for tracking transfer status and updating it in the system.
**You need to run one or both of them** depending on the transfer tool you are using:

- If you are using only FTS3, it is **recommended to use the receiver** for scalability. Although poller also works.
- If you are using only Globus and/or BitTorrent, you only need to run the poller.
- If both FTS3 and Globus and/or BitTorrent, you need both receiver and poller.

## Poller

The `conveyor-poller` daemon periodically queries the transfer tools (such as FTS, Globus, or BitTorrent) directly to retrieve the status of ongoing transfers.
Based on the results, it updates the state of each transfer in the system and/or refreshes the last access time.
The transfer tool to query is taken from the transfer information internally and **no additional configuration is needed**.

**If using Globus or BitTorrent as the transfer tool, you need to use the poller.**

## Receiver

The `conveyor-receiver` daemon subscribes to an ActiveMQ message queue and continuously listens for transfer status updates.
When it receives a message, it processes the update, changes the transfer's state accordingly, and/or updates the last access time.

To set up FTS to send transfer status updates to ActiveMQ broker, see the instructions [FTS3 Messaging Guide](https://fts3-docs.web.cern.ch/fts3-docs/docs/messaging.html).

> **Note:** The receiver connects to ActiveMQ using the **STOMP protocol**, so ensure your ActiveMQ instance has STOMP support enabled. As of this writing (May 2025), CERN FTS uses [ActiveMQ-Classic](https://activemq.apache.org/components/classic/) and
not tested with [ActiveMQ-Artemis](https://activemq.apache.org/components/artemis/).

To configure `conveyor-receiver`, your rucio server config i.e. `rucio.cfg`, include the following section with described options:

```cfg
[messaging-fts3]
# Set to True to use SSL with certificate/key authentication, or False to use username/password
use_ssl = True

# A comma-separated list of ActiveMQ broker DNS hostnames or aliases
brokers = activemq1.example.org,activemq2.example.org

# Only required if use_ssl = False
username = your-username
password = your-password

# Port to use when NOT using SSL (i.e., use_ssl = False)
nonssl_port = 61613

# Port to use when using SSL (i.e., use_ssl = True)
port = 61617

# Required if use_ssl = True
ssl_key_file = /path/to/hostkey.pem
ssl_cert_file = /path/to/hostcert.pem

# The message topic or queue where FTS publishes transfer status (complete) updates.
destination = /topic/transfer.fts_monitoring_complete

# Optional: virtual host name used to connect to the broker
# This is only needed if your ActiveMQ setup requires a virtual host (e.g., in multi-tenant environments)
broker_virtual_host = /atlas
```

---
id: configure-rucio-fts3-plugins
title: Configure A Tool to Determine Rules For FTS3 Tape Transfers
sidebar_label: FTS3 Transfertool Plugins
---

Rucio includes functionality to pass instructions to FTS3 that describe how the transfer should be concluded
(e.g., placement on tape).
This information is passed forward to the storage endpoint as a json of
`archive_metadata`, [as described by FTS3.](https://fts3-docs.web.cern.ch/fts3-docs/fts-rest/docs/bulk.html)

The plugin ensures the metadata conforms to json encoding and size constraints before adding it to the
parameters used in the transfer.


## Writing the plugin

Plugins are expected to be subclasses of `rucio.transfertool.fts3_plugins.FTS3TapeMetadataPlugin`,
and registered as `fts3_tape_metadata_plugins` algorithms.
This is done by using the
`FTS3TapeMetadataPlugin.register` method, which tells the algorithm which code to execute during runtime.

The return type of the selected policy code must be a json-encodable dictionary.
The only required keys are the keys that will be used by the storage endpoint, not any decorators for FTS3.

During the transfer, the algorithm is passed the existing job parameters generated for the transfer in the form of a dictionary.

```python
from rucio.transfertool.fts3_plugins import FTS3TapeMetadataPlugin

class ExperimentFTSPlugins(FTS3TapeMetadataPlugin):
    def __init__(self, policy_algorithm="def"):
        self.register("policy_algorithm", func=self.plugin_algorithm)  # Name and function for the new algorithm
        super().__init__(policy_algorithm)

    def plugin_algorithm(self, *hints):  # Code executed at runtime
        return {"storage_location": "this_location"}
```

This will result in the below transfer.
```json
{"files": [{
    "sources": [...],
    ...
    "archive_metadata": {
        "storage_location": "this_location"
        }
    }]
}

```

### Plugins with initialization rules

If the plugin requires set-up that would slow down transfers, using a plugin initialization is recommended.
This includes things like querying the configuration file, performing a calculation that would not change
between different transfers, or hard-coding parameters.

This is done by including a `init_func` when registering the plugin.

```python
from rucio.transfertool.fts3_plugins import FTS3TapeMetadataPlugin

class ExperimentFTSPlugins(FTS3TapeMetadataPlugin):
    def __init__(self, policy_algorithm="def"):
        self.register(
            "policy_algorithm",
            func=self.plugin_algorithm,
            init_func=self.plugin_initialization)
        super().__init__(policy_algorithm)

    def plugin_algorithm(self, *hints):
        # Can use `self.extra_params`
        return {
            "storage_location": self.extra_params[hints["name"]]
            }

    def plugin_initialization(self):
        self.extra_params = dict(get_config("transfers", "extra_params"))

```
To trigger registration it is recommended to run the plugin in the class file with the default plugin name.

```{python}
class ExperimentFTSPlugins(FTS3TapeMetadataPlugin):
    ...
ExperimentFTSPlugins("def")
```

## Configuration

Configuration set in the `rucio.cfg`. To use a plugin (here named `policy_algorithm_1`),
modify the config to include the below field.
 To use multiple plugins, their names can be listed to make each plugin algorithm run in sequence.

```
[transfers]
fts3tape_metadata_plugins = policy_algorithm_1, policy_algorithm_2
```

Size constraints can be set with the below. (Default of 4096)
```
[transfers]
metadata_byte_limit = <byte limit of transfer metadata>
```

## Pre-built plugins
### Activity Based Transfer Priority

To assign archive priority based on activity, we include the `activity_hints` plugin.
This plugin assigns an integer priority between 0 and 100 based on the activity of the transfer in question.

Output of the plugin follows the form of
```json
{"archive_metadata": {"scheduling_hints": {"priority": 0}}}
```

The plugin requires no code modification or subclassing, all set up is done in the `rucio.cfg`.

To set up, update your configuration as follows:


```cfg
[transfers]
fts3tape_metadata_plugins = policy_algorithm

[tape_priority]
<Activity 1> = 100
<Activity 2> = 80
<Activity 3> = 50
```

Baseline priority for any activity not listed is `20`.

### Tape Collocation

In order to ensure data is placed on tape with similar data, a `tape_collocation` plugin can be used.
This plugin requires custom logic, but behaves as a generic plugin does.

The collocation can be set to up to 4 different levels, such that the passed `archive_metadata` contains the following:

```json
{
    "collocation_hints":{
        "0": Highest level of grouping,
        "1": Second Highest level,
        ...
        "3": Lowest level of grouping
    }
}
```

Writing the plugin is similar to any other plugin, it just requires that these 4 levels be filled for a given transfer.

If the `collocation` wrapper is used, this format is verified and put into the `collocation_hints`
 field of the transfer parameters.
 This is done below, where `find_level_hints` is an arbitrary function written for an experiment's needs:

```python
from rucio.transfertool.fts3_plugins import FTS3TapeMetadataPlugin

class ExperimentCollocationFTSPlugins(FTS3TapeMetadataPlugin):
    def __init__(self, policy_algorithm="def"):
        super().__init__(policy_algorithm)
        self.register(
            "policy_collocation_algorithm",
            func= lambda x: self._collocation(self._experiment_plugin, x)
        )

    def find_level_hints(self, level, hints):
        ...

    def _experiment_plugin(self, *hints):
        return {
                "0": self.find_level_hints(level=0, hints=hints),
                "1": self.find_level_hints(level=1, hints=hints),
                "2": self.find_level_hints(level=2, hints=hints),
                "3": self.find_level_hints(level=3, hints=hints),
            }

ExperimentCollocationFTSPlugins("def")
```
---
id: transfers-overview
title: Transfers Overview
---

Rucio has a set of daemons in charge of transfers between rucio storage elements
(RSE). Historically, these daemons were grouped under the name of `conveyor`,
so a big part of the documentation and source code still uses this naming
when referring to the transfer machinery.

Rucio doesn't execute the actual physical data movement between storage
elements. It relies on external tools for this scope. Currently, rucio supports
[fts3](https://fts3-docs.web.cern.ch/fts3-docs/docs/overview.html) and
[globus](https://www.globus.org/data-transfer). Rucio builds on top of these
"TransferTools" and provides additional services like recovery from a transfer
failure by using another copy from another storage element, multi-hopping
using multiple transfertools (or multiple instances of the same transfertool
type) and others.

## Daemon overview

The following transfer-related daemons exist in rucio, presented in the order
they intervene in a transfer lifecycle:

- **preparer**: a strongly recommended optional daemon which is required for
  many advanced usages, like multiple transfertools together.
  It is also required to be able to use throttler. If active, performs part
  of the source selection and path computation work instead of the submitter.
  For all new rucio installation, it is recommended to run this daemon and
  activate it by setting the `conveyor/use_preparer = True` configuration
  option.
- **throttler**: an optional daemon which can throttle request submissions
  to/from an RSE
- **submitter**, **stager**: perform the actual submission of transfers to the
  external transfertool. If used without preparer, also perform path computation
  and source replica selection. Stager is a specialized submitter for issuing
  stagein operations to tape archives.
- **receiver**: optional daemons which listens for events published into a
  queueing system (activemq) by the external transfertool and reacts to those
  events to mark transfers as successful or failed.
- **poller**: regularly polls the external transfertool for the status of
  pending transfers and marks them as successful/failed
- **finisher**: acts on successful or failed transfers. For example, by
  re-scheduling a new attempt.

The minimal list of daemons needed for transfer execution is:
`submitter`, `poller` and `finisher`.

# Lifecycle of transfer requests

There is no user-facing way to schedule a transfer. All transfer requests are
created internally by rucio as result of rule evaluations. The lifetime of a
rucio transfer is thus strongly bound to the rule which created it.
Hereafter is a simple example which gives the intuition of how rucio proceeds
with a replication/transfer of a file as part of a rule.

In the rest of this example we'll assume the following 4 rucio storage
elements:

```text
┌──────┐   5    ┌──────┐
│      │◄──────►│      │
│ RSE1 │        │ RSE2 │
│      │    ┌──►│      │
└──────┘    │   └──────┘
   ▲        │
   │100     │3
   ▼        │
┌──────┐    │   ┌──────┐
│      │◄───┘   │      │
│ RSE3 │        │ RSE4 │
│      │◄──────►│      │
└──────┘   2    └──────┘
```

The numbers on the arrows represent the administrative cost which is set
by the rucio administrator. Cost is unidirectional, but, in this example,
we assume that the cost was configured identical in both directions.
For example:

```shell
rucio-admin rse add-distance --distance 5 RSE1 RSE2
rucio-admin rse add-distance --distance 5 RSE2 RSE1
# Note: before rucio 1.30 (as a consequence: also in the current LTS release 1.29),
# the --ranking option was used for the same purpose. The --distance option
# could still be set and was mentioned in documentation alongside --ranking
# but was completely ignored by rucio.
# On 1.29, you'll have to use the following command:
rucio-admin rse add-distance --ranking 5 RSE1 RSE2
rucio-admin rse add-distance --ranking 5 RSE2 RSE1
```

Assume a certain dataset `someScope:dsName`, which has two files
`someScope:file1` and `someScope:file2`, and both files are located on `RSE1`.

The destination of the transfer will be decided on the rule evaluation phase,
For example the user adds a rule to ensure that rucio maintains two copies
for each of the files on any of the RSEs.

```shell
rucio add-rule someScope:dsName 2 '*'
```

The rule evaluation mechanism detects that a copy is already available
on RSE1, but one additional copy is needed to respect the rule requirements of
2 copies. It will thus create a transfer request to one of the other 3 rses.
As of time of writing, the selection of the destination is random as
long as it respects the RSE expression. Here, `*` matches any RSE.
For the seek of the example, lets assume that RSE4 was selected.

The rule evaluation mechanism will then create two transfer requests, which
will be picked by the transfer machinery. Depending on the configuration value
`conveyor/use_preparer`, the transfer will be either handled by the `preparer`
or by the `submitter` directly.

At this stage, the transfer machinery finds all the possible sources. It
filters out the ones which don't match different rule criteria (for example:
source RSE expression) and administrative constraints (for example:
skip blocklisted RSEs). It then computes the paths. In the previous example,
the path `RSE1 -> RSE2 -> RSE3 -> RSE4` will be picked due to cost constraints.
Note that it's possible to make rucio prefer shorter parts by setting the RSE
attribute `hop_penalty`, or the global configuration value with the same name.
For more details about how a source is selected, refer to the [Preparer](transfers_preparer.md)
documentation.

The path will be then submitted to the transfertool either in its integrity,
if transfertool supports multi-hopping, or in multiple iterations.

The final steps are for the `receiver` or `poller` (refer to [Poller/Receiver](transfers_poller_receiver.md))
to monitor the transfer's
completion in transfertool and `finisher` to mark the transfers as completed.
We only described here a simple case, when the transfer is successful on the
first try. In case of errors, multiple transitions are possible between
different daemons. Check the following request state transition diagram
for a more detailed view:

![Request State Transition Chart](/img/request_state_transition_chart.svg)
---
id: configure-rucio-globus
title: Configure Rucio To Use Globus Online as a Transfer Tool
sidebar_label: Configure Rucio To Use Globus Online as a Transfer Tool
---

This document walks through an example configuration of Rucio to use Globus
Online as a transfer tool. There are four configuration points shown here:
registration of your application with Globus, RSE setup (properties and
parameters), the Rucio configuration file rucio.cfg and the Globus configuration
file config.yml.

Use of both Globus Server endpoints and Globus Personal endpoints has been
tested with the below approach. Creation of the Globus endpoints is outside the
scope here. Some knowledge of Rucio setup and familiarity with Globus
configuration is presumed.

## Register Application with Globus

Using Globus Online as a transfer tool requires
[registering](https://developers.globus.org) the client application with Globus
Online. Be sure to select Native App and include a scope for
urn:globus:auth:scope:transfer.api.globus.org:all. Once you have the Client ID
you’ll need to install the globus sdk and run the below Python code to obtain a
refresh token.

There is a [helpful
walk-through](https://globus-sdk-python.readthedocs.io/en/stable/tutorial.html)
that goes into more detail around OAuth and token retrieval.

Obtain a refresh token to access Globus resources:

```py
# obtain authorization code
import globus_sdk
CLIENT_ID = '' # your client ID obtained from registering application
client = globus_sdk.NativeAppAuthClient(CLIENT_ID)
client.oauth2_start_flow(refresh_tokens=True)
client.oauth2_get_authorize_url() # Use URL returned here to obtain the authorization
AUTH_CODE = '' # Use authorization code returned by authenticating to Globus Online

# use the authorization code to create a refresh token
token_response = client.oauth2_exchange_code_for_tokens(AUTH_CODE)
refresh_token = token_response.by_resource_server['transfer.api.globus.org']['refresh_token']
```

## RSE Setup

Below shows a typical setup for a test RSE. Options for CLI given when
supported.

The following code will create a non-determinisic RSE.

Python:

```py
# set up the target non-deterministic rse (TEST_RSE)
from rucio.client.rseclient import RSEClient
rseclient = RSEClient()
rse_name = 'TEST_RSE' # rse name MUST BE UPPER CASE
rse_properties = {
  'ASN': 'ASN',
  'availability': 7,
  'deterministic': False,
  'volatile': False,
  'city': 'Upton',
  'region_code': 'DE',
  'country_name': 'US',
  'continent': 'NA',
  'time_zone': 'America/New_York',
  'ISP': None, 'staging_area': False,
  'rse_type': 'DISK',
  'longitude': 40.868352,
  'latitude': -72.878871
}
r = rseclient.add_rse(rse_name, **rse_properties) # r is true on success
```

CLI alternative: RSE creation not supported at time of writing of this document
as there is no way to pass the properties.

The following code creates a schema to connect to Globus for the RSE created
above.

Python:

```py
from rucio.client.rseclient import RSEClient
rseclient = RSEClient()
rse_name = 'TEST_RSE' # rse name MUST BE UPPER CASE
# Globus scheme
prefix = '/~/scratch-space/' # Be sure to use a relative path for your endpoint
params = {
  'scheme': 'globus',
  'prefix': prefix,
  'impl': 'rucio.rse.protocols.globus.GlobusRSEProtocol',
  'third_party_copy': 1,
  'domains': {
    "lan": {"read": 1,"write": 1,"delete": 1},
    "wan": {"read": 1,"write": 1,"delete": 1}
  }
}
p = rseclient.add_protocol(rse_name, params) # p is true on success
```

CLI alternative: (the hostname value is required for the CLI command but is
arbitrary as it is ultimately not used in the scheme):

```bash
> rucio-admin rse add-protocol --scheme 'globus' --prefix '/~/scratch-space' \
    --impl 'rucio.rse.protocols.globus.GlobusRSEProtocol' --domain-json \
    '{"wan": {"read": 1, "write": 1, "third_party_copy": 1, "delete": 1}, \
    "lan": {"read": 1, "write": 1, "third_party_copy": 1, "delete": 1}}' \
    --hostname 'globus_online' TEST_RSE
```

The following code sets some attributes for the RSE.

Python:

```py
from rucio.client.rseclient import RSEClient
rseclient = RSEClient()
rse_name = 'TEST_RSE' # rse name MUST BE UPPER CASE

result = rseclient.add_rse_attribute(rse = rse_name, \
  key = 'naming_convention', value = 'bnl') # This is the value for relative SURL
result = rseclient.add_rse_attribute(rse = rse_name, \
  key = 'globus_endpoint_id', value = 'd6ae63d8-503f-11e9-a620-0a54e005f849')
result = rseclient.add_rse_attribute(rse = rse_name, \
  key = 'istape', value = False)
```

CLI alternative:

```bash
> rucio-admin rse set-attribute --rse TEST_RSE --key naming_convention --value bnl
> rucio-admin rse set-attribute --rse TEST_RSE --key globus_endpoint_id --value d6ae63d8-503f-11e9-a620-0a54e005f849
> rucio-admin rse set-attribute --rse TEST_RSE --key istape --value false
```

## Rucio Configuration File

The Rucio configuration file rucio.cfg should contain the following for the
conveyor mechanism. More schemes can be included but globus is required. You
only need the file scheme if you plan on using the upload method for
replicas. If the transfertype value is bulk Rucio will bundle many files into a
transfer task. If single then each file will be submitted on individual transfer
tasks.:

```bash
[conveyor]
scheme = file,globus
transfertool = globus
transfertype = bulk
globus_auth_app = MyGlobusAuthApp
```

globus_auth_app is the application given in config.yml (see below)

## Globus Configuration File

The Globus configuration file ./lib/rucio/transfertool/config.yml is a file of
YAML syntax and should include at minimum the registered application name, the
client ID and refresh token:

```yml
globus:
  apps:
    RucioGlobusXferNativeApp:
      client_id: a758...
      refresh_token: Agjo...
```
---
id: transfers-throttler
title: Transfers Throttler
---

As the name suggests, `conveyor-throttler` (transfer throttler) is used to
protect the transfertools from overload by limiting the number of submitter
transfers at any particular moment of time.

Preparer is required to be able to run throttler. See the preparer documentation
on how to activate it.

The rucio administrator must manually configure throttling rules.  As of time
of writing, there is no CLI option in rucio-admin to do it. Rules have to be
added using the rucio core functions directly from a rucio node.

**Warning**: only set transfer limits if throttler is running. Otherwise,
preparer will transition transfers to the `waiting` state, but nobody will
consume the queue of waiting transfers. Leading to these transfers never
being executed.

```python
from rucio.core.request import set_transfer_limit, list_transfer_limits

list(list_transfer_limits())
set_transfer_limit("RSE1", max_transfers=100)
```

The previous code snippet will create (or update) the 'destination' throttling
rule for transfers towards RSE1. A maximum of 100 transfers will be allowed
at a time towards RSE1. Note that "RSE1" here is an RSE expression, not
a name, complex RSE expressions can be used in rules. If more than one rule
applies to a specific RSE, the more restrictive condition applies.

Throttler supports some advanced throttler techniques. Some of them are:
- source throttling
- grouping of files from the same dataset together (grouped_fifo strategy)

These techniques can be costly on the database and were not extensively tested.
The only technique we use in production is destination throttling.
---
id: transfers-submitter
title: Transfers Submitter
---

The `conveyor-submitter` (transfer submitter) is the rucio daemon in charge
of submitting transfers for execution by an external third-party-copy
trasfertool. As input, it gets the transfer requests in `queued` state and
performs multiple tasks on them with the end goal of submitting the actual
transfer to one or more transfertools.

Historically, submitter was the main entry point into the transfer machinery
instead of the preparer. Because of that, many old rucio installations don't
run the preparer daemon. To allow running in such configuration, submitter
automatically detects if preparer is running and, if it's not running, will
perform the "Source replica selection". See the preparer documentation for more
details.

The Submitter:

- (if preparer is not running) selects the source replica
- computes the path for the selected replica
- checks transfertool-specific RSE attributes
- creates intermediate hops for multi-hop transfers
- generates the full URIs to be used by the transfertool
- performs the actual submission of the transfer

If the configuration value `conveyor/filter_transfertool` is set, submitter
will only work on transfers having the `transfertool` attribute set to the
correct value. This database field is filled by the preparer, so preparer is
required for multi-transfertool deployments.

To verify if a path cen be submitted by any of the transfertools configured
in `conveyor/transfertool`, transfertool-specific RSE attributes are used. For
example, the fts3 transfertool requires an `fts` RSE attribute with a list of
fts servers; while the globus transfertool requires the `globus_endpoint_id`
attribute on both source and destination RSE.

If a path can be submitted, all missing hops are created into the database,
and submitter goes to the submission step, which is straightforward and
consists of calling the transfertool with the correct arguments.
---
id: transfers-preparer
title: Transfers Preparer
---

`conveyor-preparer` (transfer preparer) is the main entry point into the
transfer machinery. It leverages topological information to pick the best source
replica for the transfer. It also decides if the transfer has to be handled by
the throttler or not. For all new rucio installations, it is recommended to run
this daemon and activate it by setting the `conveyor/use_preparer = True`
configuration option.

Preparer:

- finds all source RSEs which have a replica of the desired file
- filters out the source RSEs which don't respect administrative constraints
- ensures protocol compatibility between sources and destination
- performs path computations to find the best sources
- transitions the transfer request either to a `Waiting` or to a `Queued` state

# Source replica selection

One of the main jobs done by the preparer is the selection of the replica
to be used as a transfer sources. For that, it relies on multiple RSE
attributes and on the configured protocols. This section provides a summary
of what configuration parameters can influence the preparer at this stage.

We will use the notation `section/option` to speak about a configuration
value to be set in `rucio.cfg` like this:

```text
[section]
option = value
```

The preparer will start by retrieving all the possible sources from the
database.

In the following step, the preparer will skip all sources which don't
respect the administrative constraints. For example, it will ignore source
RSEs with `availability_read=False` (unless the preparer is run with
`--ignore-availability`). It also respects the `restricted_read` and
`restricted_write` RSE attributes for the source and the destination.

Some request attributes will impact the source selection. For example, preparer
will skip source RSEs which don't match the `source_replica_expression` or
`allow_tape_source` conditions. It will also ignore requests witch require a
`transfertool` that this preparer cannot use. The request attributes are
either inherited from the rule, or set by another transfer daemon
(for example: preparer)

The next step is to perform the path computation. At this stage, preparer
uses the distance between RSEs to perform shortest-path computations.
Each hop, even for single-hop transfers, must respect the protocol
compatibility between the source of the hop and its destination. The
[SCHEME_MAP](https://github.com/rucio/rucio/blob/1b8ca368523d13fd11bc0b32c14528f2fcec778b/lib/rucio/common/constants.py#L48)
constant defines the compatibility between protocols. Only protocols with
non-zero `third_party_copy_read` will be considered for source RSEs, ordered
by priority. Same for the destination: `third_party_copy_write` is used.

**Note**: distances between RSEs which are set by the administrator via

```shell
rucio-admin rse add-distance --distance 1 RSE1 RSE2
# Note: before rucio 1.30 (as a consequence: also in the current LTS release 1.29),
# the --ranking option was used for the same purpose. The --distance option
# could still be set and was mentioned in documentation alongside --ranking
# but was completely ignored by rucio.
# On 1.29, you'll have to use the following command:
rucio-admin rse add-distance --ranking 1 RSE1 RSE2
```

Once all valid paths are found, after all the filtering done previously,
the paths are ordered using the following simple
[rules](https://github.com/rucio/rucio/blob/608c9b1dc834f07396cc49dfcbc3daa613b61d56/lib/rucio/core/transfer.py#L905)
:

- the source ranking is compared first. Source ranking is an integer which is
  decreased each time a particular source is found to have an issue to perform
  this particular transfer. It is thus equal to 0 at first try, and decreased
  at transfer failure before re-trying the transfer. This ensures that
  problematic sources are much less likely to be reused.
- On equal source ranking, the RSE type is checked. Disk sources are preferred
  over tape.
- On equal source RSE type, the distance between the source RSE and the
  destination RSE is compared.
- On equal distance, we prefer single-hop paths.
---
id: special_interest_groups
title: Special Interest Groups
---

Rucio Special Interest Groups (SIG) serve the purpose to offer a forum for interested users,
operators, and developers to discuss and plan the evolution of a specific part of Rucio.
The topic of a SIG needs to be well-defined and the community interest on the topic
needs to be above a threshold to justify the creation of a SIG, instead of covering the
topic just within the weekly Rucio meeting. A SIG topic can involve one or multiple
Rucio components, or even the entire system. SIGs are open to any interested community
member.

## Structure

Each SIG must define

  - A well-defined objective of the group (Creation of a report, Development of
    a functionality, ...)
  - An expected end-date
  - Means to achieve the objective, such as regular meetings, a mailing-list, mattermost
    channel, a workshop, ...
  - A convener

## Current Special Interest Groups

| Name                                          | Start    | est. End | Convener                                               |
| --------------------------------------------- | -------- | -------- | ------------------------------------------------------ |
| [Metadata](sig_metadata.md)                   | Jul-2021 | Dec-2026 | [Rob Barnsley](https://github.com/robbarnsley)         |
| [Tokens](sig_tokens.md)                       | Sep-2023 | Mar-2026 | [Dimitrios Christidis](https://github.com/dchristidis) |

## Past Special Interest Groups

| Name                                          | Start    | End      | Convener                                               |
| --------------------------------------------- | -------- | -------- | ------------------------------------------------------ |
| [Quality of Service](sig_qualityofservice.md) | Jul-2021 | Dec-2023 | [Doug Benjamin](mailto:douglas.benjamin@cern.ch)       |
---
id: qos_rse_config
title: QoS RSE Configuration
---

## QoS RSE Configuration

The following steps show how to configure an RSE to use the Rucio storage-managed QoS feature (with two classes, disk-buffer and tape).  Rucio now supports a site's ability to set how long data lives on a tape or disk-buffer at the RSE level.  Data is pinned with a value (in seconds) at the time of injection/creation/buffering allowing data site operators to better tune data persistence and movement.

### How to Configure an RSE for QoS

1. Set the following RSE attributes:

    ```bash
    staging_required: True
    maximum_pin_lifetime: 345600 (in seconds)
    staging_buffer: {{value}} # this value can be the name of the RSE.
    ```

Upon executing `rucio-admin rse info <RSE>` command one should see something like the following in settings:

    ```bash
                                    .
                                    .
    Attributes:
    ===========
      maximum_pin_lifetime: 345600
      staging_buffer: <RSE>
      staging_required: True
                                    .
                                    .
    ```

2. Either values of `DISK` or `TAPE` are supported for the RSE setting `rse_type`.  However workflows using PanDA job brokerage require this value to be `DISK`.

Upon executing `rucio-admin rse info <RSE>` command one should see something like the following in settings:

    ```bash
                                    .
                                    .
    Settings:
    =========
      rse: <RSE>
      rse_type: DISK
                                    .
                                    .
    ```
---
id: kubernetes
title: Setting up Rucio on Kubernetes
sidebar_position: 2
---

## Helm charts

Helm charts help you define, install, and upgrade Kubernetes applications. In [this repository](https://github.com/rucio/helm-charts), you can find Helm charts for the major different components of Rucio.

## Tutorial

A tutorial for using Rucio with Kubernetes can be found [here](https://github.com/rucio/k8s-tutorial/blob/master/README.md)

## Example of Kubernetes deployment

[KM3NeT](https://www.km3net.org) runs Rucio on Kubernetes via [flux](https://fluxcd.io).
The configuration deployed on their Rucio server can be found [here](https://git.km3net.de/rucio/rucio-deployment).
The documentation of how this (specific) instance can be set up can be found in the [docs](https://rucio.pages.km3net.de/rucio-documentation/installation/)

## Bootstrapping the database

By default, no database is initialized when a new Rucio installation is set up.
An init container can be used to bootstrap the database on a Kubernetes cluster:

1. Create a `init-pod.yaml` file as in the snippet below, replacing the `<PASSWORD>` with the secret needed to connect to the database:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init
  labels:
    app: rucio
spec:
  restartPolicy: Never
  containers:
    - name: init
      image: rucio/rucio-init:latest
      imagePullPolicy: Always
      env:
        - name: RUCIO_CFG_DATABASE_DEFAULT
          value: postgresql://rucio:<PASSWORD>@postgres-postgresql/rucio
        - name: RUCIO_CFG_DATABASE_SCHEMA
          value: test
        - name: RUCIO_CFG_BOOTSTRAP_USERPASS_IDENTITY
          value: tutorial
        - name: RUCIO_CFG_BOOTSTRAP_USERPASS_PWD
          value: secret1R
        - name: RUCIO_PRINT_CFG
          value: "true"
```

2. Start the init pod:

```
kubectl apply -f init-pod.yaml
```

For more information:
- [Rucio Kubernetes tutorial repository README](https://github.com/rucio/k8s-tutorial/blob/master/README.md)
- [Init container documentation](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)


## Upgrading the database

After the Rucio version has been upgraded, there might be database changes
that have to be done. These can be done on the `rucio-server` pod of the cluster and
are performed with the [Alembic](http://alembic.zzzcomputing.com/en/latest/) tool.

The alembic.ini template can be found
[here](https://github.com/rucio/rucio/blob/master/etc/alembic.ini.template).
Fill in the correct values before transferring the file to the `rucio-server` pod:

```
cat alembic.ini | kubectl exec -i rucio-server-<pod identifier> --container rucio-server -- tee /tmp/alembic.ini
```

Open a bash prompt on the pod

```
kubectl exec rucio-server-<pod identifier> --container rucio-server -it -- bash
```

Then perform the upgrade with the Alembic tool as described in the database [documentation](./database#upgrading-and-downgrading-the-database-schema)
---
id: notifications
title: Notifications
---

## Notifications

Rucio generates several types of notifications, such as for rule state changes, transfer requests, etc.
These notifications are primarily useful to other systems for synchronisation purposes, e.g., notifying a workflow management system that a dataset has finished transferring or has been deleted.

### Rucio Notifications

The events generated by Rucio are categorized into different event types. Each event type has a different payload.

| Context                                                                                    | Event Type      | Payload example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|--------------------------------------------------------------------------------------------|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| While adding a DID, if the DID type is a Container                                         | CREATE_CNT      | ```{'account': 'ruciouser', 'scope': 'data', 'name': 'this.is.a.dataset', 'expired_at': '2023-07-05 14:51:55.378549' or None}```                                                                                                                                                                                                                                                                                                                                                                     |
| While adding a DID, if the DID type is a DataSet                                           | CREATE_DTS      | ```{'account': 'ruciouser', 'scope': 'data', 'name': 'this.is.a.dataset', 'expired_at': '2023-07-05 14:51:55.378549' or None}```                                                                                                                                                                                                                                                                                                                                                                     |
| Submit transfer requests on destination RSEs for data identifiers.                         | transfer_status | ```{'request-id': '414ac0cd34844a03a184ce4b0f640dd1', 'request-type': 'transfer', 'scope': 'data', 'name': 'this.is.a.test.file', 'dst-rse-id': '207228dfe3b246ab9d8b199c8358e864', 'dst-rse': 'TESTRSE', 'state': 'QUEUED', 'retry-count': 2, 'rule-id': '8f06b8ede5024e9fb8c1aa4d761627f0', 'activity': 'User Subscription', 'file-size': 1000000, 'bytes': 1000000,'checksum-md5': '9e107d9d372bb6826bd81d3542a419d6', 'checksum-adler': '08880271', 'queued_at': '2023-07-05 14:51:55.378549'}``` |
| Schedule removal of the entry from the DIDs table                                          | INCOMPLETE      | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Delete empty DIDs where the DIDType is DATASET                                             | ERASE           | ```{'scope': 'data', 'name': 'this.is.a.dataset','account': 'root'}```                                                                                                                                                                                                                                                                                                                                                                                                                               |
| When the replication rule transitioning into OK_STATE for DIDs which are closed            | RULE_OK         | ```{'scope': 'data','name': 'this.is.a.dataset','rule_id': '18432d3c5aad43e3a4ed10ff61e5d1ce','progress': 30,'vo': 'testvo'}```                                                                                                                                                                                                                                                                                                                                                                      |
| For replication replication rule, for each 10% of progress                                 | RULE_PROGRESS   | ```{'scope': 'data','name': 'this.is.a.dataset','rule_id': '18432d3c5aad43e3a4ed10ff61e5d1ce','progress': 30,'vo': 'testvo'}```                                                                                                                                                                                                                                                                                                                                                                      |
| When the replication rule transitioning into OK_STATE for each DATASET covered by the rule | DATASETLOCK_OK  | ```{'scope': 'data','name': 'this.is.a.dataset','rse': 'TESTRSE', 'rse_id': '3ddb29c028574f7288595711bc83f3e6''vo': 'testvo'}```                                                                                                                                                                                                                                                                                                                                                                      |
| When a transfer has been completed                                                         | transfer-done   | ```{'scope': 'data','name': 'this.is.a.dataset', 'src-rse': 'TESTRSE', 'dst-rse': 'TESTRSE', 'activity': 'User Subscription', 'request-id': None, 'transfer-id': None, 'created_at': '2023-07-05 14:51:55.378549', 'transferred_at': '2023-07-05 14:51:55.378549'}```                                                                                                                                                                                                                                                                                                                                         |
| When a transfer has been fail                                                              | transfer-failed | ```{'scope': 'data','name': 'this.is.a.dataset', 'src-rse': 'TESTRSE', 'dst-rse': 'TESTRSE', 'activity': 'User Subscription', 'request-id': None, 'transfer-id': None, 'created_at': '2023-07-05 14:51:55.378549', 'transferred_at': '2023-07-05 14:51:55.378549'}```                                                                                                                                                                                                                                                                                                                                         |
| When a deletion has been successful                                                        | deletion-done   | ```{'scope': 'data','name': 'this.is.a.dataset', 'rse': 'TESTRSE','request-id': None, 'url': None,'bytes': 1000000, 'created_at': '2023-07-05 14:51:55.378549'}```                                                                                                                                                                                                                                                                                                                                    |
| When a deletion has been fail                                                              | deletion-failed |  ```{'scope': 'data','name': 'this.is.a.dataset', 'rse': 'TESTRSE','request-id': None, 'url': None,'bytes': 1000000, 'created_at': '2023-07-05 14:51:55.378549'}```                                                                                                                                                                                                                                                                                                                                   |
---
id: k8s_guide
title: A Kubernetes tutorial
---
# Getting Started
:::warning[DISCLAIMER]
This tutorial contains some parts that are CERN-specific. <br/>
The tools utilised to set up and to run the cluster are chosen accordingly. Feel free to use your own.
:::

In order to efficiently manage the cluster and implement continuous delivery, the following tools are required.

- **Kubernetes**: `kubectl`
([Reference Documentation](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)).
- **Helm Charts**
([Reference Documentation](https://helm.sh/docs/intro/install/#from-script)). <br/>
To install Helm, execute:
  ```bash
  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3

  chmod 700 get_helm.sh

  ./get_helm.sh
  ```
  Also, add the `bitnami` repo:
  ```helm
  helm repo add bitnami https://charts.bitnami.com/bitnami
  ```

  So that we can search the repo with `helm search repo bitnami`.

- **Continuous delivery**: `flux`
([Reference Documentation](https://fluxcd.io/flux/get-started/)).
  -  [Install the Flux CLI](https://fluxcd.io/flux/installation/#install-the-flux-cli)
      ```sh
      curl -s https://fluxcd.io/install.sh | sudo bash
      git status
      git add .
      git commit -m "first commit"
      git push
      ```
  -  [Bootstrap Flux](https://fluxcd.io/flux/cmd/flux_bootstrap_gitlab/)
Then setup the flux in the gitlab repo (we recommend to clone it via https), run the command
      ```sh
      flux bootstrap gitlab \
        --owner=<git-user-or-group-name> \
        --repository=<repository-name> \
        --branch=<main-branch> \
        --path=<path-where-to-sync> \
        --hostname=gitlab.cern.ch \
        --deploy-token-auth
      ```
The git deploy token will be requested as part of the auth process.

:::tip[Why use ` --deploy-token-auth`]

When using --deploy-token-auth, the CLI generates a GitLab project deploy token and stores it in the cluster as a Kubernetes Secret named flux-system inside the flux-system namespace. [[Source](https://fluxcd.io/flux/installation/bootstrap/gitlab/#gitlab-personal-account)].
:::

- **(optional) Monitoring**: `k9s`. <br/>
  Get the binary from [here](https://github.com/derailed/k9s/releases) looking for the proper dist; extract, and move to `/usr/local/bin/`.

# Setting up the CERN infrastructure
## Creating the Rucio cluster on Openstack
Please refer to the [documentation](https://kubernetes.docs.cern.ch/docs/getting-started/).

There are two kinds of operative components within the cluster: the *master* and the *worker* nodes:

- The master node hosts the Kubernetes control plane and manages the cluster, including scheduling and scaling applications and maintaining the state of the cluster.
- The worker nodes are responsible for running the containers and executing the workloads.

It is recommended to choose a master node with a sufficient amount of memory, e.g. `m2.large`.

The minimum working configuration consists of 1 master node and 5 worker nodes.

## Create the database with DBOD
Create a DataBase On Demand (DBOD) using the [DBOD dashboard](https://dbod.web.cern.ch/pages/dashboard);  `postgres` is highly recommended.
- The operator will have to wait for the db to be approved. Will get a mail and will receive admin credentials (to be changed) as shown below.

Additional resources:
- https://github.com/vre-hub/vre/wiki/Software-components#2-database
- https://codimd.web.cern.ch/s/ZFIkp7PWG#2-Database-configuration

Once the db creation is finalised, we can use `psql`, to set up the credentials.
In the following example, we chose the name `rucioitdb` for the database; in general, the naming of the DBOD instance is the operator's choice:
```sh
dnf install postgresql-server

psql -h dbod-rucioitdb.cern.ch -U admin -p <port> -c '\password'
```
Details about which port to connect to, etc can be found on the DBOD dashboard.

While inside the db instance, we can create the `rucio` user and db, and we can assign admin privileges to it:
```sql
CREATE ROLE rucio WITH LOGIN PASSWORD 'xxx';
ALTER GROUP admin ADD USER rucio;
CREATE DATABASE rucio;
```

Then, modify the `pg.hba.conf` configuration file on the DBOD dashboard with the following values, to allow `rucio` to access the db:

```
host	rucio		rucio		0.0.0.0/0	md5
```

### Bootstrapping the database
To bootstrap the db, there are two possibilities:
#### 1. The [Rucio DB init container](https://github.com/rucio/containers/tree/master/init)

```sh
docker run --rm \
  -e RUCIO_CFG_DATABASE_DEFAULT="postgresql://rucio:xxx@dbod-rucioitdb.cern.ch:<port>/rucio" \
  -e RUCIO_CFG_BOOTSTRAP_USERPASS_IDENTITY="admin" \
  -e RUCIO_CFG_BOOTSTRAP_USERPASS_PWD="xxx" \
  rucio/rucio-init
```

This command will setup all the necessary tables in the db, and additionally will create a rucio admin user. The admin user will be used when setting up rucio. ***Please take note of the username and passw.***

:::tip
Notice the syntax of `RUCIO_CFG_DATABASE_DEFAULT="postgresql://<db-user>:<passw>@<dbod-url>:<dobd-port>/<db-name>"`
:::
#### 2. The bootstrapping pod
Create a `init-pod.yaml` file and apply it as specified in the readme of the [k8s_tutorial](https://github.com/rucio/k8s-tutorial/blob/master/README.md), replace `<PASSWORD>` with the secret needed to connect to the database:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init
  labels:
    app: rucio
spec:
  restartPolicy: Never
  containers:
    - name: init
      image: rucio/rucio-init:latest
      imagePullPolicy: Always
      env:
        - name: RUCIO_CFG_DATABASE_DEFAULT
          value: postgresql://rucio:<PASSWORD>@postgres-postgresql/rucio
        - name: RUCIO_CFG_BOOTSTRAP_USERPASS_IDENTITY
          value: admin
        - name: RUCIO_CFG_BOOTSTRAP_USERPASS_PWD
          value: xxx
        - name: RUCIO_PRINT_CFG
          value: "true"
```

And then apply the pod config:

```
kubectl apply -f init-pod.yaml
```
:::warning
Please notice that in this case, the various credentials will have to be properly stored as secrets. See the Managing Secrets section for more information.
:::

## Creating a LanDB set

>In order to [protect devices connected to the CERN network](https://security.web.cern.ch/services/en/firewall.shtml) from the regular attacks initiated from off-site, **incoming connections to all CERN devices are blocked** in the CERN outer perimeter firewall by default. In addition, source ports **0-1023/TCP and 0-1023/UDP (except 500/UDP) are blocked by default** for outgoing connections. Thus, users can initiate client applications (on so-called higher ports) but not expose server processes.

To comply with the CERN security rules, we need to use the so-called LANDB sets, where the firewall has static openings automatically set up. Usually, such sets are used for redundancy or large, homogeneous services. These sets are either managed by the Computer Security Team or by the service managers themselves.

Create a [new LanDB set](https://landb.cern.ch/portal/sets/create), following the recommendations:
1. Type: Interdomain
2. Network Domain: GPN
3. Responsible: `<your-egroup>`
4. Description: use the following fields:
```
OPENSTACK_PROJECT=cc059d57-6e98-4688-a3be-aae2b451868b,<your-openstack-project-ID>
```

:::tip[LoadBalancer tip]
The ID `cc059d57-6e98-4688-a3be-aae2b451868b` will allow the LoadBalancer as a Service (LBaaS) instance to [assign LoadBalancers to this set](https://clouddocs.web.cern.ch/networking/load_balancing.html#adding-load-balancer-to-landb-sets). In the specific case of the COMPASS rucio instance, the `openstack-landb-set-access` is being set as a member of the `rucio-it-admins` egroup.
Please refer to the LoadBalancers section for more information.
:::
![image](/img/landb-set-create.png)

# Populating the cluster
There are four main components that need to be installed in order to have the Rucio cluster operative:
1. Rucio Servers
2. Rucio Authentication
3. Rucio Daemons
4. Rucio Web UI

The following sections are based on the deployment of the [COMPASS Rucio instance](https://gitlab.cern.ch/rucio-it/flux-compass), within the CERN infrastructure.
## Managing secrets
### Sealed-Secrets
[Reference Documentation](https://github.com/bitnami-labs/sealed-secrets?tab=readme-ov-file#installation).
[Reference Helm chart](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/master/sync/sealed-secrets.yaml?ref_type=heads).

A very efficient way of managing secrets in the cluster is Bitnami's Sealed-Secrets. Install the Helm chart by executing:
```sh
helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets

kubectl create namespace sealed-secrets

kubectl apply -f sealed-secrets.yaml
```

An example of the `sealed-secrets.yaml` configuration file is provided below:
```yaml sealed-secrets.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: sealed-secrets
  labels:
    name: sealed-secrets
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  interval: 5m
  url: https://bitnami-labs.github.io/sealed-secrets
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: sealed-secrets
  namespace: sealed-secrets
spec:
  releaseName: sealed-secrets
  interval: 5m
  chart:
    spec:
      sourceRef:
        kind: HelmRepository
        name: sealed-secrets
        namespace: sealed-secrets
      chart: sealed-secrets
      interval: 5m
      version: 2.15.3
  values:
    fullnameOverride: "sealed-secrets-controller"
```

Notice that `yaml` files can be concatenated thanks to the `---` separator.

### Creating secrets for the cluster
[Reference secrets generation scripts](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/master/scripts?ref_type=heads).

The creation of secrets can be based on the following process:
```sh
#!/bin/bash

HELM_RELEASE="rucio-servers"
RAW_SECRETS_SERVERS="/root/compass-rucio-certs/servers"
# kubeseal controller namespace
CONTROLLER_NS="sealed-secrets"
CONTROLLER_NAME="sealed-secrets-controller" # This can be checked in k8s/Services

# rucio namespace
RUCIO_NS="rucio"

# Output dir
SECRETS_STORE="/root/flux-compass/SECRETS/"


# name of output secret to apply
OUTPUT_SECRET="ruciocompass-db.yaml"

kubectl create secret generic ${HELM_RELEASE}-server-hostcert --dry-run=client --from-file=${RAW_SECRETS_SERVERS}/hostcert.pem -o yaml | \
kubeseal --controller-name=${CONTROLLER_NAME} --controller-namespace=${CONTROLLER_NS} --format yaml --namespace=${RUCIO_NS} > ${SECRETS_STORE}/ss_${OUTPUT_SECRET}

kubectl apply -f ${SECRETS_STORE}/ss_${OUTPUT_SECRET}
```

Where:
- `HELM_RELEASE` is the name of the helm release that is being used.
- `RAW_SECRETS_SERVERS` is the path in which the files that will be used as secrets are stored
- `CONTROLLER_NS` is the namespace assigned to the secrets manager.
- `CONTROLLER_NAME` is the name of the secrets manager controller.
- `RUCIO_NS` is the namespace related to Rucio (in the COMPASS case, it is called `rucio`).
- `SECRETS_STORE` path to which the files containing the encrypted secrets will be stored. These files will be used by flux.

`kubectl create secret generic` Creates a secret from the specific file that needs to be transposed to a secret.

Then `kubeseal` perform the encryption and the encoding, saving the secret to a file stored in `SECRETS_STORE`.
Finally, `kubectl apply` applies the secret to the cluster. Please notice that this is eventually also achieved via flux, by pushing the secret file to the repository.

:::tip[Secret tip]
IF `kubeseal` returns an error, it's probably because it's not installed:
```sh
wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.18.0/kubeseal-0.18.0-linux-amd64.tar.gz
tar xfz kubeseal-0.18.0-linux-amd64.tar.gz
sudo install -m 755 kubeseal /usr/local/bin/kubeseal
```
Adjust the version and the build details according to your machine.
:::
The content of the secret will be of this type:
```yaml ss_rucio-servers-hostcert.yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: rucio-servers-hostcert
  namespace: rucio
spec:
  encryptedData:
    hostcert.pem: <encrypted secret>
  template:
    data: null
    metadata:
      creationTimestamp: null
      name: rucio-servers-hostcert
      namespace: rucio
```
## Rucio Servers
>The Rucio servers are the backbone of the Rucio system. They handle all core functionalities including data management, rule-based data replication, data placement, and monitoring. The servers ensure the integrity and availability of data across various storage systems.

First of all, create a namespace for rucio:

```sh
kubectl create namespace rucio
```
And consequently create a helm repository config file `rucio-helm-repo.yaml`.
```yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: rucio-charts
  namespace: rucio
spec:
  url: https://rucio.github.io/helm-charts
  interval: 1m
```
Then apply it using kubectl:
```sh
kubectl apply -f /path/rucio-helm-repo.yaml
```

At this point, in order to setup this service, four more steps are needed:
1. Produce the Helm chart.
2. Create the DB secret
3. Setup the LoadBalancers (LBs).
4. Produce the certificates related to the `landb-alias` used.
5. Add the certificates as secrets and mount them on the k8s pods.

### Produce the Helm chart
Please look at the [currently used one](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/master/sync/rucio-servers.yaml?ref_type=heads).
A few remarks:

- The db secret is being mounted on `config.database.default` in order to allow the user to correctly access it.
- `hostcert.pem`, `hostkey.pem` and `ca.pem` are automatically detected, so they don't need to be mounted. Nevertheless, the corresponding secrets must be created (see next sections).
- The GridCA cert is needed to verify all the other certs. Please look at this [git commit](https://gitlab.cern.ch/rucio-it/flux-compass/-/commit/688da7285833dafd1bbe25469f35c6059d7af24f) used to set the `GridCA` file in `/etc/grid-security/certificate`.
  - Together with this, is also necessary to setup the corresponding variable `RUCIO_CA_PATH`
- The `RUCIO_SSL_PROTOCOL` variable must be explicitly set.

Please notice that in order to have LBs produced, ***the Helm chart must be applied***.
This will come with several errors related to certificates, that will be fixed in the next steps.

### Create the DB secret
Create the file rucio-db.yaml to register the secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: rucio-db
  namespace: rucio
stringData:
  dbfullstring: postgresql://user:psw@dbod-instance.cern.ch:port/db
```
Add the secret using a script like this:
```sh
#!/bin/bash

# kubeseal controller namespace
CONTROLLER_NS="sealed-secrets"
CONTROLLER_NAME="sealed-secrets-controller" # This can be checked in k8s/Services

# rucio namespace
RUCIO_NS="rucio"

# Output dir
SECRETS_STORE="/root/flux-ams02/SECRETS/"

source_file_secret="/root/flux-ams02/SECRETS/tmp_local_secrets/rucio-db.yaml"

# name of output secret to apply
OUTPUT_SECRET="rucio-ams02-db.yaml"
cat ${source_file_secret} | kubeseal --controller-name=${CONTROLLER_NAME} --controller-namespace=${CONTROLLER_NS} --format yaml --namespace=${RUCIO_NS} > ${SECRETS_STORE}/ss_${OUTPUT_SECRET}
kubectl apply -f ${SECRETS_STORE}/ss_${OUTPUT_SECRET}
```


### Setup the LB
The minimal configuration for LoadBalancers is the following:
```yaml
    service:
      type: LoadBalancer
      port: 443
      targetPort: 443
      protocol: TCP
      name: https
```
With respect to the [original Helm chart](https://github.com/rucio/helm-charts/blob/7f8a7d9fb9cbcd01d645f24523a173c7f53fb101/charts/rucio-server/values.yaml#L33), that employs the Kubernetes native [`ClusterIP`](https://kubernetes.io/docs/concepts/services-networking/cluster-ip-allocation/) service, this configuration exploits the CERN native LBaaS, and exposes it over HTTPS.
This setup will trigger the `openstack-cloud-controller-manager` pod, and will automatically instantiate the requested LBs.

Please notice that in order to have LBs, one must request a quota change to the openstack project:
![image](/img/get-lbs.png)

To check the status of the LBs, the CLI command can be used:
```sh
openstack loadbalancer list
```

Then, a few attributes must be changed in order to obtain the necessary DNS:
```sh
openstack loadbalancer set --tag landb-set="RUCIO-IT" <lb-ID>
openstack loadbalancer set --tag landb-alias="compass-rucio.cern.ch" <lb-ID>
openstack loadbalancer set --description="compass-rucio.cern.ch" <lb-ID>
openstack loadbalancer set --tag main-user="rucio-it-admins" <lb-ID>
```

Now, the LB must be added to the LanDB set from the dashboard, under IP Addresses.
To do that, first retrieve the virtual IP address, `vip_address`, of the LB:
```sh
 openstack loadbalancer show <lb-ID> | grep vip_address
```

:::tip[LoadBalancers are slow and automatic]
The LB will be automatically added to the LanDB set allowed IPs.

IF this does ***not*** happen, follow the steps below:
:::

It will take a while (~10 minutes) for the LB hostname to come up online, so do not panic if `host <vip_address>` doesn't return anything immediately!

Retrieve the pointer to the LB, for instance:
```sh
host <vip_address>
222.xx.xxx.xxx.in-addr.arpa domain name pointer lbaas-xxx4a6c2-xxxx-xxxx-xxxx-59489ffd7xxx.cern.ch.
```
where `lbaas-xxx4a6c2-xxxx-xxxx-xxxx-59489ffd7xxx.cern.ch` represents the hostname of the LB.

Finally, add `lbaas-...cern.ch` to the list of allowed IP addresses in the LanDB set dashboard. Either the hostname or the `vip_address` used before can be used in the search bar.

### Produce the certificates related to the landb-alias
Read the [help page](https://ca.cern.ch/ca/Help/?kbid=021002) about Grid Host certs

Then, request a cert using [dedicated page](https://ca.cern.ch/ca/host/HostSelection.aspx?template=EE2Host&instructions=auto).
In the current example, a cert for `compass-rucio.cern.ch` (corresponding to the LB `landdb-alias`) was requested:

:::tip[Certs pro move]
> Create a [grid Host certificate](https://ca.cern.ch/ca/host/Request.aspx?template=ee2host) linked to both servers and auth domains. If you need, you can specify Subject Alternative Names (SANs) for your certificate, in DNS format in the SAN box. Then you'll have one cert to rule them all:
![image](/img/grid-host-certs.png)
:::

#### Create the `host`, `key`, `ca` and `GridCA` files

As recommended in the [documentation](https://ca.cern.ch/ca/Help/?kbid=024100), extract the certificate (which contains the public key) and the private key, using:
```sh
openssl pkcs12 -in compass-rucio.p12 -clcerts -nokeys -out hostcert.pem

openssl pkcs12 -in compass-rucio.p12 -nocerts -nodes -out hostkey.pem

chmod 400 hostkey.pem
chmod 644 hostcert.pem
```
Notice that to avoid protecting the key with a passphrase, the `-nodes` option is specified. Moreover, the appropriate permissions (`400`: read by owner, `644`: read by anyone, written by owner) for the extracted files are set.

These files have to be stored in the same path as described by the `RAW_SECRETS_SERVERS` variable used in the secrets creation.
##### Important information about `ca.pem` and `GridCA.pem`
An additional file, `ca.pem` (i.e. `CERN-bundle.pem`) must be created.

`CERN-bundle.pem` is a standard CA bundle provided by CERN, containing multiple CA certificates that CERN trusts. This file is used to verify the authenticity of other certificates within a secure communication process (e.g., SSL/TLS).

This file can be retrieved from lxplus:
```sh
cp /etc/pki/tls/certs/CERN-bundle.pem compass-rucio-certs/servers/ca.pem
```
The same goes for the `GridCA.pem` file:
```sh
cp /etc/grid-security/certificates/CERN-GridCA.pem compass-rucio-certs/servers/GridCA.pem
```
This file is used for the Certification Authority issuing SHA-2 certificates for Grid usage (user, host and robot certificates), in compliance with [EUGridPMA](https://www.eugridpma.org/) policies.

### Add the certificates as secrets and mount them on the k8s pods
Now it is possible to run something similar to the [servers script](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/master/scripts/rucio-servers-secret.sh?ref_type=heads) to add all the secrets to the cluster.

The output should be of this form:
```sh
Create and apply SERVER secrets
sealedsecret.bitnami.com/rucio-servers-server-hostcert configured
sealedsecret.bitnami.com/rucio-servers-server-hostkey configured
sealedsecret.bitnami.com/rucio-servers-server-cafile configured
.
.
.
```
The flux sync can be triggered by ***git pushing*** the content of the `SECRET` folder to the remote repository.
## Rucio Auth
[Reference Helm chart](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/master/sync/rucio-servers-auth.yaml?ref_type=heads).

>The authentication component of Rucio is responsible for securely verifying the identities of users and services attempting to access the Rucio system. It ensures that only authorised entities can perform actions within the system.

As for the servers, four steps are needed to deploy the authentication pod:
1. Produce the Helm chart.
2. Setup the LB.
3. Produce the certificates related to the `landb-alias` used.
4. Add the certificates as secrets and mount them on the k8s pods.

Based on the reference Helm chart, we can add a few remarks:

1. Explicitly mounting secrets for `hostcert`, `hostkey`, and `ca`, via `secretMounts`.
2. The `httpd_config` configuration block:
this block maps the certificate to an account name.
Setting up `x509` auth can be very tricky, please make sure that the [`grid_site_enabled`](https://github.com/rucio/containers/blob/b51bbceb5aab0a1e07d48845f295cbbb175bdcb9/server/rucio.conf.j2#L104) parameter is set on `True`. <br/>
This enables the `auth/x509_proxy` endpoint! <br/>
The endpoint can be tested by executing
    ```sh
    curl -k https://compass-rucio.cern.ch/auth/x509_proxy
    ```

Another important change is related to the LB attributes:
```sh
openstack loadbalancer set --tag landb-set="RUCIO-IT" <lb-ID>
openstack loadbalancer set --tag landb-alias="compass-rucio-auth.cern.ch" <lb-ID>
openstack loadbalancer set --description="compass-rucio-auth.cern.ch" <lb-ID>
openstack loadbalancer set --tag main-user="rucio-it-admins" <lb-ID>
```
Here, the `landb-alias` attribute and the `description` have been changed with respect to servers. The choice of the alias must comply with the naming used while creating the host certificates.

## Rucio Daemons
[Reference Helm chart](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/master/sync/rucio-daemons.yaml?ref_type=heads).<br/>
[Reference installation guide](./installing_daemons.md).

>Daemons in Rucio are background processes that perform various maintenance and operational tasks. They ensure that the system operates smoothly and that data management policies are enforced continuously.

The description of the various daemons can be found in [here](https://rucio.github.io/documentation/bin/rucio-abacus-account).
### FTS renewal and delegation

> [FTS](https://cs3mesh4eosc.eu/technologies/file-transfer-service-fts) is an open source software for reliable and large-scale data transfers. It provides easy user interfaces for submitting transfers: Python CLI, Python Client, WebFTS and Web Monitoring. Checksums and retries are provided per transfer and it is a flexible tool due to its multiprotocol support (Webdav/https, GridFTP, xroot, SRM). It also allows parallel transfers optimization to get the most from network without burning the storages.
#### How does FTS work: users' x509 certs

1. Users will have to pass their own x509 cert by setting its path in their rucio config.
2. Then, the x509 is used to map the cert to an identity that must be created within rucio, which corresponds to a user.
#### How does FTS work: service x509 cert

It is not the user's account that uploads, downloads, etc, but it's a ***service account*** that is set up normally by the collaboration or the team.

Together with the service account, we also need the corresponding user <u>grid cert</u> that is split in `cert` and `key` and is passed to the `ftsRenewal` daemon, that renews it periodically through a [specific script](https://github.com/rucio/containers/blob/master/fts-cron/renew_fts_proxy.sh.j2):

```yaml
- secretName: fts-cert
  mountPath: /opt/rucio/certs/
- secretName: fts-key
  mountPath: /opt/rucio/keys/
```

In the Compass example, the service account grid certificate is named `na58dst1.p12`. To retrieve the `cert.pem` and `key.pem`, one can run the [following these commands](https://stackoverflow.com/questions/15144046/converting-pkcs12-certificate-into-pem-using-openssl):
```sh
openssl pkcs12 -in na58dst1.p12 -out na58dst1.usercert.pem -clcerts -nokeys
openssl pkcs12 -in na58dst1.p12 -out na58dst1.userkey.pem -nocerts -nodes
```



#### How to connect to the proper VO?
[Reference documentation](./multi_vo_rucio.md).

:::tip[VO vs VOMS]
The VOMS (Virtual Organization Membership Service) is an attribute authority which serves as central repository for VO (Virtual Organization) user authorization information
:::
1. Specify which VO to use; in our case is `vo.compass.cern.ch`
2. If not present, create a config file for the VOMS in `/etc/vomses`.
In our case, looking at lxplus we have a file named `vo.compass.cern.ch-voms-compass-auth.cern.ch` that contains
```
"vo.compass.cern.ch" "voms-compass-auth.cern.ch" "443" "/DC=ch/DC=cern/OU=computers/CN=compass-auth.cern.ch" "vo.compass.cern.ch" "24"
```

So we need to create a secret with the proper content and ***mount*** it in the `ftsRenewal` daemon, under `/etc/vomses`:
```yaml
- secretName: voms-compass
  mountPath: /etc/vomses/
```

:::danger[Very important information about `ca-bundles`]
An additional secret is needed, and it's the `rucio-ca-bundle` secret.
It can be obtained by copying the content of `/etc/grid-security/certificates` on LXPLUS matching the regex `*.0` and `*.signing_policy` into a single folder that will be used as a secret.

An example can be found at [this link](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/1897a2252e98ca3f6ea54047b1a662f57d55f774/scripts/rucio-daemons-secret.sh#L36-37).
:::

In this way, we'll get something like the following output from the cron-job:

```sh
renew-fts-proxy: =================== Delegating ========================
renew-fts-proxy:  Proxy certificate file : /tmp/x509up
renew-fts-proxy:  User certificate file: /tmp/cert.pem
renew-fts-proxy:  User key file: /tmp/key.pem
renew-fts-proxy: Output to /tmp/x509up
renew-fts-proxy: Enter GRID pass phrase:
renew-fts-proxy: Your identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=na58dst1/CN=698253/CN=Robot: compass production
renew-fts-proxy: Using configuration file /root/.glite/vomses
renew-fts-proxy: Using configuration file /etc/vomses
renew-fts-proxy: .........+++
renew-fts-proxy: ....................+++
renew-fts-proxy: Creating temporary proxy to /tmp/tmp_x509up_u0_317  Done
renew-fts-proxy: Contacting  voms-compass-auth.cern.ch:443 [/DC=ch/DC=cern/OU=computers/CN=compass-auth.cern.ch] "vo.compass.cern.ch" Done

renew-fts-proxy: Warning: WARNING: vo.compass.cern.ch : The validity period of the issued attributes has been shortened to the maximum allowed by this VOMS server

renew-fts-proxy: Creating proxy to /tmp/x509up
renew-fts-proxy: No policy language specified, Gsi impersonation proxy assumed. Done

renew-fts-proxy: Your proxy is valid until Fri May 31 15:49:15 2024
renew-fts-proxy: Error: verification failed.

renew-fts-proxy: User certificate: /tmp/x509up
renew-fts-proxy: User private key: /tmp/x509up
renew-fts-proxy: Loaded DC=ch, DC=cern, OU=Organic Units, OU=Users, CN=na58dst1, CN=698253, CN=Robot: compass production, CN=2643790443
renew-fts-proxy: Loaded DC=ch, DC=cern, OU=Organic Units, OU=Users, CN=na58dst1, CN=698253, CN=Robot: compass production
renew-fts-proxy: Using endpoint: https://fts3-pilot.cern.ch:8446
renew-fts-proxy: REST API version: 3.13.0
renew-fts-proxy: Delegation ID: ef1e8a8e094bde07
renew-fts-proxy: No previous delegation found
renew-fts-proxy: Delegating
renew-fts-proxy: Signing request
renew-fts-proxy: Delegation id: ef1e8a8e094bde07
renew-fts-proxy: Termination time: 2024-05-31T15:49:15
```
### Add the certificates as secrets and mount them on the k8s pods
Now it is possible to run something similar to the [daemons script](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/1897a2252e98ca3f6ea54047b1a662f57d55f774/scripts/rucio-daemons-secret.sh) to add all the secrets to the cluster.

### Additional environment variables

```yaml
additionalEnvs:
  - name: USERCERT_NAME
    value: na58dst1.usercert.pem
  - name: USERKEY_NAME
    value: na58dst1.userkey.pem
  - name: RUCIO_FTS_SECRETS
    value: rucio-daemons-rucio-x509up
  - name: GRID_PASSPHRASE
    valueFrom:
    secretKeyRef:
      name: rucio-daemons-fts-passphrase
      key: passphrase
```
Please notice that `USERCERT_NAME` and `USERKEY_NAME` correspond to the name of `cert.pem` and `key.pem` extracted before, `RUCIO_FTS_SECRETS` doesn't need to be changed, and `GRID_PASSPHRASE` corresponds to the passphrase chosen for the service account (can be an empty string).

A diagram of how the proxy certificate is created and mounted on the daemons is displayed below:

```mermaid
graph TD

    voms[VOMS]
    fts[FTS Renewal Daemon]

    A((user x509)) --> B((Rucio Identity))
    B --> C[upload, <br> download, <br> ...]
    C --> D[x509 Service]


    D --> fts --"x509 service"---> voms --"auth confirmation"---> fts
    fts --"x509 proxy"--> mount[secretMount]

    mount --> i((daemon))
    mount --> j((daemon))
    mount --> k((daemon))

```

## Rucio UI
[Reference Helm Chart](https://gitlab.cern.ch/rucio-it/flux-compass/-/blob/master/sync/rucio-ui.yaml?ref_type=heads).

>The Rucio User Interface (UI) provides an accessible way for users to interact with the Rucio system. It allows users to manage data, set replication rules, and monitor data transfers and storage through a graphical or command-line interface.

As for Servers and Authentication, it is necessary to get the following components running:
- Dedicated LB: in the example, the domain associated with it is `compass-rucio-ui.cern.ch`
  - This value will be used in the `httpd` block:
  ```yaml
  rucio_hostname: "compass-rucio-ui.cern.ch"
  ```
- Grid Host certificates for the domain.

The secrets are then mounted in the following block:
```yaml
secretMounts:
  - secretName: hostcert
  mountPath: /etc/grid-security/hostcert.pem
  subPath: hostcert.pem
  - secretName: hostkey
  mountPath: /etc/grid-security/hostkey.pem
  subPath: hostkey.pem
  - secretName: cafile
  mountPath: /etc/grid-security/ca.pem
  subPath: ca.pem
```
### Proxy block
These settings configure how the Rucio UI will connect to the main Rucio service and the authentication service through specified proxies.
```yaml
proxy:
  rucioProxy: "compass-rucio.cern.ch"
  rucioAuthProxy: "compass-rucio-auth.cern.ch"
  rucioAuthProxyScheme: "https"
```

# Setting up Rucio
## Installing `rucio-clients`
Please refer to the [installation page](../user/setting_up_the_rucio_client).

:::info[TL;DR]
`pip install rucio-clients`
:::
### Installing `gfal`

:::tip[Remember to install dependencies]
Install `gfal2` "properly":
https://dmc-docs.web.cern.ch/dmc-docs/gfal2-python/pip-install.html
AND install these plugins:
```sh
 sudo dnf install gfal2-plugin-srm
 sudo dnf install gfal2-plugin-xrootd
 sudo dnf install gfal2-plugin-mock
 sudo dnf install gfal2-plugin-file
 sudo dnf install gfal2-plugin-http
```
:::
## Creating the Rucio root account configuration file
This is the minimal setup for the configuration file (`rucio.cfg`):
```yaml
[client]
rucio_host = https://compass-rucio.cern.ch
auth_host = https://compass-rucio-auth.cern.ch
ca_cert = /etc/pki/tls/certs/CERN-bundle.pem
auth_type = userpass
account = root
username = admin
password = xxx
```
Where `username` and `password` are the ones created while bootstrapping the db.

To export the created configuration, execute:
```sh
export RUCIO_CONFIG=<path-to>/rucio.cfg
```
Then test it via `rucio whoami`.
### Optional: creating a rucio client user pod in the cluster
Look at the following commits to have an example of how to create a pod in the cluster that serves as u rucio user:
- [Creating a rucio client root account](https://gitlab.cern.ch/rucio-it/flux-compass/-/commit/7579ea6fee12609a419639d3a6390cf1f9f9ee63)
- [Creating secrets](https://gitlab.cern.ch/rucio-it/flux-compass/-/commit/7e3075f1161c5728ca7e7d485242e152975f8b7c)
  - Creating secrets for username, password, and cert bundle (that HAS to be the `CERN-bundle.pem` that we find in ``/etc/pki/tls/certs/CERN-bundle.pem`)

> [!tip] How to I pass a cert to a pod?
> Create a secret for the cert, and then mount the secret to a specific path, using `volumeMounts`: take a look below

```yaml
  volumes:
    - name: cern-bundle
      secret:
        secretName: rucio-root-account-cern-bundle
  containers:
  - name: rucio-client
    image: rucio/rucio-clients:release-34.0.0
    volumeMounts:
      - name: cern-bundle
        mountPath: "/etc/pki/tls/certs/CERN-bundle.pem"
        subPath: CERN-bundle.pem
        readOnly: true
```


## Rucio admin usage
The [`rucio-admin`](https://rucio.github.io/documentation/bin/rucio-admin) command can be used to setup the Rucio instance.
### Create the Rucio Storage Elements (RSEs)
```sh
rucio rse add --rse XRD1
rucio rse add --rse XRD2
rucio rse add --rse XRD3
```

### Add the protocol definitions for the storage servers
```sh
rucio rse protocol add --host xrd1 --rse XRD1 --scheme root --prefix //rucio --port 1094 --impl rucio.rse.protocols.gfal.Default --domain-json '{"wan": {"read": 1, "write": 1, "delete": 1, "third_party_copy_read": 1, "third_party_copy_write": 1}, "lan": {"read": 1, "write": 1, "delete": 1}}'
rucio rse protocol add --host xrd2 --rse XRD2 --scheme root --prefix //rucio --port 1094 --impl rucio.rse.protocols.gfal.Default --domain-json '{"wan": {"read": 1, "write": 1, "delete": 1, "third_party_copy_read": 1, "third_party_copy_write": 1}, "lan": {"read": 1, "write": 1, "delete": 1}}'
rucio rse protocol add --host xrd3 --rse XRD3 --scheme root --prefix //rucio --port 1094 --impl rucio.rse.protocols.gfal.Default --domain-json '{"wan": {"read": 1, "write": 1, "delete": 1, "third_party_copy_read": 1, "third_party_copy_write": 1}, "lan": {"read": 1, "write": 1, "delete": 1}}'
```
### Enable FTS
```sh
rucio rse attribute add --rse XRD1 --key fts --value https://fts:8446
rucio rse attribute add --rse XRD2 --key fts --value https://fts:8446
rucio rse attribute add --rse XRD3 --key fts --value https://fts:8446
```
Note that `8446` is the port exposed by the `fts-server` pod. You can easily view ports opened by a pod by `kubectl describe pod PODNAME`.

### Fake a full mesh network
This command will set the distance between RSEs to 1, in order to make transfers possible.
```sh
rucio rse distance add --source XRD1 --destination XRD2 --distance 1
rucio rse distance add --source XRD1 --destination XRD3 --distance 1
rucio rse distance add --source XRD2 --destination XRD1 --distance 1
rucio rse distance add --source XRD2 --destination XRD3 --distance 1
rucio rse distance add --source XRD3 --destination XRD1 --distance 1
rucio rse distance add --source XRD3 --destination XRD2 --distance 1
```
### Set indefinite storage quota for root
```sh
rucio account limit add --account root --rses XRD1 --bytes infinity
rucio account limit add --account root --rses XRD2 --bytes infinity
rucio account limit add --account root --rses XRD3 --bytes infinity
```

### Create a default scope for testing
```sh
rucio scope add --account root --scope test
```

## Data management
### Create initial transfer testing data
```sh
dd if=/dev/urandom of=file1 bs=10M count=1
dd if=/dev/urandom of=file2 bs=10M count=1
dd if=/dev/urandom of=file3 bs=10M count=1
dd if=/dev/urandom of=file4 bs=10M count=1
```

### Upload the files
```sh
rucio upload --rse XRD1 --scope test --files file1 file2
rucio upload --rse XRD2 --scope test --files file3 file4
```

### Create a few datasets and containers
```sh
rucio did add --type dataset --did test:dataset1
rucio did content add --to test:dataset1 --did test:file1 test:file2

rucio did add --type dataset --did test:dataset2
rucio did content add --to test:dataset2 --did test:file3 test:file4

rucio did add --type container --did test:container
rucio did content add --to test:container --did test:dataset1 test:dataset2

rucio did add --type dataset --did test:dataset3
rucio did content add --to test:dataset3 --did test:file4
```

### Create a rule and remember returned rule ID

```sh
rucio rule add --did test:container --rses XRD3 --copies 1
```

This command will output a rule ID, which can also be obtained via:

```sh
rucio rule list --did test:container
```

Query the status of the rule until it is completed. Note that the daemons are running with long sleep cycles (e.g. 30 seconds, 60 seconds) by default, so this will take a bit. You can always watch the output of the daemon containers to see what they are doing.

```sh
rucio rule show --rule-id <rule_id>
```

Add some more complications

```sh
rucio did add --type dataset -d test:dataset3
rucio attach test:dataset3 test:file4
```
---
id: suspicious-replica-recoverer
title: Suspicious Replica Recoverer
---

Rucio has a daemon that is in charge of handling and recovering suspicious replicas, called the suspicious replica recoverer. A suspicious replica is a file where an attempt to access the file resulted in certain types of error messages. Whether the error was the result of an issue with the file itself or if the problem was caused by something else (e.g. there could have been a problem with the server on which the file is located), is unknown and needs to be taken into account by the suspicious replica recoverer.

An overview of the typical workflow for a replica before it reaches the suspicious replica recoverer can be seen [here](https://rucio.cern.ch/documentation/started/concepts/replica_workflow/).

## Suspicious Replicas

Replicas can have various states, two of which are `SUSPICIOUS` and `BAD`. Files that are `BAD` need to either be replaced or removed. The job of the suspicious replica recoverer is to decide how a suspicious replica should be handled.

A replica can be declared suspicious multiple times: each time an attempt to access the replica results in an error message, the replica is declared suspicious. This allows the daemon to handle replicas differently depending on how many times it has been declared suspicious. As long as a file has been declared suspicious less than a certain number of times (referred to as `nattempts`), it's assumed that there is nothing wrong with the replica and that the errors can be ignored. Once there are more that `nattempts` suspicious declarations, the replica is handled by the daemon.

Before replicas are handled individually, the daemon checks how many suspicious replicas are on each Rucio storage element (RSE), which are the servers that host replicas. If an RSE has more than `limit_suspicious_files_on_rse` suspicious replicas, then it is assumed that the problem lays with the RSE and not the replicas themselves. Under such a circumstance, the replicas on that RSE are set to the state `TEMPORARILY UNAVAILABLE` for three days. A replica in this state can't be interacted with. The assumption is that after three days, problems with the RSE will have been fixed. If not, then the replicas on that RSE will end up being declared suspicious en masse, which will result in them being set as `TEMPORARILY UNAVAILABLE` again. This cycle will be repeated until the underlying issue is solved.

## Last remaining copy of a file

Before any handling of a replica is done, the daemon first checks to see if the replica is the last remaining copy of a file. Specifically, if there is another copy of the replica on a different RSE, then the suspicious replica can safely be declared `BAD`. If, however, the replica is the last remaining copy, then steps are taken to decide whether or not the replica can be declared `BAD`:
-**Auditor/checksum**: if the replica was declared `SUSPICIOUS` by the auditor daemon or if the term `checksum` appears in the error message that lead to the replica being declared `SUSPICIOUS`, then the replica is declared `BAD`.
-**User and log files**: if the replica is for a user or log file, then the replica is declared `BAD`.
-**Policy**: if a replica survives the first two checks, then the handling of the replica is decided by its associated policy.

## Replica policies

A policy defines how a replica is to be handled by the daemon based on the replica's datatype and scope. This approach allows for flexibility and specific handling. By default, all replicas are ignored, meaning that if there is no policy specified for the datatype/scope, then no actions are taken on the replica and it will remain untouched.

The current polices are:
-**ignore**: this is the default policy. Datatypes and scopes can be explicitly set to be ignored, which highlights that a decision has purposefully been made to not perform any actions on these replicas. This is done to prevent mistakes in the future.
-**declare bad**: this dictates that any associated datatypes or scopes will be declared `BAD` by the daemon.
-**dry run**: this policy makes the daemon handle the replicas as if they were to be declared `BAD`, but at the final step, no actions are taken. This results in log messages with which it becomes possible to see how many replicas of the given datatype/scope would be declared `BAD` by the daemon.

The replica policies can easily be expanded in the future.

The policies are stored in a JSON file, an example of which can be seen in he following:

````
[
    {
        "action": "declare bad",
        "datatype": ["HITS"],
        "scope": []
    },
    {
        "action": "ignore",
        "datatype": ["RAW"],
        "scope": []
    }
    {
        "action": "dry run",
        "datatype": [],
        "scope": ["mc.*"]
    }
]
````


## `nattempts = 1`

A very large number of suspicious replicas have `nattempts = 1`. To clean up the database, replicas with `nattempts = 1` that also have a policy that would result in the replica being declared bad are given a "boost". This means that rules for those replicas are created. These rules only exist to create an attempt to interact with the replica. If there is in fact a problem with the replica (or the RSE), then each rule will result in an error and that replica will be declared `SUSPICIOUS` once for each rule, which will bring the number of declarations over the `nattempts` barrier. This results in the replica being handled normally by the daemon during the daemon's next cycle.

## Passive and active modes
The suspicious replica recoverer has two modes of operation:
- **Passive (default)**: no actions are taken by the daemon, but log files are generated as if the actions were taken (like a dry-run mode). Useful for testing daemon runs without affecting data.
- **Active**: the daemon is allowed to take actions on the replicas. This option has to explicitly be set when the daemon is called.


A simplified version of the finite state diagram of the daemon can be found below:

![Suspicious replica recoverer diagram](/img/suspicious_replica_recoverer_diagram.png)---
id: s3_rse_config
title: S3 Storage Configuration
---

## S3 Storage Configuration (FTS3 & GFAL2)

Rucio supports S3 storages which can be configured and used as RSEs. This section describes the steps needed to setup and use an S3 storage RSE when using FTS3 as the transfer tool and GFAL2 as the file access library.

### FTS3 & GFAL2 Specifications

There are two ways in which one can employ [FTS3](https://fts3-docs.web.cern.ch/fts3-docs/docs/s3_support.html#s3-support)  and [GFAL2](https://dmc-docs.web.cern.ch/dmc-docs/gfal2/plugins.html#gfal2-plugin-http) ([davix](https://davix.web.cern.ch/davix/docs/devel/cloud-support.html#amazon-s3)) to communicate with an S3 storage:

1. Using [pre-signed](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html) URLs which can be used to access and modify the resources. In this case the endpoint protocol must be `https://` and the user must pre-sign the URL before presenting it to the tools. Please note that in the case of FTS3, the actual timestamp that the transfer will initiate cannot be known a priori, so a pre-signed URL with a predefined lifetime can expire before the beginning of the transfer.

2. Delegating the signature of the URL to FTS3 and GFAL2. This requires providing the relevant configurations ([gfal_config](https://dmc-docs.web.cern.ch/dmc-docs/gfal2/plugins.html#for-a-specific-host) & [fts_config](https://fts3-docs.web.cern.ch/fts3-docs/docs/s3_support.html#configuration)) and using `s3s://` as the endpoint protocol. In this case the user must also be cautious to use the `alternate` configuration/flag appropriately. This will guide the usage of the [Path-Style](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html) URL (`alternate=true`) or the [Virtual-Style](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html) URL (`alternate=false`) during the signing process. Please note that FTS3 currently cannot handle transfers between S3 path-style URLs to S3 virtual-style URLs and vice versa. The `alternate` flag affects all endpoints in the submitted job, thus all S3 endpoints should adhere to the same URL-style.

### How to Setup an S3 RSE

1. Create the RSE. Upon executing `rucio-admin rse info <RSE>` command one should     have the following indicative result for the protocols section:

    ```bash
                                    .
                                    .
    Protocols:
    ==========
    https
        domains: '{"lan": {"read": 1, "write": 1, "delete": 1}, "wan":..}'
        extended_attributes: None
        hostname: <S3_HOSTNAME>
        impl: rucio.rse.protocols.gfal.Default
        port: 443
        prefix: <PATH> # bucket name in case of path-style URLs
        scheme: https
                                    .
                                    .
    ```

2. Set the following RSE attributes:

    ```bash
    sign_url: s3
    skip_upload_stat: True
    verify_checksum: False
    strict_copy: True
    s3_url_style: path(default)|host
    ```

3. Deploy the S3 configuration to the Rucio servers by creating a `<release-name-servers>-rse-accounts` containing the following:

    ```bash
    # vim /opt/rucio/etc/rse-accounts.cfg
    {
        "f4dc2967e329vdf5a73c154eb8d9ffae": {  #rse_id
                "access_key": "...",
                "secret_key": "...",
                "signature_version": "s3v4",   # must be s3v4
                "region": "us-west-2"          # adapt as necessary
        },
        ...
    }
    ```

    And add in your servers helm chart:

    ```
    values:
      secretMounts:
        - secretName: rse-accounts
          mountPath: /opt/rucio/etc/rse-accounts.cfg
          subPath: rse-accounts.cfg

      config:
        credentials:
          gcs: "/opt/rucio/etc/rse-accounts.cfg"

    ```

    Restart the servers.

4. Give every Rucio account the following attribute to be able to sign URLs:

    ```bash
    rucio-admin account add-attribute <accountname> --key sign-gcs --value true
    ```


    In order for this step to be effective, one has to make sure the relevant permission is given when the sign-gcs key is present for the account, for example [this](https://github.com/rucio/rucio/blob/ba102506d470c417fd2b136304e4fa4f7fc3a870/lib/rucio/core/permission/atlas.py#L1219) is the way it is currently done for ATLAS.


5. Configure FTS3 to be able to use the same access and secret keys as you did for the Rucio servers:

    * You need to have access to the FTS3 server config page
      * Visit `<fts_server>:8446/config/cloud_storage`
    * Add a new cloud storage (the name should be `S3:<URL>`)
    * Configure the added cloud storage as the following indicative example:

    ![image](/img/fts_s3_config.png)

    * Add `*` to User to include all users. If this cannot be done via the UI you need to contact the people who manage your FTS3 server.
    * Configure the VO roles
    * Add the __access token__ and the __access secret__, these correspond to the __access_key__ and __secret_key__ you also configured for Rucio.

### Rucio Specifications

* When a client tries __rucio upload__ & __rucio download__, or when the __reaper__ daemon tries to delete data, Rucio pre-signs the S3 URL and passes that signed URL to GFAL2. GFAL2 needs `https://` as the scheme for the protocol when this is the case.

* When the __conveyor-submitter__ daemon submits a transfer to FTS3, it does not pre-sign the URL but it depends on the FTS3 server to do so (Step 5 of the configuration in the previous section), in this case the proper scheme to use for the protocol is `s3s://`, Rucio will automatically translate `https -> s3s` in order to submit the transfer properly. The approach of not pre-signing the URL is necessary since those URLs have an expiration time and there is no guarantee of when the FTS3 transfer will really happen. This way, FTS3 signs the URL just before the transfer actually starts.
---
id: configuration_parameters
title: Configuration parameters
---
Description of the configuration parameters split in three different places:
Rucio configuration file (`rucio.cfg`), Rucio configuration table and RSE attributes.

## Rucio Configuration File (`rucio.cfg`)
* Only the Configuration File provides information for the
[client connection information](#client-client_config) or the
[database configuration](#database-db_config),
this information cannot be set in a different location.
* If not specified in the environmental variables (as `$RUCIO_CONFIG`),
Rucio will look for the config in the following locations -
    - `$RUCIO_HOME/etc/rucio.cfg`
    - `$VIRTUAL_ENV/etc/rucio.cfg`
    - `/opt/rucio/etc/rucio.cfg`


### Options and Defaults
#### **accounts**
  - **special_accounts** <!--NOT USED IN CODE-->
#### **alembic**
  - **cfg**: Path to the configuration file (.ini) for Alembic. Example:
    `/opt/rucio/etc/alembic.ini`. No default.
#### **api**
  - **endpoints**: _(Optional)_ Endpoints separated by commas. Default:
    `['accountlimits', 'accounts', 'config', 'credentials', 'dids', 'export',
    'heartbeats', 'identities', 'import', 'lifetime_exceptions', 'locks',
    'meta', 'ping', 'redirect', 'replicas', 'requests', 'rses', 'rules',
    'scopes', 'subscriptions']`.
#### **auditor**
  - **cache**: Path to the folder to store the `rucio-auditor` cache. Example:
    `/opt/rucio/auditor-cache`. No default.
  - **results**: Path to the folder to store the `rucio-auditor`
    results. Example: `/opt/rucio/auditor-results`. No default.
  - **threshold**: _(Optional)_ Floating number used in a sanity check,
    comparing the number of entries with the total number of files on the RSE:

    ```python
    if len(dark_replicas) > threshold * usage['files']
    ```

    ```python
    if len(lost_replicas) > threshold * usage['files']
    ```

    Default: `0.2`.
#### **bb8**
  - **dump_production_day**: _(Optional)_ Day of the week of the most recent
    dump. Values: `{Sunday, Monday, Tuesday, Wednesday, Thursday, Friday,
    Saturday}`. Default: `None`.
  - **dump_url_template**: _(Optional)_ URL of the template (structure) of a
    dump. Default:
    `http://rucio-analytix.cern.ch:8080/LOCKS/GetFileFromHDFS?date=${date}&rse=${rse}`.
#### **bootstrap**
  - **gss_email**: _(Optional)_ Email of the Kerberos auth method which identity
    is specified in `gss_identity`.
  - **gss_identity**: _(Optional)_ Identity of the Kerberos auth method.
  - **saml_email** <!--NOT USED IN CODE-->
  - **saml_id** <!--NOT USED IN CODE-->
  - **ssh_email**: _(Optional)_ Email of the SSH auth method which identity is
    specified in `ssh_identity`. Default: `ph-adp-ddm-lab@cern.ch`.
  - **ssh_identity**: _(Optional)_ SSH auth using an RSA key. Default: ```
    ssh-rsa
    AAAAB3NzaC1yc2EAAAABIwAAAQEAq5LySllrQFpPL614sulXQ7wnIr1aGhGtl8b+HCB/0FhMSMTHwSjX78UbfqEorZV16rXrWPgUpvcbp2hqctw6eCbxwqcgu3uGWaeS5A0iWRw7oXUh6ydnVy89zGzX1FJFFDZ+AgiZ3ytp55tg1bjqqhK1OSC0pJxdNe878TRVVo5MLI0S/rZY2UovCSGFaQG2iLj14wz/YqI7NFMUuJFR4e6xmNsOP7fCZ4bGMsmnhR0GmY0dWYTupNiP5WdYXAfKExlnvFLTlDI5Mgh4Z11NraQ8pv4YE1woolYpqOc/IMMBBXFniTT4tC7cgikxWb9ZmFe+r4t6yCDpX4IL8L5GOQ==
    ddmlab ```
  - **userpass_email**: _(Optional)_ Email of the root account which name is
    specified in `userpass_identity`. Default: `ph-adp-ddm-lab@cern.ch`.
  - **userpass_identity**: _(Optional)_ Name of the root account. Default:
    `ddmlab`.
  - **userpass_pwd**: _(Optional)_ Password of the root account which name is
    specified in `userpass_identity`. Default: `secret`.
  - **x509_email**: _(Optional)_ Email of the X.509 identity specified in
    `x509_identity`. Default: `ph-adp-ddm-lab@cern.ch`.
  - **x509_identity**: _(Optional)_ Identity of the X.509 certificate. Default:
    `emailAddress=ph-adp-ddm-lab@cern.ch,CN=DDMLAB Client
    Certificate,OU=PH-ADP-CO,O=CERN,ST=Geneva,C=CH`.
#### **cache**
  - **url**: _(Optional)_ URL of the cache. Default: `127.0.0.1:11211`.
  - **use_external_cache_for_auth_tokens**: _(Optional)_ if True, use remote
    cache provider for auth tokens. If False, use a private in-memory cache.
    Default: `False`
#### **common**
  - **extract_scope**: _(Optional)_ Default: `def`.
  - **logdir**: Path of the directory for logs. Contains `auditor.log`.
  - **logformat**: _(Optional)_ Formatter of the log. See [the logging formatter documentation](https://docs.python.org/3/library/logging.html#logging.Formatter)
  - **loglevel**: _(Optional)_. Set the root logger level to the specified
    level.

    Values: `{'NOTSET', 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'}`. See [documentation for logging levels](https://docs.python.org/3/library/logging.html#levels)

    Default: `DEBUG`. For `rucio-auditor` default is `WARNING`.
  - **mailtemplatedir**: _(Optional)_ Path of the folder with mail templates
    (.tmpl). Example: `/opt/rucio/etc/mail_templates`.
  - **multi_vo**: _(Optional)_ Default: `False`.
#### **conveyor**
  - **allow_user_oidc_tokens**: _(Optional)_ Boolean. Default: `False`.
  - **bring_online**: Integer, bring online timeout. Default: `43200`.
  - **cacert** <!--NOT USED IN CODE-->
  - **cache_time**: _(Optional)_ Integer, expiration time in seconds passed to
    the dogpile system. Default: `600`.
  - **failover_scheme**: Failover schemes. Default: `None`.
  - **filter_transfertool**: _(Optional)_ Default: `None`.
  - **ftshosts**: URL of the [File Transfer Service
    (FTS)](https://fts.web.cern.ch/) hosts (separated by commas). Example:
    `https://fts3-pilot.cern.ch:8446, https://fts3-pilot.cern.ch:8446`. <!--NOT
    USED IN CODE-->
  - **globus_auth_app**: _(Optional)_ Default: `None`.
  - **max_time_in_queue**: _(Optional)_ (separated by
    commas). Default: `{}`.
  - **poll_timeout**: Float, timeout. Default: `None`.
  - **queue_mode**: _(Optional)_ Values: `{'strict', 'default'}`Default:
    `default`.
  - **request_oidc_audience**: _(Optional)_. Default: `fts:example`.
  - **request_oidc_scope**: _(Optional)_. Default: `fts:submit-transfer`.
  - **scheme**: _(Optional)_ Schemes to process (separated by commas). Default:
    `None`.
  - **submit_timeout**: _(Optional)_ Timeout. Default: `None`.
  - **transfertool**: _(Optional)_ Default: `None`.
  - **transfertype**: _(Optional)_. Values: `{bulk, single}`. Default: `single`.
  - **usercert**: Path to the certificate for the FTS3 implementation of a Rucio
    transfertool. Default: `None`.
  - **user_activities**: _(Optional)_ Default: `['user',
    'user_test']`.
  - **user_transfers**: _(Optional)_ Default: `None`.
  - **using_memcache**: _(Optional)_ Boolean. Default: `False`.
  - **webdav_transfer_mode**: _(Optional)_. Default: `None`.
#### **core**
  - **default_mail_from**: _(Optional)_ Default email. Default: `None`.
  - **geoip_ignore_error**: _(Optional)_ Whether to ignore errors when
    downloading and parsing the GeoIP database. Otherwise exceptions will be
    raised for errors. Boolean. Default: `True`.
  - **geoip_licence_key**: _(Optional)_ License key for GeoLite2. Get a free
    licence key at [the signup page](https://www.maxmind.com/en/geolite2/signup). Default:
    `NOLICENCE`.
  - **use_temp_tables**: _(Optional)_ Use Rucio with temporary table workflows.
    Default: `False`.
#### **client** {#client_config}
  - **account**: Rucio account. Example: `root`.
  - **auth_host**: URL of the host of the rucio authentication server. Example:
    `https://rucio-auth-prod.cern.ch:443`.
  - **auth_token_file_path**: _(Optional)_ If token file path is defined in the
    rucio.cfg file, use that file. Currently this prevents authenticating as
    another user or VO. Default: `None`.
  - **auth_type**: Type of authentication in rucio. Values: `{userpass, x509,
    x509_proxy, gss, ssh, saml, oidc}`.
  - **ca_cert**: Path of the cert file for HTTPS. Example:
    `/opt/rucio/etc/web/ca.crt`.
  - **client_cert**: Path of the X.509 client cert file. This can be overwritten
    by the `RUCIO_CLIENT_CERT` environment variable. Example:
    `/opt/rucio/etc/web/client.crt`.
  - **client_key**: Path of the X.509 client key file for the cert defined in
    `client_cert`. This can be overwritten by the `RUCIO_CLIENT_KEY` environment
    variable. Example: `/opt/rucio/etc/web/client.key`.
  - **client_x509_proxy**: Path of the X.509 client proxy. Mandatory if
    `auth_type = x509_proxy`.
  - **oidc_audience**: _(Optional)_ Only used if `auth_type = oidc`. Default:
    `None`.
  - **oidc_auto**: _(Optional)_ Boolean. Only used if `auth_type =
    oidc`. Default: `False`.
  - **oidc_issuer**: _(Optional)_ Only used if `auth_type = oidc`. Default:
    `None`.
  - **oidc_password**: _(Optional)_ Only used if `auth_type = oidc`. Default:
    `None`.
  - **oidc_refresh_lifetime**: _(Optional)_ Only used if `auth_type =
    oidc`. Default: `None`.
  - **oidc_scope**: _(Optional)_ Only used if `auth_type = oidc`. Default:
    `openid profile`.
  - **oidc_username**: _(Optional)_ Only used if `auth_type = oidc`. Default:
    `None`.
  - **password**: Password of the user specified in `username`. Mandatory if
    `auth_type = userpass` or `auth_type = saml`.
  - **protocol_stat_retries**: _(Optional)_ Integer, number of retries if stat
    file fails. The time of the retries are: 1s, 2s, 4s, 8s, 16s, 32s
    later. Default: `6`.
  - **request_retries**: _(Optional)_ Integer, number of retries if an
    unauthorized error is returned. Default: `3`.
  - **rucio_host**: URL of rucio host. Example:
    `https://rucio-server-prod.cern.ch:443`.
  - **ssh_private_key**: Path of the SSH private key. Mandatory if `auth_type =
    ssh`. Example: `$HOME/.ssh/id_rsa`.
  - **username**: Mandatory if `auth_type = userpass` or `auth_type = saml`.
  - **vo**: _(Optional)_ VO name. Default: `def`.
#### **credentials**
  - **gcs**: _(Optional)_ Path of the Google Cloud Storage credentials. Default:
    `/opt/rucio/etc/google-cloud-storage-test.json`.
  - **signature_lifetime**: _(Optional)_ ?. Default: `600`.
#### **database** {#db_config}
  - **default**: Type of the SQL connection. Values: `{mysql, postgresql,
    sqlite, oracle}`.
  - **echo**: Enable echo for database logs. Values: `{0 (disable), 1
    (enable)}`.
  - **echo_pool**: See [sqlalchemy documentation](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.echo_pool)
  - **max_overflow**: See [sqlalchemy documentation](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.max_overflow)
  - **pool_recycle**: See [sqlalchemy documentation](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_recycle)
  - **pool_reset_on_return**: See [sqlalchemy documentation](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_reset_on_return)
  - **pool_size**: See [sqlalchemy documentation](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_size)
  - **pool_timeout**: [sqlalchemy documentation](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_timeout)
  - **poolclass**: Which connection pooling mechanism to use. Values: `nullpool`
    (disables pooling), `queuepool` (default for all but SQLite engine), or
    `singletonthreadpool` (default for SQLite engine).
    See [sqlalchemy documentation](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.poolclass)
  - **schema**: _(Optional)_ Schema to be applied to a database, if not set in
    config, try to create automatically.
  - **use_threadlocal**
#### **download**
  - **transfer_speed_timeout**: _(Optional)_ Minimum allowed average transfer
    speed (in KBps). Default: `500`. Used to dynamically compute the timeout if
    `--transfer-timeout` not set. Is not supported for `--pfn`.
  - **transfer_timeout**: _(Optional)_ Transfer timeout (in seconds). Default:
    computed dynamically from `--transfer-speed-timeout`. If set to any value >=
    0, `--transfer-speed-timeout` is ignored.
#### **es-atlas**
  - **ca_cert**: _(Optional)_ Path of the certificate for Elasticsearch. No
    default.
  - **password**: _(Optional)_ Password of the username defined in `username` to
    authenticate to Elasticsearch. No default.
  - **url**: _(Optional)_ URL of Elasticsearch. Example:
    `http://aianalytics01.cern.ch:9200`. No default.
  - **username**: _(Optional)_ Username to authenticate to Elasticsearch. No
    default.
#### **hermes**
  - **elastic_endpoint**: _(Optional)_ URL of Elasticsearch. Example:
    `http://aianalytics01.cern.ch:9200`. Mandatory if `elastic` is specified in
    `services_list`.
  - **influxdb_endpoint**: _(Optional)_ URL of InfluxDB. Mandatory if `influx`
    is specified in `services_list`.
#### **importer**
  - **attr_sync_method**: _(Optional)_ Values: `{append, edit, hard}`. Default:
    `edit`.
  - **rse_sync_method**: _(Optional)_ Values: `{append, edit, hard}`. Default:
    `edit`.
#### **injector**
  - **adler32** <!--NOT USED IN CODE-->
  - **bytes** <!--NOT USED IN CODE-->
  - **file** <!--NOT USED IN CODE-->
  - **md5** <!--NOT USED IN CODE-->
#### **lifetime**
  - **directory**: _(Optional)_ Path to the policies directory with JSON files
    named `config_DTYPE.json`, where `DTYPE` is a value in `{data, mc, valid,
    other}`. Default: `/opt/rucio/etc/policies`.
#### **logging**
  - ***CFG_OPTION***: _(Optional)_ ?. Default: `None`.
#### **messaging-cache**
  - **account**
  - **broker_virtual_host**: _(Optional)_ ?
  - **brokers**: Default message broker name for `rucio-cache-client`. Ignored
    if `rucio-cache-client` executed with `--broker`.
  - **destination**: Default message broker topic for
    `rucio-cache-client`. Ignored if `rucio-cache-client` executed with
    `--destination`.
  - port
  - **ssl_cert_file**: Default certificate file for
    `rucio-cache-client`. Ignored if `rucio-cache-client` executed with
    `--certificate`.
  - **ssl_key_file**: Default certificate key file for
    `rucio-cache-client`. Ignored if `rucio-cache-client` executed with
    `--certificate-key`.
  - **voname** <!--NOT USED IN CODE-->
#### **messaging-fts3**
  - **broker_virtual_host**: _(Optional)_ ?. No default.
  - **brokers**: Brokers separated by commas. Example: `dashb-test-mb.cern.ch`.
  - **destination**: Name of the destination topic. Example:
    `/topic/transfer.fts_monitoring_queue_state`.
  - **nonssl_port**: _(Optional)_ Port of the broker if `use_ssl` is not set.
  - **password**: _(Optional)_ Password of the `username`. Only used if
    `use_ssl` is not set. No default.
  - **port** <!--NOT USED IN CODE-->
  - **ssl_cert_file**: _(Optional)_ Path of the certificate file. No default.
  - **ssl_key_file**: _(Optional)_ Path of the certificate key file defined in
    `ssl_cert_file`. No default.
  - **use_ssl**: _(Optional)_ Boolean. Default: `True`.
  - **username**: _(Optional)_ Username of the broker. Only used if `use_ssl` is
    not set. No default.
  - **voname** <!--NOT USED IN CODE-->
#### **messaging-hermes**
  - **broker_virtual_host**: _(Optional)_ No default.
  - **brokers**: Brokers separated by commas. Example: `atlas-test-mb.cern.ch`.
  - **destination**: Name of the destination topic. Example:
    `/topic/rucio.events`.
  - **email_from**: Example: `Rucio <spamspamspam@cern.ch>`.
  - **email_test**: Example: `spamspamspam@cern.ch`.
  - **nonssl_port**: _(Optional)_ Port of the broker if `use_ssl` is not set.
  - **password**: _(Optional)_ Password of the `username`. Mandatory if
    `use_ssl` is not set. No default.
  - **port**: _(Optional)_ Port of the broker if `use_ssl` is set.
  - **ssl_cert_file**: _(Optional)_ Path of the certificate file. No
    default. Mandatory if `use_ssl` is set.
  - **ssl_key_file**: _(Optional)_ Path of the certificate key file defined in
    `ssl_cert_file`. No default. Mandatory if `use_ssl` is set.
  - **use_ssl**: _(Optional)_ Boolean. Default: `True`.
  - **username**: _(Optional)_ Username of the broker. Mandatory if `use_ssl` is
    not set. No default.
  - **voname** <!--NOT USED IN CODE-->
#### **metadata**
  - **plugins**: _(Optional)_ Metadata handler modules separated by
    commas. Default: `rucio.core.did_meta_plugins.json_meta.JSONDidMeta`.
#### **monitor**
  - **enable_metrics**: _(Optional)_ Enable `statsd` metrics. Boolean. Default:
    `False`.
  - **carbon_server**: _(Optional)_ Hostname or IP address of the `statsd`
    server. Default: `localhost`
  - **carbon_port**: _(Optional)_ Port of the `statsd` server. Default: `8125`.
  - **user_scope**: _(Optional)_ Prefix to distinguish and group stats from an
    application or environment. Default: `rucio`.
  - **metrics_port**: _(Optional)_ Port of Prometheus Python Client. Default:
    `8080`.
#### **nagios**
  - **fts_servers** <!--NOT USED IN CODE-->
  - **proxy** <!--NOT USED IN CODE-->
  - **rfcproxy** <!--NOT USED IN CODE-->
#### **nongrid-trace**
  - **broker_virtual_host**: _(Optional)_ ?. No default.
  - **brokers**: Brokers separated by commas. Example: `atlas-test-mb.cern.ch`.
  - **logformat**: _(Optional)_ Formatter of the log. See [logging documentation](https://docs.python.org/3/library/logging.html#logging.Formatter).
  - **loglevel**: _(Optional)_ Set the root logger level to the specified level.

    Values: `{'NOTSET', 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'}`. See [logging documentation](https://docs.python.org/3/library/logging.html#levels).

    Default: `DEBUG`.
  - **password**: Password of the `username`.
  - **topic**: Name of the destination topic.
  - **tracedir**: _(Optional)_ Path of the directory for traces. Default:
    `/var/log/rucio`.
  - **username**: Username of the broker.
#### **oidc**
  - **admin_issuer**: Example: `wlcg`.
  - **default_jwt_refresh_lifetime**: _(Optional)_ Integer. Default: `96`.
  - **exchange_grant_type**: _(Optional)_ Default:
    `urn:ietf:params:oauth:grant-type:token-exchange`.
  - **expected_audience**: _(Optional)_ Default: `rucio`.
  - **expected_scope**: _(Optional)_ Default: `openid profile`.
  - **idpsecrets**: Path of the idpsecrets JSON. Example:
    `/opt/rucio/etc/idpsecrets.json`.
#### **permission**
  - **policy**: _(Optional)_ Permission policy. Values: `{atlas, belleii, cms,
    generic, generic_multi_vo}`. Default: `generic`.
#### **policy**
  - **lfn2pfn_algorithm_default**: _(Optional)_ Default algorithm name for
    LFN2PFN translation for this server. Default: `hash`.
  - **package**
  - **package-*VO***
  - **permission**: Same as `permission/policy`.
  - **schema**
  - **scratchdisk_lifetime**: _(Optional)_ Integer. Default: `14`.
  - **support**: _(Optional)_ Contact information.
  - **support_rucio**: _(Optional)_ Rucio contact information. Default:
    `https://github.com/rucio/rucio/issues`.
#### **saml**
  - **config_path**: Path to the SAML config folder. Example:
    `/opt/rucio/lib/rucio/web/ui/common/saml/`.
#### **test**
  - **cacert**: Path of the CA certificate for tests. Example:
    `/opt/rucio/etc/web/ca.crt`
  - **usercert** Path of the user certificate for tests. Example:
    `/opt/rucio/etc/web/usercert.pem`
  - **userkey**: Path of the user certificate key for tests.
#### **trace**
  - **broker_virtual_host**: _(Optional)_ No default.
  - **brokers**: Brokers separated by commas. Example: `atlas-test-mb.cern.ch`.
  - **logformat**: _(Optional)_ Formatter of the log. See [logging documentation](https://docs.python.org/3/library/logging.html#logging.Formatter).
  - **loglevel**: _(Optional)_ Set the root logger level to the specified level.
    Values: `{'NOTSET', 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'}`. See [logging documentation](https://docs.python.org/3/library/logging.html#levels).

    Default: `DEBUG`.
  - **password**: Password of the `username`.
  - **port**: _(Optional)_ Port of the broker. Example: `61013`.
  - **topic**: Name of the destination topic. Example: `/topic/rucio.tracer`.
  - **tracedir**: _(Optional)_ Path of the directory for traces. Default:
    `/var/log/rucio/trace`.
  - **username**: Username of the broker.
#### **tracer-kronos**
  - **broker_virtual_host**: _(Optional)_ No default.
  - **brokers**: Brokers separated by commas. Example: `atlas-test-mb.cern.ch`.
  - **chunksize**: Integer
  - **excluded_usrdns**: Separated by commas. Example:
    `CN=proxy,CN=Robot: Ganga Robot,CN=722147,CN=gangarbt,OU=Users,OU=Organic
    Units,DC=cern,DC=ch`.
  - **password**: _(Optional)_ Password of the `username`. Mandatory if
    `use_ssl` is not set. No default.
  - **port**: Port of the broker.
  - **prefetch_size**: `activemq.prefetchSize`, see [activemq documentation](https://activemq.apache.org/what-is-the-prefetch-limit-for)
  - **queue**: The topic or queue to subscribe to. Example:
    `/queue/Consumer.kronos.rucio.tracer`.
  - **reconnect_attempts**: Maximum attempts to reconnect. Integer. Example:
    `100`.
  - **ssl_cert_file**: _(Optional)_ Path of the certificate file. No
    default. Mandatory if `use_ssl` is set.
  - **ssl_key_file**: _(Optional)_ Path of the certificate key file defined in
    `ssl_cert_file`. No default. Mandatory if `use_ssl` is set.
  - **subscription_id**: A unique id to represent the subscription. Example:
    `rucio-tracer-listener`.
  - **use_ssl**: _(Optional)_ Boolean. Default: `True`.
  - **username**: _(Optional)_ Username of the broker. Mandatory if `use_ssl` is
    not set. No default.
#### **transmogrifier**
  - **maxdids** <!--NOT USED IN CODE-->
#### **upload**
  - **transfer_timeout**: _(Optional)_ Transfer timeout (seconds,
    integer). Default: `360`.
#### **webui**
  - **auth_issuer**: _(Optional)_ Mandatory if `auth_type` = `oidc`. No default.
  - **auth_type**: _(Optional)_ Preferred server side config for webui
    authentication. Values: `{oidc, None}`. Default: `None`.
  - **usercert** <!--NOT USED IN CODE-->
  - **urls**: A CSV specifying urls of Rucio WebUI 2.0 clients. Required for
    correctly handling pre-flight CORS requests.

## Rucio configuration table
* Checked only if Section/Option pair is not in the Configuration File

### Updating
The table can be updated with the `Rucio Client`, using either
* `rucio-admin config set [section] [option] [value]`
* `rucio.ConfigClient().set_config_option([section], [option], [value])`

These changes take impact immediately without requiring a restart.
Using the client only updates the Configuration Table and does not overwrite
anything in the Configuration File, and thus will not change anything if there
a setting already specified in the Configuration File.

### Options and Defaults
#### **automatix**
  - **account**: _(Optional)_ Account to use. Default: `root`.
  - **dataset_lifetime**: _(Optional)_. Default: `0`.
  - **did_prefix**: _(Optional)_ Default: ` `.
  - ***DIDTYPE*\_pattern**: _(Optional)_ Separated by `separator` char. No
    default.
  - **rses**: Separated by commas.
  - **scope**: _(Optional)_ Default: `test`.
  - **separator**: _(Optional)_ Separator char. No default.
  - **set_metadata**: _(Optional)_ Default: `True`.
  - **sites**: Separated by commas _(to be deprecated, please use `rses`)_.
  - **sleep_time**: _(Optional)_ Integer. Default: `30`.
#### **clientcachemap**
  - ***client_location['site']***
#### **conveyor**
  - **activity-source-strategy**: _(Optional)_ Default: `{}`.
  - **default-source-strategy**: _(Optional)_ Default: `orderly`.
#### **hermes**
  - **services_list**: List of services separated by commas. Values: `{activemq,
    elastic, influx}`.
#### **kronos**
  - **bad_files_patterns**: _(Optional)_ Patterns (regular expression) separated
    by commas for bad files. Default: `[]`.
#### **lifetime_model**
  - **approvers_email**: _(Optional)_ Separated by commas. Default: `[]`.
#### **reaper**
  - **auto_exclude_threshold**: _(Optional)_ Number of service unavailable
    exceptions after which the RSE gets temporarily excluded. Integer. Default:
    `100`.
  - **auto_exclude_timeout**: Timeout for temporarily excluded
    RSEs. Integer. Default: `600`.
  - **max_deletion_threads_*HOSTNAME***: _(Optional)_ Max number of deletion
    threads (integer). If `nb_workers_by_hostname` is also not defined, default:
    `5`.
  - **max_evaluator_backlog_count**: _(Optional)_ Integer. Default: `None`.
  - **max_evaluator_backlog_duration**: _(Optional)_ Minutes (integer). Default:
    `None`.
  - **nb_workers_by_hostname**: _(Optional)_ Integer. Default: `5`.
#### **root-proxy-internal**
  - ***client_location['site']***
#### **rules**
  - **apply_rule_max_partition_size**: _(Optional)_ Integer . Default: `2000`.
  - **force_epoch_when_detach**: _(Optional)_ Purge setting of the
    rule. Boolean. Default: `False`.
  - **use_new_rule_algorithm**: _(Optional)_ Boolean. Default: `False`.
#### **subscriptions**
  - **keep_history**: _(Optional)_ Boolean. Default: `False`.
  - **reevaluate_dids_at_close**: _(Optional)_ Flag to reevaluate the DID against
    all the subscriptions when the DID is closed. Boolean. Default: `False`.
#### **transfers**
  - **hop_penalty**: _(Optional)_ Penalty to be applied to each further
    hop. Integer. Default: `10`.
  - **multihop_tombstone_delay**: Seconds (integer). Default: `7200`.
  - **fts3tape_metadata_plugins**: _(Optional)_ Plugins to use with FTS3 to include archive
  metadata in the transfer process. List[String]. Default: `None`.
  - **metadata_byte_limit**: _(Optional)_ Limit applied to `archive_metadata` during a transfer.
  Only used with archive metadata plugins using FTS3. Integer. Default `None`.
#### **virtual_placement**
  - **vp_endpoint**: _(Optional)_ Virtual Placement server. Once VP is
    integrated in Rucio it won't be needed. Default: ` `.
#### **vo-map**
  - ***VO***: _(Optional)_ Internal short VO name. No default.

## RSE settings
The RSE settings are set separately using `rucio.RSEClient.update_rse` or `rucio rse update`, and specifies RSE configuration used by the Rucio instance.
Mutable settings are `deterministic`, `rse_type`, `staging_area`, `volatile`, `qos_class`, `availability_delete`, `availability_read`, `availability_write`, `city`, `country_name`, `latitude`, `longitude`, `region_code`, and `time_zone`.
Read more about RSEs [here](started/concepts/rucio_storage_element.md)
and how to set them up [here](operator/configuration.md#creating-new-rses).

- **availability_delete**: Boolean. Control if this RSE allows deletions by the Reaper daemon using any protocol. Default: `True`.
- **availability_read**: Boolean. Control if this RSE allows reads using any protocol. Default: `True`.
- **availability_write**: Boolean. Control if this RSE allows writes using any protocol. Default: `True`.
- **credentials**: Dictionary[String, Any]: Specify token credentials used for accessing this RSE if it is in a commercial cloud. No default.
- **delete_protocol**: Integer: Cannot be changed. Controls matching of protocol priorities for deletions. Default: `1`.
- **deterministic**: Boolean: Controls if the RSE is allowed to generate paths based solely on the DID (scope:filename). More info about non-deterministic RSEs [here](started/concepts/replica_workflow.md#replica-paths-on-storage). Default: `True`.
- **domain**: List[String]: Specifies the locations this RSE may be accessed by clients from. Cannot be changed.
- **id**: String: Identification string of the RSE. Cannot be changed.
- **lfn2pfn_algorithm**: String: Name of the algorithm in the configured policy package to be used for creating replica paths. Cannot be changed. If no lfn2pfn_algorithm attribute is set, then the setting defaults to lfn2pfn_algorithm_default in the configured policy package. Default: `default`.
- **qos_class**: String: No functionality in modern Rucio. No default.
- **read_protocol**: Integer: Cannot be changed. Controls matching of protocol priorities for reads. Default: `1`.
- **rse**: String: The name of the Rucio Storage Element as given at creation time. Cannot be changed.
- **rse_type**: String: Specify `DISK` or `TAPE` for control of [QoS](operator/qos_rse_config.md). Default: `DISK`.
- **sign_url**: Optional[str]. Enable cloud support for this storage element. No default.
- **staging_area**: Boolean.Specify if this RSE is a disk buffer to a tertiary storage backend, subject to additional constraints (specifically a lifetime for rules on this RSE must be defined). Default: `False`.
- **third_party_copy_read_protocol**: Integer: Cannot be changed. Controls matching of protocol priorities for TPC reads. Default `1`.
- **third_party_copy_write_protocol**: Integer: Cannot be changed. Controls matching of protocol priorities for TPC writes. Default `1`.
- **verify_checksum**: Boolean: Specifies if the RSE has support for checksum verification. Default: `True`.
- **volatile**: Boolean. Specifies if the RSE is cache storage. Subject to volatile RSE restrictions detailed [here](https://rucio.github.io/documentation/operator/qos_rse_config). Default: `False`.
- **write_protocol**: Integer: Cannot be changed. Controls matching of protocol priorities for writes. Default: `1`.

## RSE attributes
The RSE Attributes are set separately using `rucio.RSEClient.add_rse_attribute` or `rucio rse attribute add`.
and only contains information about the specific RSE's for the Rucio instance.
Read more about RSEs [here](started/concepts/rucio_storage_element.md)
and how to set them up [here](operator/configuration.md#creating-new-rses).

- **associated_sites**: String. Separated by commas. Used for chaining of subscriptions so that transfers to one RSE will also be mirrored to associated_sites. No default.
- **archive_timeout**: Integer: Only used for transfers with a tape destination. Controls the number of seconds the FTS3 transfer manager will wait for the tape archival of the file to go `FAILED` or `FINISHED`. No default.
- **auto_approve_bytes**: String: Upper limit for the size in bytes of a DID for which rules will be automatically approved. Example: `500GB`. No default.
- **auto_approve_files**: Integer: Upper limit for the number of files covered by a rule which will be automatically approved. No default.
- **available_for_multihop**:  Boolean. If True, allow to use this RSE as an intermediate hop in a multi-hop transfer. Default: `False`.
- **block_manual_approval**: Boolean. Disable manual rule approval for this RSE. Default: `False`.
- **bittorrent_tracker_addr**: String. Used to configure the URL of the bittorrent tracker API when using the torrent transfer manager. No Default.
- **checksum_key**: String. Used to specify an alternate RSE attribute to search for supported checksums beyond those with global support (ADLER32, MD5). Default: `supported_checksums`.
- **country**: String. Used for country level granularity of RSE selectors. No default.
- **decommission**: Boolean. Indicates to the RSE Decommissioning Daemon that this RSE is to be decommissioned. Default: `False`.
- **driver_name_rse_attribute**: String. Used to specify alternate drivers when using the Bittorrent transfer manager. Default: `bittorrent_driver`.
- **fts**: String. Specify the REST API URL of the FTS3 transfer manager. No default.
- **greedyDeletion**: Boolean. Allow files without a rule locking them to be deleted by a Reaper Daemon. Default behavior only marks a file for deletion when there is no space on an RSE for a new required file. Default: `False`.
- **group_by_rse_attribute**: String. Control the RSE attribute (such as `country`) which transfer source RSEs will be grouped by when determining an appropriate transfer source. Default: `UNKNOWN`.
- **globus_endpoint_id**: String. Specify the REST API URL of the Globus transfer manager. No default.
- **hop_penalty**: Integer. Usage cost of this RSE as an intermediate in [multihop transfers](operator_transfers/transfers_overview.md). Overrides the global `transfers/hop_penalty` configuration for this particular RSE.
 Requires `available_for_multihop` attribute is True on the RSE. No default.
- **is_object_store**: Boolean. Control the auditor daemon's behavior. Instead of dumping all files, list them by date. Default: `False`.
- **istape**: Boolean. Default: `False`.
- **lfn2pfn_algorithm**: String. Name of the algorithm to be used for generating paths to files on the storage. Must be defined in the configured policy package. Default: `default`.
- **mock**: Boolean. Default: `False`.
- **multihop_tombstone_delay**: Integer. Delay before a multihop transfer intermediate rule is to be deleted. Defined in seconds. Default: `7200`.
- **naming_convention**: String. Name of the algorithm in the configured policy package which is to be used to validate DIDs on this RSE. Default: `None`.
- **oidc_support**: Boolean. Specifies that the RSE supports OIDC authentication for FTS3 transfers. Default: `False`.
- **overwrite_when_only_on_disk**: Boolean. On a `TAPE` RSE, controls if a file can be overwritten. A file may only be overwritten if it has not yet been written to the tape backend. Default: `False`.
- **overwrite**: Boolean. Controls if a file can be overwritten on the RSE. Default: `True` for `rse_type: DISK`. `False` for `rse_type: TAPE`.
- **physgroup**: String. Used for grouping of rules by CERN experiments. Default: ` `.
- **qbittorrent_management_address**: String. Used to configure the URL of the bittorrent management API when using the torrent transfer manager. No Default.
- **quota_approvers**: List[string]. List of Rucio users separated by commas. Only used in the permission layer of the policy package, and likely specific to the CERN experiments only. Default: `None`.
- **restricted_read**: Boolean. If True, only allow transfers FROM this RSE if started by an account with admin privileges. Default: `False`
- **restricted_write**: Boolean. If True, only allow transfers TO this RSE if started by an account with admin privileges. Default: `False`
- **rule_approvers**: List[string]. List of Rucio users separated by commas which will be notified by email to approve rules on this RSE. Default: `None`.
- **rule_deleters**: List[string]. List of Rucio users separated by commas. Only used in the permission layer of the policy package, and likely specific to the CERN experiments only. Default: `None`.
- **sign_url**: Optional[str]. Controls if URLs for uploads and transfers are to be cryptographically signed by an external service. No default.
- **simulate_multirange** Integer. Control the number of connections for multirange byte requests on commercial cloud storage. Multirange is not supported on S3. No default.
- **site**: String. Used to determine if downloads/transfers are local or wide area network. No default.
- **skip_upload_stat**: Boolean. Disables the use of a GFAL `stat` when calling `rucio upload` for this RSE. Default: `False`.
- **source_for_total_space**: String. Used to specify where Rucio should obtain storage capacity information from. Default: `storage`.
- **source_for_used_space**: String. Used to specify where Rucio should obtain storage usage information from. Default: `storage`.
- **staging_buffer**: String. Used with `TAPE` RSEs to specify to which RSE a file on tape should be transferred to as a disk buffer. Destination RSE should have `staging_area: True`. No default.
- **staging_required**: Boolean. Duplicates the `rse_type` RSE setting. Specifies that files on this RSE will require staging from high-latency storage. Default `False`.
- **strict_copy**: Boolean. Instructs the transfer manager to ONLY copy the file, disabling all validation checks such as `PROPFIND` and checksumming. Default `False`.
- **s3_url_style**: String. For S3 storage elements [specify](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html) `path` or `host`. No default.
- **tier**: Datacenter TIER. Integer (from 1 to 4). No default.
- **type**: Values: `{LOCALGROUPDISK, LOCALGROUPTAPE, GROUPDISK, SCRATCHDISK, MOCK, TEST, DATADISK}`. Default: ` `.
- **use_ipv4**: Boolean. Force the transfer manager to use IPv4 for transfers to this RSE. Default `False`.
- **verify_checksum**: Boolean. Control if checksum is to be queried on transfer source and destination to confirm successful transfers. Default: `True`
---
id: administration
title: Rucio Administration Tricks
sidebar_label: Administration Tricks
---

## Rucio container code hot-patching

The rucio official containers provide a way to hotpatch the source code
before running rucio. This behavior may be used to rapidly fix a production
deployment without having to wait for the change to be merged and released by
the rucio team.

The behavior is very simple: any file found in the `/patch/*` directory inside
the containers will be fed to the `patch` command-line tool in the order
returned by this glob matching.

The procedures described below are only intended for temporary fixes. If a
permanent change is needed, we highly encourage you to open a pull request
in rucio to spare you the toil related to maintaining your own local patch set.

### Creating a patch from existing pull request

Lets assume you run rucio `1.27.3` in production, and you realize there is
a bug in the `conveyor-poller` daemon. You contact the rucio team and are
told that a fix was already done in the pull request
[#5246](https://github.com/rucio/rucio/pull/5246/commits)
and will be released next week. Follow
the following steps to create a patch file from this pull request without
having to wait for the next rucio release to fix your issue:

Clone the main rucio repository:

```bash
git clone https://github.com/rucio/rucio.git
cd rucio
# Create a new branch from the 1.27.3 release tag
git checkout 1.27.3 -b patch-0-hotfix_conveyor_poller_on_1.27.3
```

The #5246 pull request was submitted by the user `rcarpa` from his rucio
fork [rcarpa/rucio](https://github.com/rcarpa/rucio). You'll have to add
this fork repository as a git remote:

```bash
git remote add rcarpa https://github.com/rcarpa/rucio.git
git fetch rcarpa
# Cherry-pick the commit from the #5246 pull request.
# Beware, some PR can have multiple commits
git cherry-pick 47d36345469ac9c1391cacd09487d4ec6ced627f
```

Now you can create the patch with the differences between the rucio 1.27.3
release and the current state of the repository:

```bash
git diff 1.27.3 > hotfix_conveyor_poller.patch
```

### Mounting a patch in a kubernetes cluster

If you deploy rucio in a kubernetes cluster using the official rucio
[helm charts](https://github.com/rucio/helm-charts/) and want to hotfix
rucio using a patch file created in the previous section, follow these steps:

Create a kubernetes secret from the hotfix patch:

```bash
kubectl -n rucio create secret generic hotfix-conveyor-poller-patch --from-file=hotfix_conveyor_poller.patch
```

*Note:* if you have more than one cluster, don't forget to create the
secrets in each cluster to be patched.

Now you'll have to update the helm `values` file for each helm release and
add the following

```yaml
    secretMounts:
      - secretFullName: hotfix-conveyor-poller-patch
        mountPath: /patch/hotfix_conveyor_poller.patch
        subPath: hotfix_conveyor_poller.patch
```
---
id: setting_up_demo
title: Setting up a Rucio demo environment
sidebar_position: 1
---

## Prerequisites

We provide a containerised version of the Rucio development environment
for a quick start. Our containers are ready-made for Docker, which means
you need to have a working Docker installation. To install Docker for
your platform, please refer to the [__Docker installation
guide__](https://docs.docker.com/install/), for example, for CentOS
[__follow these instructions for the Docker Community
Edition__](https://docs.docker.com/install/linux/docker-ce/centos/).
Please make sure that you install this recent Docker version especially
if you are on CentOS, i.e. its default version is ancient and does not
support some features we rely on.

Start the Docker daemon with `sudo systemctl start docker`.
You can confirm that Docker is running properly by executing (might need
`sudo`:

```bash
docker run hello-world
```

If successful, this will print an informational message telling you that
you are ready to go. Now, also install the `docker-compose`
helper tool with `sudo yum install docker-compose` (might
need [__EPEL__](https://fedoraproject.org/wiki/EPEL) enabled). You are now
ready to install the Rucio development environment.

## Preparing the environment

The first step is to check if SELinux is running. SELinux will block
access to the directories mounted inside the container, and so depending
on you node might need to be put in permissive mode with `setenforce
permissive`.

The second step is to fork the [__main Rucio repository on
GitHub__](https://github.com/rucio/rucio) by clicking the yellow Fork Star
button, and then clone your private forked Rucio repository to your
`/dev/rucio`. Afterwards add the main upstream repository
as an additional remote to be able to submit pull requests later on:

```bash
cd ~/dev
git clone git@github.com:<your_username>/rucio.git
cd rucio
git remote add upstream git@github.com:rucio/rucio.git
git fetch --all
```

Now, ensure that the `.git/config` is proper, i.e.,
mentioning your full name and email address, and create the
`.githubtoken` file that contains a full access token from
[__Github Account Settings__](https://github.com/settings/tokens).

Next, startup the Rucio development environment with docker-compose.
There are three different types: a standard one to just run the
unittests and do basic development, which includes just Rucio without
any transfer capabilities. One slightly larger one, which includes the
File Transfer Service (FTS) and three XrootD storage servers to develop
upload/download and transfers capabilities. And a third large one, which
adds the full monitoring stack with Logstash, Elasticsearch, Kibana and
Grafana.

## Using the standard environment

Run the containers using docker-compose (again might need
`sudo`):

```bash
docker-compose --file etc/docker/dev/docker-compose.yml up -d
```

And verify that it is running properly:

```bash
docker ps
```

This should show you a few running containers: the Rucio server, the
PostgreSQL database and the Graphite monitoring.

Finally, you can jump into the container with:

```bash
docker exec -it dev-rucio-1 /bin/bash
```

To verify that everything is in order, you can now either run the full
unit tests or only set up the database. Running the full testing suite
takes \~10 minutes:

```bash
tools/run_tests.sh
```

Alternatively, you can bootstrap the test environment once with the
`-i`option and then selectively or repeatedly run test case
modules, test case groups, or even single test cases, for example:

```bash
tools/run_tests.sh -i
pytest -v --full-trace lib/rucio/tests/test_replica.py
pytest -v --full-trace lib/rucio/tests/test_replica.py:TestReplicaCore
pytest -v --full-trace lib/rucio/tests/test_replica.py:TestReplicaCore.test_delete_replicas_from_datasets
```

## Using the environment including storage

Again run the containers using docker-compose:

```bash
docker-compose --file etc/docker/dev/docker-compose.yml --profile storage up -d
```

This should show you a few more running containers: the Rucio server,
the PostgreSQL database, FTS and its associated MySQL database, the
Graphite monitoring, and three XrootD storage servers.

With this container you can upload and download data to/from the storage
and submit data transfers. To set this up, add the `-r`
option to the setup.

```bash
tools/run_tests.sh -ir
```

This creates a few random files and uploads them, creates a few datasets
and containers, and requests a replication rule for the container, which
starts in state REPLICATING. To demonstrate the transfer capability, the
daemons can be run in single-execution mode in order:

```bash
rucio rule-info <rule-id>

rucio-conveyor-submitter --run-once
rucio-conveyor-poller --run-once  --older-than 0
rucio-conveyor-finisher --run-once

rucio rule-info <rule-id>
```

On the second display of the rule, its state has cleared to OK.

## Using the environment including monitoring

Again run the containers using docker-compose:

```bash
docker-compose --file etc/docker/dev/docker-compose.yml --profile storage --profile monitoring up -d
```

Now you will have the same containers as before plus a full monitoring
stack with Logstash, Elasticsearch, Kibana and Grafana.

To create some events and write them to Elasticsearch first run again
the tests as before:

```bash
tools/run_tests.sh -ir
```

Then you will have to run the transfer daemons (conveyor-\*) and
messaging daemon (hermes) to send the events to ActiveMQ. There a script
for that which repeats these daemons in single execution mode from the
section in a loop:

```bash
run_daemons
```

When all the daemons ran you will be able to find the events in Kibana.
If you run the docker environment on you local machine you can access
Kibana at `<http://localhost:5601>`. The necessary index pattern will be
added automatically. There is also one dashboard available in Kibana. If
it is running on remote machine you can SSH forward it:

```bash
ssh -L 5601:127.0.0.1:5601 <hostname>
```

Additionally, there is also a Grafana server running with one simple
dashboard. You can access it at `<http://localhost:3000>`. The default
credentials are \"admin/admin\". Also ActiveMQ web console can be
accessed at `<http://localhost:8161>`.

If you would like to continuously create some transfers and events there
are scripts available for that. Open two different shells and in one
run:

```bash
create_monit_data
```

And in the other run:

```bash
run_daemons
```

## Development

The idea for containerised development is that you use your host machine
to edit the files, and test the changes within the container
environment. On your host machine, you should be able to:

```bash
cd ~/dev/rucio
emacs <file>
```

To see your changes in action the recommended way is to jump twice into
the container in parallel. One terminal to follow the output of the
Rucio server with a shortcut to tail the logfiles
(`logshow`), and one terminal to actually run interactive
commands:

From your host, get a separate Terminal 1 (the Rucio \"server log
show\"):

```bash
docker exec -it dev-rucio-1 /bin/bash
logshow
```

Terminal 1 can now be left open, and then from your host go into a new
Terminal 2 (the \"interactive\" terminal):

```bash
docker exec -it dev-rucio-1 /bin/bash
rucio whoami
```

The command will output in Terminal 2, and at the same time the server
debug output will be shown in Terminal 1.

The same `logshow` is also available in the FTS container:

```bash
docker exec -it dev-fts-1 /bin/bash
logshow
```

## Development tricks

### Server changes

If you edit server-side files, e.g. in `lib/rucio/web`, and
your changes are not showing up then it is usually helpful to flush the
memcache and force the webserver to restart without having to restart
the container. Inside the container execute:

```bash
echo 'flush_all' | nc localhost 11211 && httpd -k graceful**
```

### Database access

The default database is PostgreSQL, and `docker-compose` is
configured to open its port to the host machine. Using your favourite
SQL navigator, e.g., [__DBeaver__](https://dbeaver.io), you can connect to
the database using the default access on `localhost:5432` to
database name `rucio`, schema name `dev`, with
username `rucio` and password `secret`.

### Docker is eating my disk space

You can reclaim this with:

```bash
docker system prune -f --volumes
```

### Where do I find the Dockerfile

This container can be found on Dockerhub as
`rucio/rucio-dev`, and the corresponding
[__Dockerfile__](https://github.com/rucio/containers/tree/master/dev) is
also available. It provides a Rucio environment which allows you to
mount your local code in the containers `bin`,
`lib`, and `tools` directory. The container is
set up to run against a PostgreSQL database with fsync and most
durability features for the WAL disabled to improve testing IO
throughput. Tests and checks can be run against the development code
without having to rebuild the container.

### I need a Docker based on another branch! (not rucio/master)

In such cases, you can download the Rucio container files and e.g. choose
to modify the dev container before build:

```bash
cd /opt
sudo git clone https://github.com/rucio/containers
cd ../containers/dev
```

Change anything you need, e.g. the code branch cloned to your docker
container:

```bash
# from
RUN git clone https://github.com/rucio/rucio.git /tmp/rucio
# to e.g.:
RUN git clone --single-branch --branch next https://github.com/rucio/rucio.git /tmp/rucio
#build your docker
sudo docker build -t rucio/rucio-dev
```

Compose as usual using docker-compose:

```bash
cd /opt/rucio
sudo docker-compose --file etc/docker/dev/docker-compose.yml up -d
```

Start the daemons

Daemons are not running in the docker environment, but all daemons
support single-execution mode with the \--run-once argument. Reset the
system first with:

```bash
tools/run_tests.sh -ir
```

Some files are created. Let\'s add them to a new dataset:

```bash
rucio add-dataset test:mynewdataset
rucio attach test:mynewdataset test:file1 test:file2 test:file3 test:file4
```

If you run the command below, the files are not in the RSE XRD3, but
only in XRD1 and 2.:

```bash
$ **rucio list-file-replicas test:mynewdataset**
+-------+-------+-----------+----------+-----------------------------------+
| SCOPE | NAME  | FILESIZE  | ADLER32  | RSE: REPLICA                      |
|-------|-------|-----------|----------|-----------------------------------|
| test  | file1 | 10.486 MB | 141a641e | XRD1: root://xrd1:1094//rucio/... |
| test  | file2 | 10.486 MB | fdfa7eea | XRD1: root://xrd1:1094//rucio/... |
| test  | file3 | 10.486 MB | c669167d | XRD2: root://xrd2:1095//rucio/... |
| test  | file4 | 10.486 MB | 65786e49 | XRD2: root://xrd2:1095//rucio/... |
+-------+-------+-----------+----------+-----------------------------------+
```

So let\'s add a new rule on our new dataset to oblige Rucio to create
replicas also on XRD3:

```bash
rucio add-rule test:mynewdataset 1 XRD3**
1aadd685d891400dba050ad43e71fea9**
```

Now we can check the status of the rule. We will see there are 4 files
in `Replicating` state:

```bash
rucio rule-info 1aadd685d891400dba050ad43e71fea9|grep Locks**
Locks OK/REPLICATING/STUCK: 0/4/0**
```

Now we can run the daemons. First the rule evaluation daemon
(judge-evaluator) will pick up our rule. Then the transfer submitter
daemon (conveyor-submitter) will send the newly created transfers
requests to the FTS server. After that, the transfer state check daemon
(conveyor-poller) will retrieve from FTS the transfer state information.
Finally, the transfer sign-off daemon (conveyor-finisher) updates the
internal state of the Rucio catalogue to reflect the changes.:

```bash
rucio-judge-evaluator --run-once**
rucio-conveyor-submitter --run-once**
rucio-conveyor-poller --run-once**
rucio-conveyor-finisher --run-once**
```

If we see the state of the rule now, we see the locks are OK:

```bash
rucio rule-info 1aadd685d891400dba050ad43e71fea9|grep Locks
Locks OK/REPLICATING/STUCK: 4/0/0**
```

And if we look at the replicas of the dataset, we see the there are
replicas of the files also in XRD3:

```bash
$ rucio list-file-replicas test:mynewdataset
+-------+-------+-----------+----------+-----------------------------------+
| SCOPE | NAME  | FILESIZE  | ADLER32  | RSE: REPLICA                      |
|-------|-------|-----------|----------|-----------------------------------|
| test  | file1 | 10.486 MB | 141a641e | XRD3: root://xrd3:1096//rucio/... |
| test  | file1 | 10.486 MB | 141a641e | XRD1: root://xrd1:1094//rucio/... |
| test  | file2 | 10.486 MB | fdfa7eea | XRD3: root://xrd3:1096//rucio/... |
| test  | file2 | 10.486 MB | fdfa7eea | XRD1: root://xrd1:1094//rucio/... |
| test  | file3 | 10.486 MB | c669167d | XRD2: root://xrd2:1095//rucio/... |
| test  | file3 | 10.486 MB | c669167d | XRD3: root://xrd3:1096//rucio/... |
| test  | file4 | 10.486 MB | 65786e49 | XRD2: root://xrd2:1095//rucio/... |
| test  | file4 | 10.486 MB | 65786e49 | XRD3: root://xrd3:1096//rucio/... |
+-------+-------+-----------+----------+-----------------------------------+
```
---
id: did_meta
title: DID Metadata
sidebar_label: DID Metadata
---

Rucio supports adding Metadata on the dids.

Example:

```bash
# Create a dataset to use on the Example
$ rucio add-dataset mock:testing_metadata

# Add 'optimized' metadata that exist as columns in the did table
$ rucio set-metadata --did mock:testing_metadata --key panda_id --value 9999

# Add 'generic' metadata. If there is no custom metadata plugin,
# the plugin 'JSON' will be used
$ rucio set-metadata --did mock:testing_metadata --key random_key_name --value 8888

# Get the 'optimized' metadata
$ rucio get-metadata mock:testing_metadata

# Get the Generic metadata
$ rucio get-metadata mock:testing_metadata --plugin JSON

# Get all the metadata
$ rucio get-metadata mock:testing_metadata --plugin ALL

# List dids according to metadata
$ rucio list-dids-extended mock:* --filter "type=ALL,panda_id=9999"
$ rucio list-dids-extended mock:* --filter "type=ALL,random_key_name=8888"
```

Even though regular users use metadata out of the box using the CLI, advanced
users and Rucio admins should be aware that in the backend there are multiple
options on how to store and manage the did metadata per experiment needs.

The concepts of DID Metadata Plugins exists on Rucio. While deploying the Rucio
server you can configure which existing did plugins to use or even develop your
own.

The default plugin in use the one originally developed for the needs of ATLAS,
stores the metadata on fixed columns on the DID table and is the most optimal
for the specific metadata.

Another option available is the JSON metadata plugin which stores the metadata
in JSON blobs in the relational databased used by the Rucio Server.

When you are trying to add or fetch a VALUE for a given KEY, Rucio which asks in
order each configured metadata plugin if it supports this KEY.

## How to develop a custom metadata solution

The module you develop needs to extend the
[DidMetaPlugin](https://github.com/rucio/rucio/blob/master/lib/rucio/core/did_meta_plugins/did_meta_plugin_interface.py)
Abstract class. The methods needed are:

```python
get_metadata(scope, name, session=None)
"""
Returns metadata stored in Plugin for given scope:name
"""

set_metadata(scope, name, key, value, recursive, session=None)
"""
Sets the metadata in Plugin for given scope:name
"""

delete_metadata(scope, name, key, session=None)
"""
Removes the metadata from the Plugin for given scope:name
"""

list_dids(scope, filters, type='collection', ignore_case=False, limit=None, \
  offset=None, long=False, recursive=False, session=None)
"""
Returns a list of dids for given filters.
For long = True return should be a list of dictionaries having the keys 'scope',
'name', 'did_type', 'bytes', 'length'.
For long = False return should be a list of strings containing the did names.
"""

manages_key(key, session=None)
"""
Returns if Plugin is willing to manage metadata with given KEY.
Some Plugins might decide to accept only specific hardcoded keys, others might match
against a particular regex while other might accept all possible keys.
"""
```

## How to configure which metadata plugin to use

Configuration options for Metadata are:

```python
[metadata]
# plugins = list_of_plugins,comma_separated
plugins = rucio.core.did_meta_plugins.did_column_meta.DidColumnMeta,escape.rucio.did_meta_plugin
```


## API Documentation

[The API for existing metadata plugins can be found here](pathname:///html/did_meta_plugins/did_column_meta.html)---
id: configuration
title: Configuration
sidebar_label: Configuration
sidebar_position: 5
---

## Prerequisites

You need to have a Rucio server up and running with the root account
created. Please refer to [installation documentation](operator/installing_server.md) for
further information

## Creating new users

The first step is to create new accounts:

```bash
  $ rucio-admin account add --type USER --email jdoe@blahblih.com jdoe
```

You can choose from different types in the list USER, GROUP, SERVICE. Different
policies/permissions can be set depending on the account type.  Once the account
is created, you need to create and attach an identity to this account:

```bash
  $ rucio-admin identity add --type X509 \
      --id "CN=jdoe,OU=Users,OU=Organic Units,DC=blih,DC=blah" \
      --email jdoe@blahblih.com --account jdoe
```

The list of possible identity types is X509, GSS, USERPASS, SSH, OIDC:

```bash
  $ rucio-admin account list-identities jdoe
  Identity: CN=jdoe,OU=Users,OU=Organic Units,DC=blih,DC=blah,        type: X509
```

You can set attributes to the users:

```bash
  $ rucio-admin account add-attribute --key country --value xyz jdoe
```

And list these attributes:

```bash
  $ rucio-admin account list-attributes jdoe
  +---------+-------+
  | Key     | Value |
  |---------+-------|
  | country | xyz   |
  +---------+-------+
```

You can also list all the accounts matching a certain attribute using the filter
option:

```bash
  $ rucio-admin account list --filters "country=xyz"
  jdoe
```

### X509 identity format

By default, X509 identities must be formatted according to the relevant RFCs: a
comma-separated list of the DN components, ordered last-to-first (e.g.
`CN=jdoe,OU=Users,OU=Organic Units,DC=blih,DC=blah`).  However, operators might
prefer to store them in the legacy format: a slash-separated list of the DN
components, starting with a slash, ordered first-to-last (e.g.
`/DC=blah/DC=blih/OU=Organic Units/OU=Users/CN=jdoe`).

To do so, it is necessary to enable the `LegacyDNStringFormat` configuration
option of mod_ssl.  When using the official Rucio container images, one must set
the `RUCIO_HTTPD_LEGACY_DN` environmental variable to `True`.  For custom
installations, one must edit the appropriate Apache configuration file so that
the `SSLOptions` directive looks like this:

```
SSLOptions +StdEnvVars +LegacyDNStringFormat
```

## Creating scope

One needs then to create some scopes associated with the accounts:

```bash
  $ rucio-admin scope add --account jdoe --scope user.jdoe
```

Only the owner of the scope or privileged users can write into the scope.

To list all the scopes:

```bash
  $ rucio-admin scope list
  user.janedoe
  user.jdoe
```

## Creating new RSEs

To create a new RSE:

```bash
  $ rucio-admin rse add SITE3_DISK
  Added new RSE: SITE3_DISK
```

Then you can attach protocols to this RSE. In the following example, a file
protocol is added to the site created previously:

```bash
  $ rucio-admin rse add-protocol --hostname blahblah --scheme file \
      --impl rucio.rse.protocols.posix.Default --domain-json \
      '{"wan": {"read": 1, "write": 1, "third_party_copy": 0, "delete": 1}, \
      "lan": {"read": 1, "write": 1, "third_party_copy": 0, "delete": 1}}' \
      --prefix /tmp/SITE3_DISK/ SITE3_DISK
```

The different parameters are explained in more details if you use the --help
option.

Last step is to create RSE attributes that can be used to build RSE expressions:

```bash
  $ rucio-admin rse set-attribute --rse SITE3_DISK --key tier --value 1
  Added new RSE attribute for SITE3_DISK: tier-1
  $ rucio-admin rse set-attribute --rse SITE3_DISK --key disk --value 1
  Added new RSE attribute for SITE3_DISK: disk-1
  $ rucio list-rses --rses "disk=1&tier=1"
  SITE3_DISK
```

Let's check that everything is properly defined:

```bash
  $ rucio-admin rse info SITE3_DISK
  Settings:
  =========
    third_party_copy_protocol: 1
    rse_type: DISK
    domain: [u'lan', u'wan']
    availability_delete: True
    delete_protocol: 1
    rse: SITE3_DISK
    deterministic: True
    write_protocol: 1
    read_protocol: 1
    staging_area: False
    credentials: None
    availability_write: True
    lfn2pfn_algorithm: default
    availability_read: True
    volatile: False
    id: 4079d6873603462b8867e4a49674cc11
  Attributes:
  ===========
    tier: True
    disk: True
    istape: False
    SITE3_DISK: True
  Protocols:
  ==========
    file
      extended_attributes: None
      hostname: blahblih
      prefix: /tmp/SITE3_DISK/
      domains: {u'wan': {u'read': 1, u'write': 1, u'third_party_copy': 0, \
        u'delete': 1}, u'lan': {u'read': 1, u'write': 1, u'delete': 1}}
      scheme: file
      port: 0
      impl: rucio.rse.protocols.posix.Default
  Usage:
  ======
    rucio
      used: 0
      rse: SITE3_DISK
      updated_at: 2018-02-22 13:05:45
      free: None
      source: rucio
      total: 0
```

## Setting quota and permissions

The root account has all the privileges. You can define other admin accounts by
setting the account attribute admin:

```bash
  $ rucio-admin account add-attribute --key admin --value 1 jdoe
  $ rucio-admin account list --filter "admin=1"
  jdoe
```

The permissions are easily tunable by overloading the [generic permission file](https://github.com/rucio/rucio/blob/master/lib/rucio/core/permission/generic.py).

This is an advanced feature that is not explained there, for more details get in
touch with the developers.

To set the quota for one account on a given RSE:

```bash
  $ rucio-admin account set-limits jdoe SITE3_DISK 10000000000000
  Set account limit for account jdoe on SITE3_DISK: 10.000 TB
  $ rucio-admin account get-limits jdoe SITE3_DISK
  Quota on SITE3_DISK for jdoe : 10 TB
```
---
id: installing_daemons
title: Installing Rucio Daemons
sidebar_position: 4
---

## Prerequisites

The Rucio daemons run on Python 2.7, 3.6 and 3.7 on any Unix-like
platform.

## Install via pip

Heads up: We recommend to use the docker-based install (see next
section) as it will configure many things for you automatically. Only
use the pip-based install if you have a good reason and know how to
configure your web service manually:

```bash
pip install rucio
```

This will pull the latest release from
[__PyPi__](https://pypi.python.org/pypi/rucio/). The Rucio server also needs
several Python dependencies. These are all listed in the file
[`requirements.server.txt`](https://github.com/rucio/rucio/blob/master/requirements/requirements.server.txt)
and will be pulled in as necessary.

## Install via Docker

This image provides the Rucio daemons. Each daemon has to be run in a
separate container. It supports MySQL, PostgreSQL, Oracle, and SQLite as
database backends.

This image expects that there is an already initialised Rucio DB. To
start a simple `judge-cleaner` daemon using a database on
`mysql.db` without any additional parameters just run this:

```bash
docker run --name=rucio-judge-cleaner \
  -e RUCIO_CFG_DATABASE_DEFAULT="mysql+pymysql://rucio:rucio@mysql.db/rucio" \
  -e RUCIO_DAEMON=judge-cleaner \
  rucio/rucio-daemons
```

The `RUCIO_DAEMON` environment variable gives the name of
the rucio daemon.

Rucio can be configured fully using environment variables like
`RUCIO_CFG_DATABASE_DEFAULT`. If you want to instead use a
complete rucio.cfg it can also be mounted. This will then ignore the
`RUCIO_CFG` environment variables:

The rucio.cfg is used to configure the database backend and the daemons:

```bash
docker run --name=rucio-judge-cleaner \
  -v /tmp/rucio.cfg:/opt/rucio/etc/rucio.cfg \
  -e RUCIO_DAEMON=judge-cleaner \
  rucio/rucio-daemons
```

By default the daemon logs are written to stdout and stderr if you want
to write to a file you can use `RUCIO_ENABLE_LOGS` like
this:

```bash
docker run --name=rucio-judge-cleaner \
  -v /tmp/rucio.cfg:/opt/rucio/etc/rucio.cfg \
  -v /tmp/logs:/var/log/rucio -e RUCIO_DAEMON=judge-cleaner \
  -e RUCIO_ENABLE_LOGS=True \
  rucio/rucio-daemons
```

## Environment Variables

As shown in the examples above the rucio-daemon image can be configured
using environment variables that are passed with `docker run`.
Below is a list of all available variables and their behaviour:

### RUCIO_DAEMON

This variable is mandatory and it specifies the name of the daemon,
e.g., `hermes`, `kronos`,
`judge-evaluator`, etc.

### RUCIO_DAEMON_ARGS

Any additional command line parameter can be specified here, e.g.,
`\--run-once` This field is optional.

### RUCIO_ENABLE_LOGS

By default, the log output of the daemon is written to stdout and
stderr. If you set this variable to `True` the output will
be written to `access_log` and `error_log` under
`/var/log/rucio`

### RUCIO_CFG configuration parameters

Environment variables can be used to set values for the auto-generated
rucio.cfg. The names are derived from the actual names in the
configuration file prefixed by `RUCIO_CFG`, e.g., the
`default` value in the `database` section
becomes `RUCIO_CFG_DATABASE_DEFAULT`. All available
environment variables are:

- RUCIO_CFG_ACCOUNTS_SPECIAL_ACCOUNTS
- RUCIO_CFG_COMMON_LOGDIR
- RUCIO_CFG_COMMON_LOGLEVEL
- RUCIO_CFG_COMMON_MAILTEMPLATEDIR
- RUCIO_CFG_DATABASE_DEFAULT
- RUCIO_CFG_DATABASE_SCHEMA
- RUCIO_CFG_DATABASE_SCHEMA
- RUCIO_CFG_DATABASE_POOL_RESET_ON_RETURN
- RUCIO_CFG_DATABASE_ECHO
- RUCIO_CFG_DATABASE_POOL_RECYCLE
- RUCIO_CFG_DATABASE_POOL_SIZE
- RUCIO_CFG_DATABASE_POOL_TIMEOUT
- RUCIO_CFG_DATABASE_MAX_OVERFLOW
- RUCIO_CFG_DATABASE_POWUSERACCOUNT
- RUCIO_CFG_DATABASE_POWUSERPASSWORD
- RUCIO_CFG_MONITOR_CARBON_SERVER
- RUCIO_CFG_MONITOR_CARBON_PORT
- RUCIO_CFG_MONITOR_USER_SCOPE
- RUCIO_CFG_PERMISSION_POLICY
- RUCIO_CFG_PERMISSION_SCHEMA
- RUCIO_CFG_PERMISSION_LFN2PFN_ALGORITHM_DEFAULT
- RUCIO_CFG_PERMISSION_SUPPORT
- RUCIO_CFG_PERMISSION_SUPPORT_RUCIO
- RUCIO_CFG_AUTOMATIX_SITES
- RUCIO_CFG_AUTOMATIX_SLEEP_TIME
- RUCIO_CFG_AUTOMATIX_DATASET_LIFETIME
- RUCIO_CFG_AUTOMATIX_SET_METADATA
- RUCIO_CFG_AUDITOR_RESULTS
- RUCIO_CFG_AUDITOR_CACHE
- RUCIO_CFG_CONVEYOR_SCHEME
- RUCIO_CFG_CONVEYOR_TRANSFERTOOL
- RUCIO_CFG_CONVEYOR_FTSHOSTS
- RUCIO_CFG_CONVEYOR_CACERT
- RUCIO_CFG_CONVEYOR_USERCERT
- RUCIO_CFG_CONVEYOR_CACHE_TIME
- RUCIO_CFG_CONVEYOR_USE_DETERMINISTIC_ID
- RUCIO_CFG_CONVEYOR_POLL_TIMEOUT
- RUCIO_CFG_CONVEYOR_SUBMIT_TIMEOUT
- RUCIO_CFG_CONVEYOR_BRING_ONLINE
- RUCIO_CFG_CONVEYOR_QUEUE_MODE
- RUCIO_CFG_CONVEYOR_USING_MEMCACHE
- RUCIO_CFG_CONVEYOR_FTSMONHOSTS
- RUCIO_CFG_MESSAGING_FTS3_PORT
- RUCIO_CFG_MESSAGING_FTS3_SSL_KEY_FILE
- RUCIO_CFG_MESSAGING_FTS3_SSL_CERT_FILE
- RUCIO_CFG_MESSAGING_FTS3_DESTINATION
- RUCIO_CFG_MESSAGING_FTS3_BROKERS
- RUCIO_CFG_MESSAGING_FTS3_VONAME
- RUCIO_CFG_MESSAGING_HERMES_USERNAME
- RUCIO_CFG_MESSAGING_HERMES_PASSWORD
- RUCIO_CFG_MESSAGING_HERMES_PORT
- RUCIO_CFG_MESSAGING_HERMES_NONSSL_PORT
- RUCIO_CFG_MESSAGING_HERMES_USE_SSL
- RUCIO_CFG_MESSAGING_HERMES_SSL_KEY_FILE
- RUCIO_CFG_MESSAGING_HERMES_SSL_CERT_FILE
- RUCIO_CFG_MESSAGING_HERMES_DESTINATION
- RUCIO_CFG_MESSAGING_HERMES_BROKERS
- RUCIO_CFG_MESSAGING_HERMES_VONAME
- RUCIO_CFG_MESSAGING_HERMES_EMAIL_FROM
- RUCIO_CFG_MESSAGING_HERMES_EMAIL_TEST
- RUCIO_CFG_TRACER_KRONOS_BROKERS
- RUCIO_CFG_TRACER_KRONOS_PORT
- RUCIO_CFG_TRACER_SSL_KEY_FILE
- RUCIO_CFG_TRACER_SSL_CERT_FILE
- RUCIO_CFG_TRACER_QUEUE
- RUCIO_CFG_TRACER_PREFETCH_SIZE
- RUCIO_CFG_TRACER_CHUNKSIZE
- RUCIO_CFG_TRACER_SUBSCRIPTION_ID
- RUCIO_CFG_TRACER_USE_SSL
- RUCIO_CFG_TRACER_RECONNECT_ATTEMPTS
- RUCIO_CFG_TRACER_EXCLUDED_USRDNS
- RUCIO_CFG_TRACER_KRONOS_USERNAME
- RUCIO_CFG_TRACER_KRONOS_PASSWORD
- RUCIO_CFG_TRACER_DATASET_WAIT
- RUCIO_CFG_MESSAGING_CACHE_PORT
- RUCIO_CFG_MESSAGING_CACHE_SSL_KEY_FILE
- RUCIO_CFG_MESSAGING_CACHE_SSL_CERT_FILE
- RUCIO_CFG_MESSAGING_CACHE_DESTINATION
- RUCIO_CFG_MESSAGING_CACHE_BROKERS
- RUCIO_CFG_MESSAGING_CACHE_VONAME
- RUCIO_CFG_MESSAGING_CACHE_ACCOUNT
- RUCIO_CFG_CREDENTIALS_GCS
- RUCIO_CFG_CREDENTIALS_SIGNATURE_LIFETIME
---
id: installing_server
title: Installing Rucio Server
sidebar_position: 3
---

## Prerequisites

The Rucio server runs on Python 2.7, 3.6 and 3.7 on any Unix-like platform.

## Install via pip

Heads up: We recommend to use the docker-based install (see next section) as it
will configure many things for you automatically. Only use the pip-based install
if you have a good reason and know how to configure your webservices manually:

```bash
pip install rucio
```

This will pull the latest release from
[__PyPi__](https://pypi.python.org/pypi/rucio/). The Rucio server also needs
several Python dependencies. These are all listed in the file
[`requirements.server.txt`](https://github.com/rucio/rucio/blob/master/requirements/requirements.server.txt)
and will be pulled in as necessary.

## Install via Docker

A simple server without SSL can be started like this:

```bash
docker run --name=rucio-server -p 80:80 -d rucio/rucio-server
```

This will start up a simple server using sqlite based on an automatically
generated configuration. You can check if the server is running with

```bash
curl http://localhost/ping
```

This should return the Rucio version used in the container. Any other curl
requests will not work as the database backend is not initialized as this image
is meant to be used with an already bootstrapped database backend. I.e., that
the container has to be configured to point to the correct database. There are
two ways to manage the Rucio configuration: using environment variables or by
mounting a full rucio.cfg.

If you want to set the connection string for the database it can be done using
the `RUCIO_CFG_DATABASE_DEFAULT` environment variable, e.g., to start a
container connecting to a MySQL DB running at `mysql.db` you could use something
like this:

```bash
docker run --name=rucio-server \
  -e RUCIO_CFG_DATABASE_DEFAULT="mysql+pymysql://rucio:rucio@mysql.db/rucio" \
  -p 80:80 \
  -d \
  rucio/rucio-server
```

The are much more configuration parameters available that will be listed at the
end of this readme.

Another way to configure Rucio is to directly mount a complete rucio.cfg into
the container. This will then be used instead of the auto-generated one, e.g.,
if you have a rucio.cfg ready on your host system under `/tmp/rucio.cfg` you
could start a container like this:

```bash
docker run --name=rucio-server \
  -v /tmp/rucio.cfg:/opt/rucio/etc/rucio.cfg \
  -p 80:80 \
  -d \
  rucio/rucio-server
```

The rucio.cfg is used to configure the database backend.

If you want to enable SSL you would need to set the `RUCIO_ENABLE_SSL` variable
and also need to include the host certificate, key and the the CA certificate as
volumes. E.g.,:

```bash
docker run --name=rucio-server \
  -v /tmp/ca.pem:/etc/grid-security/ca.pem \
  -v /tmp/hostcert.pem:/etc/grid-security/hostcert.pem \
  -v /tmp/hostkey.pem:/etc/grid-security/hostkey.pem \
  -v /tmp/rucio.cfg:/opt/rucio/etc/rucio.cfg \
  -p 443:443 \
  -e RUCIO_ENABLE_SSL=True \
  -d \
  rucio/rucio-server
```

By default the output of the Apache web server is written directly to stdout and
stderr. If you would rather direct them into separate files it can be done using
the `RUCIO_ENABLE_LOGS` variable. The storage folder of the logs can be used as
a volume:

```bash
docker run --name=rucio-server \
  -v /tmp/rucio.cfg:/opt/rucio/etc/rucio.cfg \
  -v /tmp/logs:/var/log/httpd \
  -p 80:80 \
  -e RUCIO_ENABLE_LOGS=True \
  -d \
  rucio/rucio-server
```

## Environment Variables

As shown in the examples above the rucio-server image can be configured using
environment variables that are passed with `docker run`. Below is a list of all
available variables and their behavior:

### RUCIO_ENABLE_SSL

By default, the rucio server runs without SSL on port 80. If you want to enable
SSL set this variable to `True`. If you enable SSL you will also have to provide
the host certificate and key and the certificate authority file. The server will
look for `hostcert.pem`, `hostkey.pem` and `ca.pem` under `/etc/grid-security`
so you will have to mount them as volumes. Furthermore you will also have to
expose port 443.

### RUCIO_SSL_PROTOCOL

By default, the server will only accept TLSv1.2 connections. Defining this
environment variable will allow specification of a custom Apache SSLProtocol.

### RUCIO_CA_PATH

If you are using SSL and want use `SSLCACertificatePath` and
`SSLCARevocationPath` you can do so by specifying the path in this variable.

### RUCIO_DEFINE_ALIASES

By default, the web server is configured with all common rest endpoints except
the authentication endpoint. If you want to specify your own set of aliases you
can set this variable to `True`. The web server then expects an alias file under
`/opt/rucio/etc/aliases.conf`

### RUCIO_ENABLE_LOGS

By default, the log output of the web server is written to stdout and stderr. If
you set this variable to `True` the output will be written to `access_log` and
`error_log` under `/var/log/httpd`.

### RUCIO_LOG_LEVEL

The default log level is `info`. You can change it using this
variable.

### RUCIO_LOG_FORMAT

The default rucio log format is
`%ht%tt%{X-Rucio-Forwarded-For}it%Tt%Dt\"%{X-Rucio-Auth-Token}i\"t%{X-Rucio-RequestId}it%{X-Rucio-Client-Ref}it\"%r\"t%\>st%b`
You can set your own format using this variable.

### RUCIO_HOSTNAME

This variable sets the server name in the apache config.

### RUCIO_SERVER_ADMIN

This variable sets the server admin in the apache config.

## RUCIO_CFG configuration parameters

Environment variables can be used to set values for the auto-generated
rucio.cfg. The names are derived from the actual names in the configuration file
prefixed by `RUCIO_CFG`, e.g., the `default` value in the `database` section
becomes `RUCIO_CFG_DATABASE_DEFAULT`. All available environment variables are:

- RUCIO_CFG_COMMON_LOGDIR
- RUCIO_CFG_COMMON_LOGLEVEL
- RUCIO_CFG_COMMON_MAILTEMPLATEDIR
- RUCIO_CFG_DATABASE_DEFAULT
- RUCIO_CFG_DATABASE_SCHEMA
- RUCIO_CFG_DATABASE_POOL_RESET_ON_RETURN
- RUCIO_CFG_DATABASE_ECHO
- RUCIO_CFG_DATABASE_POLL_RECYCLE
- RUCIO_CFG_DATABASE_POOL_SIZE
- RUCIO_CFG_DATABASE_POOL_TIMEOUT
- RUCIO_CFG_DATABASE_MAX_OVERFLOW
- RUCIO_CFG_DATABASE_POWUSERACCOUNT
- RUCIO_CFG_DATABASE_USERPASSWORD
- RUCIO_CFG_MONITOR_CARBON_SERVER
- RUCIO_CFG_MONITOR_CARBON_PORT
- RUCIO_CFG_MONITOR_USER_SCOPE
- RUCIO_CFG_TRACE_TRACEDIR
- RUCIO_CFG_TRACE_BROKERS
- RUCIO_CFG_TRACE_PORT
- RUCIO_CFG_TRACE_USERNAME
- RUCIO_CFG_TRACE_PASSWORD
- RUCIO_CFG_TRACE_TOPIC
- RUCIO_CFG_PERMISSION_POLICY
- RUCIO_CFG_PERMISSION_SCHEMA
- RUCIO_CFG_PERMISSION_LFN2PFN_ALGORITHM_DEFAULT
- RUCIO_CFG_PERMISSION_SUPPORT
- RUCIO_CFG_PERMISSION_SUPPORT_RUCIO
- RUCIO_CFG_WEBUI_USERCERT

## Server Configuration for Open ID Connect AuthN/Z

In order to be able to use [OIDC](https://openid.net/connect/)
JSON Web Tokens ([JWTs](https://en.wikipedia.org/wiki/JSON_Web_Token)) and
related [OAuth2.0](https://oauth.net/2/) authentication and authorization with Rucio,
one first needs to have an account with an Identity Provider (IdP)
which will act as Rucio admin account representing the Rucio Application.
Currently, the only fully supported IdP is [INDIGO IAM](https://indigo-iam.github.io/v/current/).
Once you have got your Rucio Service IAM Account [A]
(and its subject claim identifier), you will need to [register two IAM Rucio
clients](https://indigo-iam.github.io/docs/v/current/user-guide/client-registration.html)
linked to this account. Please save the relevant __client_id__,
__client_secret__, and __registration access token (RAT)__ in
a safe place, as you will be needing them.

In both clients, one needs to setup the __redirect_uris__ to
include the following paths:

```bash
https://<your_server_name>/auth/oidc_token
https://<your_server_name>/auth/oidc_code
```

We will use one client as
__Rucio Auth IAM Client__ [C1] (i.e. client for the authentication and
authorization on the Rucio server). This client needs to have __token_exchange__,
__token_refresh__, and __authorization_code__ grants enabled. For __token_exchange__
and __token_refresh__ you might need to contact the IAM admin as such settings are
usually not accessible to IAM users. In addition, you will need to request your
IAM admin to allow your client returning refresh tokens with lifetime being visible
in their unverified header. In addition Rucio assumes refresh tokens to expire
immediately after their first use, which has to be also confirmed by your IAM admin.

The second
client, let's call it __Rucio Admin IAM Client__ [C2], can be used by a Rucio probe
script (e.g. [check_scim](https://github.com/rucio/probes/blob/master/attic/check_scim),
[sync_iam_rucio](https://github.com/ESCAPE-WP2/Utilities-and-Operations-Scripts/blob/master/iam-rucio-sync/sync_iam_rucio.py))
in order to synchronize existing Rucio accounts with Rucio
identities. Rucio will also use this client's credentials in order to request
tokens for itself. The IAM administrator must include the __scim:read__ scope and
allow the __client_credentials__ grant type for [C2] in order
to grant you rights to pre-provision IAM users for Rucio. Examples of the
configuration of these two clients follow below:

Example of the __Rucio Auth IAM Client__ [C1] configuration:

```json
{
  "client_id": "AbcCDe123...",
  "registration_access_token": "AbcCDe123...",
  "redirect_uris": [
    "https://rucio-auth.cern.ch/auth/oidc_token",
    "https://rucio-auth.cern.ch/auth/oidc_code",
  ],
  "client_name": "rucio-admin-client",
  "client_uri": null,
  "logo_uri": null,
  "contacts": [
    "jaroslav.guenther@gmail.com"
  ],
  "tos_uri": null,
  "token_endpoint_auth_method": "client_secret_basic",
  "scope": "address fts phone openid profile offline_access \
    rucio email wlcg wlcg.groups fts:submit-transfer",
  "grant_types": [
    "refresh_token",
    "urn:ietf:params:oauth:grant-type:token-exchange",
    "authorization_code"
  ],
  "response_types": [
    "code"
  ],
  "policy_uri": null,
  "jwks_uri": null,
  "jwks": null,
  "jwksType": "URI",
  "application_type": null,
  "sector_identifier_uri": null,
  "subject_type": null,
  "request_object_signing_alg": null,
  "userinfo_signed_response_alg": null,
  "userinfo_encrypted_response_alg": null,
  "userinfo_encrypted_response_enc": null,
  "id_token_signed_response_alg": null,
  "id_token_encrypted_response_alg": null,
  "id_token_encrypted_response_enc": null,
  "default_max_age": 60000,
  "require_auth_time": true,
  "default_acr_values": null,
  "initiate_login_uri": null,
  "post_logout_redirect_uris": null,
  "claims_redirect_uris": [],
  "request_uris": [],
  "software_statement": null,
  "software_id": null,
  "software_version": null,
  "code_challenge_method": null,
  "registration_client_uri": "https://wlcg.cloud.cnaf.infn.it/register/fdc297fc-0907-4a68-9022-3ccc7dd2501a",
  "client_secret_expires_at": 0,
  "client_id_issued_at": 1574700620
}
```

Example of the __Rucio Admin IAM Client__ [C2] configuration:

```bash
{
  "client_id": "AbcDe123...",
  "registration_access_token": "AbcDe123...",
  "client_secret": "AbcDe123...",
  "redirect_uris": [],
  "client_name": null,
  "client_uri": null,
  "logo_uri": null,
  "contacts": [
    "jaroslav.guenther@gmail.com"
  ],
  "tos_uri": null,
  "token_endpoint_auth_method": "client_secret_basic",
  "scope": "address scim:read phone email wlcg profile \
    fts:submit-transfer rucio fts fts:submit-transfer",
  "grant_types": [
    "client_credentials"
  ],
  "response_types": [],
  "policy_uri": null,
  "jwks_uri": null,
  "jwks": null,
  "jwksType": "URI",
  "application_type": null,
  "sector_identifier_uri": null,
  "subject_type": null,
  "request_object_signing_alg": null,
  "userinfo_signed_response_alg": null,
  "userinfo_encrypted_response_alg": null,
  "userinfo_encrypted_response_enc": null,
  "id_token_signed_response_alg": null,
  "id_token_encrypted_response_alg": null,
  "id_token_encrypted_response_enc": null,
  "default_max_age": 60000,
  "require_auth_time": true,
  "default_acr_values": null,
  "initiate_login_uri": null,
  "post_logout_redirect_uris": null,
  "claims_redirect_uris": [],
  "request_uris": [],
  "software_statement": null,
  "software_id": null,
  "software_version": null,
  "code_challenge_method": null,
  "registration_client_uri": "https://wlcg.cloud.cnaf.infn.it/register/5b5e5d37-926b-4b42-8a98-a0b4b28baf18",
  "client_secret_expires_at": 0,
  "client_id_issued_at": 1574700703
}
```

To make the Rucio server aware of the two clients above, one has to exchange the
empty dictionary in `etc/idpsecrets.json` file with one containing the
relevant information. Example of such dictionary (for multiple IdPs) follows:

```json
{
    "<IdP nickname>": {

        "issuer": "https://<issuer_server_name>",

        "redirect_uris": [
            "https://<auth_server_name>/auth/oidc_token",
            "https://<auth_server_name>/auth/oidc_code"
        ],

        "client_id": "<C1_client_id>",
        "client_secret": "<C1_client_secret>",

        # this is not really needed for the OIDC functionality
        # but it is suggested to store it anyway as it is required
        # to edit the client in INDIGO IAM
        "registration_access_token": "<C1_client_RAT_string>",

        "SCIM": {
            "client_id": "<C2_client_id>",
            "client_secret": "<C2_client_secret>",
            "registration_access_token": "<C2_client_RAT_string>"
        }
    },

    "wlcg": {

        "issuer": "https://wlcg.cloud.cnaf.infn.it/",

        "redirect_uris": [
            "https://rucio-auth.cern.ch/auth/oidc_token",
            "https://rucio-auth.cern.ch/auth/oidc_code"
        ],

        "client_id": "fdc297fc-09 ...",
        "client_secret": "APFVcga_X ...",
        "registration_access_token": "eyJraWQiOi ...",

        "SCIM": {
            "client_id": "5b5e5d3 ...",
            "client_secret": "IQqAcMOa ...",
            "registration_access_token": "eyJraW ..."
        }
    },

    "xdc": { ... },
}
```

After this is done, please make sure your `rucio.cfg` file contains the
following section:

```cfg
[oidc]
idpsecrets = /path/to/your/idpsecrets.json
admin_issuer = <IdP_nickname>
expected_audience = '<rucio>'
expected_scope = 'openid profile'
```

Parameters __idpsecrets__ and __admin_issuer__ have to be present.
__IdP nickname__ stands for your preferred IdP (e.g. 'wlcg'). The IdP
specified under __admin_issuer__ will be contacted to get information about Rucio
Users (SCIM) and to request tokens for the Rucio __root__ account.  The
__expected_scope__ and __expected_audence__ parameters are optional and if not filled,
the Rucio server will set them to `openid profile` and `rucio`
respectively. The expected scopes and audiences have to be configured
correspondingly on the side of your registered clients at your IdP (usually you
can control accepted scopes and audiences for your clients via an IdP web
interface).

To finalize the process, one should assign the OIDC identities to the relevant
Rucio __admin_account__ (e.g. 'root', 'ddmadmin'). This identity ID is
composed of the Rucio Service IAM Account [A] subject claim and
issuer url such as demonstrated below:

```bash
# Add the Rucio Service IAM Account ID as an OIDC identity
rucio-admin identity add --account rucio_admin_account \
  --type OIDC \
  --id "SUB=b3127dc7-2be3-417b-9647-6bf61238ad01, \
    ISS=https://wlcg.cloud.cnaf.infn.it/" \
  --email "wlcg-doma-rucio@cern.ch"
```

A second identity has to be added to the same __admin_account__ representing
the __client_credentials__ flow of the Rucio application, i.e.  of the
__Rucio Admin IAM Client__ [C2] from above. This identity consists of
the __client_id__ of [C2] and the issuer (the token obtained via the
client credentials flow using [C2] will contain in the __sub__ claim the
__client_id__ of [C2] instead of Rucio Service IAM Account [A] __sub__ claim):

```bash
# Add the Rucio Admin IAM Client client_id as an OIDC identity
rucio-admin identity add --account rucio_admin_account \
  --type OIDC \
  --id "SUB=5b5e5d37-926b-4b42-8a98-a0b4b28baf18, \
    ISS=https://wlcg.cloud.cnaf.infn.it/" \
  --email "wlcg-doma-rucio@cern.ch"
```

Note: In case you can not/will not run any IAM -> Rucio user mapping tool in
order to sync Rucio accounts with their IAM identities, you should assign the
appropriate OIDC identity manually (as in the example above) to each Rucio
account which is meant to use the OIDC authN/Z:

```bash
# Add an IAM User Account ID as an OIDC identity
# (needs to be done for each user!)
rucio-admin identity add --account rucio_user_account \
  --type OIDC \
  --id "SUB=5b5e5d37-926b-4b42-8a98-a0b4b28baf18, \
    ISS=https://wlcg.cloud.cnaf.infn.it/" \
  --email "wlcg-doma-rucio@cern.ch"
```

Finally, in order to ensure the correct lifetime management of the tokens and auth
sessions, one has to run the __oauth-manager__ daemon.

### Configuration for Daemons

OIDC authN/Z is also supported by the Rucio conveyor daemons and more
specifically by the __conveyor-submitter__ and __conveyor-poller__ ones.
__Conveyor-submitter__ is responsible for submission of the transfers created in
connection with an existing Rucio rule. __Conveyor-poller__ is responsible for
polling the state of the transfers that have been submitted and updating the
relevant state in the database.

In order to enable this functionality, RSEs must have an attribute set as follows:

```cfg
oidc_support: True
```

In general, the Rucio account which created such a rule will be used to request a
JWT token for OAuth2 authentication with FTS3. More specifically, there
are three Rucio authentication flows that are possible:

1. __User Token Exchange__: In this case, a valid OIDC token that the user authenticated
   with in Rucio is getting [exchanged](https://indigo-iam.github.io/docs/v/current/user-guide/api/oauth-token-exchange.html)
   with an appropriate token that is intended to be served to the FTS3 server.
   This FTS3 intended token must have a specific audience [*] as well as
   specific scopes [**] that the FTS3 server expects, this applies for the next
   authentication flows as well. It is also worth noting that the acquired FTS3
   intended token includes all original claims that were present in the initial token.

1. __Admin Flow__: In this Rucio authN/Z flow, the [client_credentials](https://auth0.com/docs/get-started/authentication-and-authorization-flow/client-credentials-flow)
   flow is used with the __Rucio Admin IAM Client__ [C2]. The __sub__ claim of the
   acquired token becomes the __client_id__ of [C2]. In this case any group membership
   that was present in the original token is not included in the new FTS3 intended
   token. Additionally, for this flow to be successful a valid user OIDC token
   must already be present in the database.

1. __Admin Root Flow__: This scenario has the same logic as flow 2, with the
   difference that it is used when the relevant rule is created by the
   Rucio __admin_account__ (e.g. 'root').
   No other user token is involved in this case.

In all three formerly mentioned cases, if a valid FTS3 intended token
already exists in the Rucio database then a new token is not requested
and the existing one is used.

The OIDC authentication mechanism shall be configured by the
following parameters in the `rucio.cfg` file:

```cfg
[conveyor]
# if set to True, then only flow 1 will be tried
# if set to False, then flow 1 will never be tried
allow_user_oidc_tokens = False (default)

# FTS3 intended audience [*]
request_oidc_audience = 'fts:example' (default)

# FTS3 intended scopes [**]
request_oidc_scope = 'fts:submit-transfer' (default)
```

For the __conveyor-poller__ to work an additional configuration is needed:

```cfg
[conveyor]
poller_oidc_account = rucio_admin_account
```

On an another level, the __reaper__ daemon can be also configured to
perform deletions of files on the storage by using an OIDC token,
the following configuration is needed:

```cfg
[reaper]
oidc_account = rucio_admin_account
oidc_audience = same logic as [*] but for the storage
oidc_scope = same logic as [**] but for the storage
```

Note aside: For some IdPs it may happen that the scope and audience claims are
not a part of the token payload. For this reason Rucio has a fall-back mechanism
to get this information using the IdPs introspection endpoint. To allow Rucio to
introspect tokens that were not issued by its clients, please talk to the IdP
admin who should enable this functionality for your clients.

### Rucio WebUI Login with CERN SSO

By using the Rucio OIDC capabilities it is possible to integrate the
[CERN SSO](https://auth.docs.cern.ch/user-documentation/oidc/oidc/) service with
the WebUI so users will be able to login with a CERN account.
Please note that in contrast to INDIGO IAM, the CERN IdP can only be
used for WebUI login at the moment and not for the other operations
that were described previously. The following steps are needed:

1. The Rucio administrators need to create a new application at the
   [Application Portal](https://application-portal.web.cern.ch/).
   Please note that the __Application Identifier__ field will be the
   audience claim in the tokens acquired by the CERN Authorization Service.

1. In the newly created Application, a new __SSO Registration__ is needed.
   Please select OIDC in the
   'Which protocol does your application use for authentication?' question.
   At the same time, the two Rucio redirect URIs are needed as
   described in the `etc/idpsecrets.json` configuration that was mentioned previously.

1. The new CERN IdP needs to be added in the `etc/idpsecrets.json` configuration,
   with the newly acquired client secret that was given after step 2.
   Please note that in this case the SCIM field needs to be filled even though
   it will never be used for this IdP, one can just repeat the original
   client_id and client_secret. The configuration will have the following format:

    ```json
    {
        # ...
        "cern": {

            "issuer": "https://auth.cern.ch/auth/realms/cern",

            "redirect_uris": [
                "https://<auth_server_name>/auth/oidc_token",
                "https://<auth_server_name>/auth/oidc_code"
            ],

            "client_id": "<SSO_client_id>", # Same as Application Identifier
            "client_secret": "<SSO_client_secret>",

            "SCIM": {
                "client_id": "<SSO_client_id>",
                "client_secret": "<SSO_client_secret>",
            }
        }
        # ...
    }
    ```

1. Finally, the CERN user identities need to be mapped to Rucio accounts
   as it was previously described. One example mapping follows:

    ```bash
    # Add an CERN User Account Username as an OIDC identity
    # (needs to be done for each user!)
    # Note that the SUB field is the CERN Account username
    rucio-admin identity add --account rucio_user_account \
    --type OIDC \
    --id "SUB=ridona, \
        ISS=https://auth.cern.ch/auth/realms/cern" \
    --email "rucio@cern.ch"
    ```
---
id: policy_packages_overview
title: Policy Packages Overview
---

Policy packages are separate Python packages that can be used to add
experiment-specific customisations to Rucio. They typically customise
Rucio's handling of permissions and schema as well as optionally adding
their own algorithms for various purposes, such as lfn to pfn conversion
and surl construction.

Policy packages may be installed from a Python package repository such
as [PyPi](https://pypi.python.org/) or they may simply be installed in a
local directory. In the latter case this directory will need to be added
to the Rucio server's `PYTHONPATH` environment variable.

The name of the policy package in use is specified by the `package` value
in the `policy` section of the Rucio configuration file. Alternatively,
the package can be specified by the `RUCIO_POLICY_PACKAGE` environment
variable (if both are set, the environment variable takes priority). If
no package is specified, a built in generic policy will be used. If a
package is specified but cannot be loaded, Rucio will exit with an error.

Multi-VO Rucio installations can load a different policy package for each
VO. In this case, the configuration parameter or environment variable name
is suffixed with the VO name (for example, `package-vo1` or
`RUCIO_POLICY_PACKAGE_VO1`).

[The API for policy packages can be found here.](pathname:///html/rse_policies/rsemanager.html)
---
id: policy-package-development
title: Developing a Policy Package
---

The basic elements of a policy package are the following:

- An `__init__.py` file that:
  - indicates the supported Rucio version via the `SUPPORTED_VERSION` field;
  - indicates the algorithms provided by the package (optional)
- A `permission.py` module implementing permission
  customisations (optional).
- A `schema.py` module implementing schema customization (optional).
- One or more files for experiment-specific algorithms (optional).

The recommended Python package layouts can be found [here](https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/). An example `src`-layout based policy package is as such:

```
experiment-rucio-policy-package
│   README.md
│   pyproject.toml
│
└───src
│   │
│   └───experiment-rucio-policy-package
│       │   __init__.py               # required
│       │   permission.py             # optional
│       │   schema.py                 # optional
│       │   pfn2lfn.py                # optional (deterministic scope translation algorithm)
│       │   non_deterministic_pfn.py  # optional (non-deterministic scope translation algorithm)
│       │   ...
```


### `__init__.py`

#### `SUPPORTED_VERSION`

`__init__.py` should define a `str | list[str]` called `SUPPORTED_VERSION`,
indicating the version(s) of Rucio that your package supports.

This is checked against the Rucio server
version to ensure compatibility when loading the policy package.

##### From Rucio 36
From Rucio 36, version checking was modified
to use [PEP-compliant version specifiers](https://peps.python.org/pep-0440/#version-specifiers).

For example, to specify support for the entire Rucio 36 release line (so 36.1.0, 36.2.0...)
without yet supporting Rucio 37,
the [**compatible release** operator](https://peps.python.org/pep-0440/#compatible-release) `~=`
can be used, as seen below.

```python
SUPPORTED_VERSION = '~=36.0'
```

Multiple constraints can be specified, either as a string:

```python
SUPPORTED_VERSION = '~=36.0,!=36.4.0'
```

Or as a list:

```python
SUPPORTED_VERSION = ['~=36.0','!=36.4.0']
```

##### Before Rucio 36

On Rucio versions older than 36, only major versions can be specified.
This can be done as either a string:

```python
SUPPORTED_VERSION = '35'
```

Or as a list, to indicate support for multiple major versions:

```python
SUPPORTED_VERSION = ['34', '35']
```

#### `get_algorithms`

The `__init__.py` file can also contain
an optional function called `get_algorithms` that
returns a dictionary of custom algorithms implemented within the package.
In fact, this structure should be a "dictionary of dictionaries" where
the outer dictionary contains algorithm types, and each inner
dictionary contains all the algorithms provided by the package for that
type. Currently supported types are `lfn2pfn` for generating PFNs for
deterministic storage, `non_deterministic_pfn` for generating PFNs for
non-deterministic storage, and `scope` for scope extraction algorithms.
(For backwards compatibility, `surl` can be used in place of
`non_deterministic_pfn`, however this is not recommended for new policy
packages).

> [!NOTE]
> Some base algorithm classes depend on `schema` being loaded.
> To avoid **circular import** issues,
> import the algorithm classes as part of the `get_algorithms` function,
> to ensure that `schema` is loaded first.

Example:

```python
def get_algorithms():
    from vo_policy_package.non_deterministic_pfn import VONonDeterministicPFNAlgorithm
    from vo_policy_package.lfn2pfn import VORSEDeterministicTranslation
    from vo_policy_package.scope import VOScopeExtractionAlgorithm
    return {
      'non_deterministic_pfn': {
        'voname_non_deterministic_pfn': VONonDeterministicPFNAlgorithm.construct_non_deterministic_pfn_voname
        },
      'lfn2pfn': {
        'voname_lfn2pfn': VORSEDeterministicTranslation.lfn2pfn_voname
        },
      'scope': {
        'voname_extract_scope': VOScopeExtractionAlgorithm.extract_scope_voname
        }
      }
```

In all cases the names used to register the functions (e.g. `voname_extract_scope`) must be prefixed
with the name of the virtual organisation that owns the policy package,
to avoid naming conflicts on multi-VO Rucio installations.

See [the Algorithms documentation page](operator/policy_packages/policy_package_algorithms.md) for more on developing algorithms.

### Permission and schema modules

The `permission.py` and `schema.py` modules are optional; an experiment
that does not need to customise these modules can omit one or both of
them from the policy package and the Rucio generic versions will be
used instead. If these modules are required, the easiest way to create
them is to modify the generic versions from the Rucio codebase. These
can be found in
[`lib/rucio/core/permission/generic.py`](https://github.com/rucio/rucio/blob/master/lib/rucio/core/permission/generic.py)
and [`lib/rucio/common/schema/generic.py`](https://github.com/rucio/rucio/blob/master/lib/rucio/common/schema/generic.py) respectively.

The `has_permission` function in the permission module may return `None`
if your experiment does not implement a custom permission check for a
particular action. In this case, the generic permission module will be
called for this action instead.

The schema module of a policy package does not need to define all of
the schema values. Any missing ones will automatically be loaded from
the generic schema module instead.
---
id: policy-package-deployment
title: Deploying a Policy Package
---

Broadly speaking, three things must happen in order for a policy
package to be deployed successfully:

1. The policy package code must be available to the Rucio server
   (and possibly other components such as daemons).
1. The directory containing the policy package must be in the server's
   `PYTHONPATH`.
1. Rucio must be configured to find the policy package.

## Deploying a policy package

The table below describes the different approaches you can use to deploy a policy package.

| Approach / columns                         	| Ease of setup 	| Amount of work needed for new Rucio and new policy package versions          	| Extra external dependencies at runtime                                                                                            	|
|--------------------------------------------	|---------------	|------------------------------------------------------------------------------	|-----------------------------------------------------------------------------------------------------------------------------------	|
| Kubernetes init container                  	| Easy          	|                                                                              	| Index (e.g. PyPI) or Git registry (e.g. GitLab) in question (if a new policy package version is released)                         	|
| Pass build argument to Dockerfile          	| Easy          	| - Building + hosting the image yourself - Rebuilding the image - Redeploying 	| Registry where you host the image  (no more risk than what is already there,  assuming your registry is as reliable as DockerHub) 	|
| Generate YAML and add as Kubernetes secret 	| Medium        	| - Need to duplicate actual policy package content into configuration         	| None                                                                                                                              	|

### Deploying via Kubernetes init container
In the `values.yaml` for `server` and `daemons` (and optionally for `ui` / `webui`), under `policyPackages`:
1. Set `policyPackages.enabled` to `true`
2. List your policy packages under `policyPackages.packages` in the following format:
```
    example: install from an index (default is PyPI)
    - moduleName: vo_1_policy_package
      requirement: vo_1_policy_package==1.4.0
      version: 1.4.0
    example: install from a git repository
    - moduleName: vo_2_policy_package
      requirement: git+https://github.com/vo-2/vo-2-policy-package@v0.1.0
      version: 0.1.0
```
3. (Optional) set `policyPackages.pvc.createPvc` to true to create a PVC for the policy packages; leave false if providing it separately.

### Deploying via Dockerfile build argument
1. In the `server`, `clients`, `daemons`, `ui` and `init` containers, pass the `POLICY_PACKAGE_REQUIREMENTS` build argument. Example:
Example:
```
docker build -t server --build-arg POLICY_PACKAGE_REQUIREMENTS=vo_1_policy_package==0.4.0,git+https://github.com/vo-2/vo-2-policy-package@v0.1.0
```

### Deploying via Kubernetes secret
You can generate `yaml` for all the files included in your policy package,
and add them as Kubernetes secrets.
You can find information in the [Kubernetes guide](operator/k8s_guide.md) on how to create and manage secrets.

This process can be somewhat automated by having a cronjob that creates the secret policy and applies it.


## Configuring Rucio to find a policy package
To configure Rucio, you should either:
1. Modify the configuration file, by adding the package name as `package = name` in
the `[policy]` section, or
2. Setting the the package name in the `RUCIO_POLICY_PACKAGE`
environment variable.

When deploying using Kubernetes and Helm charts, it is possible to specify
configuration options in `values.yaml`. Values included in the `config:`
section of this file are automatically merged into `rucio.cfg` by the
`docker-entrypoint.sh` script, so the policy package can be set as:

```yaml
config:
  policy:
    package: name
```
---
id: policy-package-algorithms
title: Developing Policy Package algorithms
---

A policy package can be used to define custom algorithms
based on core Rucio algorithms, in order to support custom logic.

The current core algorithms are:
- [`NonDeterministicPFNAlgorithms`](https://github.com/rucio/rucio/blob/0d44febcfd5d0a773a24d60668177324c534bd18/lib/rucio/common/utils.py#L384): Construct PFNs for non-deterministic RSEs
- [`AutoApprove`](https://github.com/rucio/rucio/blob/0d44febcfd5d0a773a24d60668177324c534bd18/lib/rucio/core/rule.py#L96): Handle automatic approval for replication rules
- [`ScopeExtractionAlgorithms`](https://github.com/rucio/rucio/blob/0d44febcfd5d0a773a24d60668177324c534bd18/lib/rucio/common/utils.py#L546): Extract scope from DID
- [`RSEDeterministicScopeTranslation`](https://github.com/rucio/rucio/blob/0d44febcfd5d0a773a24d60668177324c534bd18/lib/rucio/rse/translation.py#L31): Translate a PFN dictionary into a scope and name
- [`RSEDeterministicTranslation`](https://github.com/rucio/rucio/blob/0d44febcfd5d0a773a24d60668177324c534bd18/lib/rucio/rse/translation.py#L102): Translate an LFN to a path
- [`FTS3TapeMetadataPlugin`](https://github.com/rucio/rucio/blob/0d44febcfd5d0a773a24d60668177324c534bd18/lib/rucio/transfertool/fts3_plugins.py#L29): Add `archive_metadata` to FTS transfers to `TAPE`.

Most of these algorithms implement some default behaviour.
It is recommended to check this default behaviour to see
if it is suitable for your needs; if it needs to be changed,
please see below for how to create custom algorithms.

For `FTS3TapeMetadataPlugin` in particular,
please check the (FTS3 Transfertool Plugin documentation)[operator_transfers/configure_fts3_plugins.md].

## Developing a custom algorithm
To develop a custom algorithm:

1. Create a new class for your algorithm, subclassing the relevant core algorithm
2. Register the custom algorithm with a name that is unique in relation to all other algorithms of that type
(i.e. you cannot have two `RSEDeterministicScopeTranslation`-based algorithms with the same name,
but you can have a `RSEDeterministicScopeTranslation`-based algorithm and a `ScopeExtractionAlgorithms`-based algorithm
with the same name. In general, using your VO name should suffice.)
3. Trigger registration of the algorithm by calling the relevant class method at the bottom of your file

As an example, this is the custom `RSEDeterministicScopeTranslation` algorithm used in ATLAS:

```python
class ATLASScopeExtractionAlgorithm(rucio.common.utils.ScopeExtractionAlgorithms):
    def __init__(self) -> None:
        """
        Initialises scope extraction algorithm object
        """
        super().__init__()

    @classmethod
    def _module_init_(cls) -> None:
        """
        Registers the included scope extraction algorithms
        """
        cls.register('atlas', cls.extract_scope_atlas)

    @staticmethod
    def extract_scope_atlas(did: str, scopes: Optional['Sequence[str]']) -> 'Sequence[str]':
        # Try to extract the scope from the DSN
        if did.find(':') > -1:
            if len(did.split(':')) > 2:
                raise RucioException('Too many colons. Cannot extract scope and name')
            scope, name = did.split(':')[0], did.split(':')[1]
            if name.endswith('/'):
                name = name[:-1]
            return scope, name
        else:
            scope = did.split('.')[0]
            if did.startswith('user') or did.startswith('group'):
                scope = ".".join(did.split('.')[0:2])
            if did.endswith('/'):
                did = did[:-1]
            return scope, did


ATLASScopeExtractionAlgorithm._module_init_()
```

## Registering a custom algorithm in your policy package

The system for registering algorithms within policy packages is
intended to be extensible so that new algorithm classes can be added
relatively easily. The basic workflow is as follows:

- The `get_algorithms` function within the policy package
  should return a dictionary of functions of the new class, indexed
  by name
- The core Rucio code should maintain a dictionary of functions of the
  new class, ready to be called when required. The details of this
  will differ depending on what the new class actually does and how it
  integrates with the Rucio code, but typically the algorithm name to
  be used will be selected by a value in the config file, as for the
  current `lfn2pfn` and `non_deterministic_pfn` algorithm types.

## lfn2pfn vs. non_deterministic_pfn algorithms

`lfn2pfn` algorithms and `non_deterministic_pfn` algorithms are
conceptually similar, but there are important differences between
them. Both produce a physical filename for use on an RSE, however
`lfn2pfn` algorithms can only be used on deterministic RSEs - for
example, disk systems where the appropriate physical filename can be
derived from the file's scope and name alone (as well as
protocol-specific information for the RSE in question).
`non_deterministic_pfn` algorithms are used on non-deterministic
RSEs (most often tape systems), and may use additional information
about the file (such as its metadata, any datasets that it is a part
of, etc.) to construct the physical filename. Because files cannot
be uploaded directly to non-deterministic storage,
`non_deterministic_pfn` algorithms are only ever called for
replications, but `lfn2pfn` algorithms can also be called for
initial uploads.

The `lfn2pfn` algorithm to be used is determined by the
`lfn2pfn_algorithm` attribute of the relevant RSE. If this is not set,
the `lfn2pfn_algorithm_default` value from the `[policy]` section of
the config file is used instead. The `non_deterministic_pfn` algorithm
to be used is determined by the `naming_convention` attribute of the
relevant RSE.
---
id: multi_vo_rucio
title: Multi-VO Rucio
sidebar_label: Multi-VO Rucio
---

Multi-VO Rucio configuration allows a single instance of Rucio to support
multiple experiments or Virtual Organisations (VOs). Multi-VO Rucio are run by a
"super_root" which administers the Rucio instance, creating VOs within Rucio,
each VO is then administered by VO specific "root" accounts created with the VO
that deal with the VO requirements and needs. Each VO uses Rucio as a normal or
"Single-VO" instance, meaning a Rucio instance can transition to support more
VOs without significant disruption. Each VO's accounts, scopes and RSEs are
associated with their VO which ensures all rules and replicas are kept separate
from other VOs using the instance.

## Changes to the Client

To utilise the Rucio client against a Multi-VO Rucio the client needs to first
know if it is interacting with a multi-VO Rucio instance, then which VO the client
should be accessing. This is done in the `rucio.cfg` as shown below:

```cfg
[common]
...
multi_vo = True

[client]
...
vo = abc
```

## Changes to the rucio.cfg on the Server and Daemons

Similar settings need to be changed on the server and daemon rucio.cfg files as
well as on the client end. For the server, `multi_vo` should also be set in the
config file. For the daemons, another section needs to be added: this is to
map each VO to its own proxy certificate. Rucio uses this information when
submitting and polling transfers to use the correct certificates.

```cfg
[common]
...
multi_vo = True

[vo_certs]
...
<3 char vo name> = <path/to/vo/proxy>
```

It is recommended that the proxies are placed in /tmp/x509up\_[VO], and the
certificates and keys are placed in /opt/rucio/certs/[VO]/ and
/opt/rucio/keys/[VO]/ respectively.

Unlike the Rucio client, `vo` in the `client` section should not be configured
for the server and daemons, unless specifically to ensure certain daemons act
for one VO alone:

```cfg
[common]
...
multi_vo = True
```

For the daemons, files and configuration are needed to allow daemons to act on
the various VOs: this includes the VO specific certificates, keys, and proxies,
as well as an additional configuration section which maps each VO to its
respective x.509 authentication credentials. Rucio uses this information when
submitting and polling transfers to use the correct certificates.

```yaml
[vo_certs]
 ...
[3 char vo name] = [path/to/vo/proxy]
```

## Role of the super_root

For overall administration of Multi-VO Rucio another layer of admin role has
been created outside of the VO structure. This means each VO has its own
root/admin accounts still retain their administrative role within a VO, for
example adding and editing accounts, adding and modifying RSEs for the VO.
Functions relating to the creation and management of VOs are handled by the
super_root account. It is worth noting that the super_root account **cannot** be
used to perform individual VO administration; the roles of super_root and root
are distinct.

## Access of super_root Functions

As the super_root functions aren't intended for use by normal users of admins,
they do not have an implementation in the client or CLI.

The super_root functions can be accessed from the core or the
:ref:`vo-rest-api`. Access to the functions through the API will require the VO
endpoint to be added to the endpoints list used when setting up the server as it
is disabled by default.

## Starting a M-VO Instance

When bootstrapping the database as part of the Rucio installation, if M-VO is
enabled in `rucio.cfg` then the super_root account is created automatically. The
default VO "def" is also created, and the super_root account is associated with
it. The identity used to access this account can be managed in the usual way.

## Creating VOs

When creating a new VO with the `add_vo` function you need to specify the three
character identifier for the new VO, which can contain letters and numbers. This
must be unique for the instance (A long VO name can be enabled for usage if
required, as shown in the [`Long VO Name Mapping`](#long-vo-name-mapping)
section). A more complete description can also be optionally included, along
with an email to use for the root of this new VO. As the new VO is created, its
corresponding root account is also created, and has all identities associated
with super_root added to it. The identities for the VO root can then be
configured as usual.

## Long VO Name Mapping

The rucio database stores all VO references as a single three-character tag for
performance reasons. It's possible to create aliases for these tag to allow
users/clients to specify long VO names when getting a token (and modifying VOs)
and have these converted to the internal tag automatically. Long VO names should
only use the basic DNS name character set of alphanumeric characters, hyphen
and dot (a-zA-Z0-9-.). The alias mappings are stored in the vo-map section of
the configs database table and can be edited via the commands below. The option
name is the long VO name and the value is the short name; for example these can
be added using the CLI:

```bash
rucio-admin config set --section vo-map --option my.long.vo --value mlv
rucio-admin config set --section vo-map --option another.vo --value ant
```

You may specify more than one alias for a VO if required.

## Managing VOs

Super_root can also change the description and email for a VO using the `update_vo`
API call. If a VO root user loses access to their account, the super_root can
associate a new identity with it using `recover_vo_root_identity`. Finally, a
list of current VOs and their descriptions is accessible via `list_vos`.

## Converting Existing Instances

Rather than initialising a new Rucio instance to support multiple VOs, a
single-VO instance of Rucio can be converted to a Multi-VO instance if desired.
This conversion allows the Rucio instance to expand the number of supported VOs
with minimal disruption. The tools to perform this can be found in
[`rucio/tools/convert_database_vo.py`](https://github.com/rucio/rucio/blob/master/tools/convert_database_vo.py),
and further documentation on the [single VO to multi VO](#s-vo-to-m-vo) and
[multi-VO to single VO](#m-vo-to-s-vo) instances are found below.

The function `convert_to_mvo` facilitates the conversion of a single-VO instance
to a multi-VO instance, where `convert_to_svo` performs the opposite. VOs can
also be renamed using `rename_vo`, or deleted using `remove_vo`. The conversion
functions are callable by using the command line with details on what each
function requires to be carried out, as well as various optional arguments.

Remember that after any database conversion tools are used to update the `rucio.cfg`
appropriately, this may include adding the `multi_vo = True`, as found in
[`changes to the rucio.cfg section`](#changes-to-the-ruciocfg-on-the-server-and-daemons).

These above tools will allow Rucio to change its VO support model. However,
when converting an existing instance, any entries already in the database will
not be associated with a VO (or associated with their old one if previously in
M-VO mode). In order to change these, direct operations on the database are
required. These commands are generated using SQLAlchemy, and can either be run
directly on the database or printed out and run manually.

## Practicalities

Before attempting to convert existing data, it is recommended that a backup of
the database is taken in case any issues arise. Furthermore, of the databases
supported by Rucio, only PostgreSQL has been tested on real data. Based on this
test (which was performed on a machine with 64GB memory and four Intel Xeon
E5-2430 v2), the tables with 2 columns that needed updating were converted at a
rate of 5GB of data per hour. However many tables do not need any changes, so
the process will likely be faster than this in practice. Another approach to
speed up the conversion is to skip the "history" tables, as these can be very
large. Unlike other tables these do not have foreign key constraints set, and so
do not need to be updated in order to use the database. While the history will
be inaccessible from the new VO, it will still exist in the database and could
be accessed using the `super_root` account if needed.

## S-VO to M-VO

Before starting, ensure that `multi_vo` is set to `True` in the config file. The
SQL commands needed to convert the database involve dropping foreign key
constraints that affect accounts/scopes, then altering the relevant columns,
before re-adding the constraints. The 3 character identifier for the VO, a full
description and an admin email should be provided:

```bash
$ tools/convert_database_vo.py convert_to_mvo new "New VO for existing data" rucio@email.com
ALTER TABLE account_limits DROP CONSTRAINT "ACCOUNT_LIMITS_ACCOUNT_FK";
...
UPDATE account_limits SET account=(split_part(account_limits.account, '@', 1) \
  || CAST('@new' AS CHAR(4))) WHERE \
    split_part(account_limits.account, '@', 2) = '';
...
ALTER TABLE account_limits ADD CONSTRAINT "ACCOUNT_LIMITS_ACCOUNT_FK" \
  FOREIGN KEY(account) REFERENCES accounts (account);
```

In this example, no changes will be made to the database by running the script,
and so the SQL will need to be run manually. After running the commands, a
`super_root` account should be setup to allow administrative functions like
adding more VOs:

```bash
$ python
>>> from rucio.db.sqla.util import create_root_account
>>> create_root_account(create_counters=False)
```

Alternatively by specifying `--commit_changes` the script will attempt to modify
the database as it runs, however this requires the account used by the Rucio
instance to access the database to be the owner of the the tables. In this case,
the `super_root` account can be added as part of the script by passing the
argument `--create_super_root`. If there is an error during the conversion, then
none of the changes will be committed.

```bash
$ tools/convert_database_vo.py --commit_changes \
  convert_to_mvo new "New VO for existing data" rucio@email.com \
  --create_super_root
```

Finally, there is the option to skip the (potentially very large) tables of
historical data using `--skip_history`. In this case the commands to alter those
tables are omitted:

```bash
$ tools/convert_database_vo.py --skip_history \
  convert_to_mvo new "New VO for existing data" rucio@email.com
```

## M-VO to S-VO

Before starting, ensure that `multi_vo` is set to `True` in the config file
(this option can be removed after completing the conversion). The first stage of
the conversion is the same as before, dropping foreign key constraints and
renaming the entries that were associated with the old VO. The name of this VO
is the only required argument:

```bash
$ tools/convert_database_vo.py
convert_to_svo old ALTER TABLE account_limits DROP CONSTRAINT
"ACCOUNT_LIMITS_ACCOUNT_FK"; ...  UPDATE account_limits SET
account=split_part(account_limits.account, '@', 1) WHERE
split_part(account_limits.account, '@', 2) = 'old'; ...  ALTER TABLE
account_limits ADD CONSTRAINT "ACCOUNT_LIMITS_ACCOUNT_FK" FOREIGN KEY(account)
REFERENCES accounts (account);
```

By default, data associated with any other VOs is left in the database, but will
be inaccessible to Rucio users.
These entries can be completely deleted from the database
by passing the `--delete_vos` argument.

```bash
tools/convert_database_vo.py convert_to_svo old --delete_vos ...  \
  DELETE FROM account_limits WHERE \
    split_part(account_limits.account, '@', 2) = 'xyz'; \
  ...  DELETE FROM account_limits WHERE \
    split_part(account_limits.account, '@', 2) = '123'; ...
```

Once again, historical tables skipped with `--skip_history`, and the commands
can be run directly against the database using the `--commit_changes` argument;
if this is not set then the `super_root` account should be manually deleted
after running the SQL:

```bash
$ python
>>> from rucio.common.types import InternalAccount
>>> from rucio.core.account import del_account
>>> del_account(InternalAccount('super_root', vo='def'))
```
---
title: Monitoring
sidebar_label: Monitoring
---

There are three different monitoring components:

- Rucio internal monitoring using Graphite/Grafana
- Transfer monitoring using the messages sent by Hermes
- File/Dataset Access monitoring using the traces

## Internal Monitoring

This is to monitor the internals of Rucio servers and daemons, e.g., submission
rate of the conveyor, state of conveyor queues, reaper deletion rate, server
response times, server active session, etc. We use Graphite[^1] for this. It's
easy to setup and then you have to point your Rucio instance to the Graphite
server using the \"carbon_server" options in the "monitor" section in
etc/rucio.cfg.

The different Rucio components will then send metrics using those "record"
functions you will find all over the code. Graphite has a built-in web interface
to show graphs but more comfortable to use is the Grafana[^2] tool.

The internal monitoring functions are defined in core/monitor.py, it includes:

1) record_counter. This is to send the StatsD counter metrics. Counters are the
most basic and default type. They are treated as a count of a type of event per
second, and are, in Graphite, typically averaged over one minute. That is, when
looking at a graph, you are usually seeing the average number of events per
second during a one-minute period.

2) record_timer. Timers are meant to track how long something took. They are an
invaluable tool for tracking application performance. The statsd server collects
all timers under the stats.timers prefix, and will calculate the lower bound,
mean, 90th percentile, upper bound, and count of each timer for each period (by
the time you see it in Graphite, that’s usually per minute).

3) record_timer_block. This is the same to record_timer, just for simple using,
to calculate timer of a certain code block.

4) record_gauge. Gauges are a constant data type. They are not subject to
averaging, and they don’t change unless you change them. That is, once you set a
gauge value, it will be a flat line on the graph until you change it again.

### Set up the Rucio internal monitoring dashboard

Set up a rucio server for development

```bash
git clone https://github.com/rucio/rucio.git
docker-compose --file etc/docker/dev/docker-compose.yml up -d
```

The command will fire up various containers such as `dev-rucio-1`, `dev-graphite-1`, and
`dev-activemq-1`. `dev-graphite-1` is the one collecting internal
metrics from Rucio. The configurations of Rucio internal metrics sender are
defined under the [monitor] section of rucio.cfg. Change the carbon_server and
carbon_port according to your setting

```toml
[monitor]
carbon_server = graphite
carbon_port = 8125
user_scope = docker
```

The Graphite builtin web page is on port 80 of the host. To use Grafana, setup
Grafana and enable the graphite data source

```bash
docker pull grafana/grafana
docker run -d --name=grafana -p 3000:3000 grafana/grafana
```

The Grafana web-portal is on port 3000 of the host. Add one data source of the
type Graphite, choose access method to "Browser" and set URL to [http://ip:80](http://ip:80),
where ip is the address of the server hosting the Graphite container
`dev-graphite-1`.

A set of pre-defined Grafana Rucio internal plots is provided
[here](https://github.com/rucio/rucio/blob/master/tools/monitoring/visualization/rucio-internal.json).
Users could import them directly into Grafana.

### The list of Rucio internal metrics

1) Core

```text
credential.signswift, credential.signs3 (timer)
trace.nongrid_trace
core.request.* (counter)
core.request.archive_request.* (timer)
rule.add_rule, rule.add_rule.*, rule.delete_rule, rule.evaluate_did_detach, rule.evaluate_did_attach.(timer)
trace.trace (counter)
```

2) Transfertool

```text
transfertool.fts3.delegate_proxy.success.*, \
  transfertool.fts3.delegate_proxy.fail.* (timer)
transfertool.fts3.submit_transfer.[externalhost] (timer)
transfertool.fts3.[externalhost].submission.success/failure (counter)
transfertool.fts3.[externalhost].cancel.success/failure (counter)
transfertool.fts3.[externalhost].update_priority.success/failure  (counter)
transfertool.fts3.[externalhost].query.success/failure  (counter)
transfertool.fts3.[externalhost].whoami.failure (counter)
transfertool.fts3.[externalhost].Version.failure (counter)
transfertool.fts3.[externalhost].query_details.failure (counter)
transfertool.fts3.[externalhost].bulk_query.failure (counter)
transfertool.fts3.[externalhost].query_latest.failure (counter)
transfertool.fts3myproxy.[externalhost].submission.success/failure (counter)
```

3) Judge

```text
rule.judge.exceptions.*
```

4) Transmogrified

```text
transmogrifier.addnewrule.errortype.* (counter)
transmogrifier.addnewrule.activity.* (counter)
transmogrifier.did.*.processed (counter)
```

5) Tracer

```text
daemons.tracer.kronos.* (counter)
```

6) Reaper

```text
reaper.list_unlocked_replicas, reaper.delete_replicas (timer)
reaper.deletion.being_deleted, reaper.deletion.done (counter)
daemons.reaper.delete.[scheme].[rse] (timer)
```

7) Undertaker

```text
undertaker.delete_dids, undertaker.delete_dids.exceptions.LocksDetected (counter)
undertaker.rules, undertaker.parent_content, undertaker.content, \
  undertaker.dids (timer)
undertaker.content.rowcount (counter)
```

8) Replicarecover

```text
replica.recoverer.exceptions.* (counter)
```

9) Hermes

```text
daemons.hermes.reconnect.* (counter)
```

10) Conveyor

```text
daemons.conveyor.[submitter].submit_bulk_transfer.per_file, \
  daemons.conveyor.[submitter].submit_bulk_transfer.files (timer)
daemons.conveyor.[submitter].submit_bulk_transfer (counter)
daemons.conveyor.finisher.000-get_next (timer)
daemons.conveyor.finisher.handle_requests (timer & counter)
daemons.conveyor.common.update_request_state.request-requeue_and_archive (timer)
daemons.conveyor.poller.000-get_next (timer)
daemons.conveyor.poller.bulk_query_transfers (timer)
daemons.conveyor.poller.transfer_lost (counter)
daemons.conveyor.poller.query_transfer_exception (counter)
daemons.conveyor.poller.update_request_state.* (counter)
daemons.conveyor.receiver.error
daemons.conveyor.receiver.message_all
daemons.conveyor.receiver.message_rucio
daemons.conveyor.receiver.update_request_state.*
daemons.conveyor.receiver.set_transfer_update_time
daemons.messaging.fts3.reconnect.*
daemons.conveyor.stager.get_stagein_transfers.per_transfer, \
  daemons.conveyor.stager.get_stagein_transfers.transfer (timer)
daemons.conveyor.stager.get_stagein_transfers (count)
daemons.conveyor.stager.bulk_group_transfer (timer)
daemons.conveyor.submitter.get_stagein_transfers.per_transfer, \
  daemons.conveyor.submitter.get_stagein_transfers.transfer (timer)
daemons.conveyor.submitter.get_stagein_transfers (count)
daemons.conveyor.submitter.bulk_group_transfer (timer)
daemons.conveyor.throttler.set_rse_transfer_limits.\
  [rse].max_transfers/transfers/waitings (gauge)
daemons.conveyor.throttler.delete_rse_transfer_limits.[rse] (counter)
daemons.conveyor.throttler.delete_rse_transfer_limits.[activity].[rse] (counter)
daemons.conveyor.throttler.set_rse_transfer_limits.[activity].[rse] (gauge)
daemons.conveyor.throttler.release_waiting_requests.[activity].[rse].[account] (counter)
```

11) Necromancer

```text
necromancer.badfiles.lostfile, necromancer.badfiles.recovering (counter)
```

## Transfer monitoring

If a transfer is submitted, queued, waiting, done or failed messages are sent to
ActiveMQ via Hermes and are also archived in the messages_history table. Same is
true for deletions. In the case of ATLAS we have a dedicated monitoring
infrastructure that reads the messages from
[ActiveMQ](https://activemq.apache.org), aggregates them and then writes the
aggregated data into ElasticSearch/InfluxDB from where it then can be visualised
using Kibana/Grafana.

### Set up the Rucio internal monitoring dashboard

1) Configure Rucio

Rucio need to be configured to enable the message broker. In Rucio, message are
sent by the Hermes daemon. Settings are defined in therucio.cfg under the
[messaging-hermes] section

```toml
[messaging-hermes]
username =
password =
port = 61613
nonssl_port = 61613
use_ssl = False
ssl_key_file = /etc/grid-security/hostkey.pem
ssl_cert_file = /etc/grid-security/hostcert.pem
destination = /topic/rucio.events
brokers = activemq
voname = atlas
email_from =
email_test =
```

The default settings are listed above. If ssl is not used, set use_ssl to False
and define username and password. They should be "admin", "admin" for the
default activemq settings. If you are not using the containers created by the
docker-compose command, change the brokers and port to the server hosting the
message queue.

2) Setup Elasticsearch and Kibana

Next is to setup and configure Elasticsearch and Kibana for storing and
visualising the messages. This is an example of creating them in containers

```bash
docker run -d \
  -p 9200:9200 \
  -p 9300:9300 \
  -e "discovery.type=single-node" \
  --name elasticsearch \
  docker.elastic.co/elasticsearch/elasticsearch:7.8.1

docker run -d \
  --link elasticsearch \
  -p 5601:5601 \
  --name kibana \
  docker.elastic.co/kibana/kibana:7.8.1
```

3) Import Elasticsearch indices

Before transferring messages from the message queue to Elasticsearch, indices
need to be defined in Elasticsearch. This is a list of the message formats of
Rucio.

### Transfer events

```jsi
{
  created_at: when the message was created (yyyy-MM-dd HH:mm:ss.SSSSSS)
  event_type: type of this event (transfer-submitted, \
    transfer-submittion_failed, transfer-queued, transfer-failed, \
    transfer-done)
  payload: {
    account: account submitting the request
    activity: activity of the request
    bytes: size of the transferred file (byte)
    checksum-adler: checksum using adler algorithm
    checksum-md5: checksum using md5 algorithm
    created_at: Time when the message was created (yyyy-MM-dd HH:mm:ss.SSSSSS)
    dst-rse: destination rse
    dst-type: type of destination rse (disk, tape)
    dst-url: destination url of the transferred file
    duration: duration of the transfer (second)
    event_type: type of this event (transfer-submitted, \
      transfer-submittion_failed, transfer-queued, \
      transfer-failed, transfer-done)
    file-size: same as bytes
    guid: guid of the transfer
    name: name of transferred file
    previous-request-id: id of previous request
    protocol: transfer protocol
    reason: reason of the failure
    request-id: id of this request
    scope: scope of the transferred data
    src-rse: source rse
    src-type: type of source rse (disk, tape)
    src-url: source file url
    started_at: start time of the transfer
    submitted_at: submission time of the transfer
    tool-id: id of the transfer tool in rucio (rucio-conveyor)
    transfer-endpoint: endpoint holder of the transfer (fts)
    transfer-id: uuid of this transfer
    transfer-link: link of this transfer (in form of fts url)
    transferred_at: done time of this transfer
  }
}
```

### Deletion events

```json
{
  created_at: when the message was created (yyyy-MM-dd HH:mm:ss.SSSSSS)
  event_type: type of this event (deletion-done,deletion-failed)
  payload: {
    scope: scope of the deleted replica
    name: name of the deleted replica
    rse: rse holding the deleted replica
    file-size: size of the file
    bytes: size of the file
    url: url of the file
    duration: duration of the deletion
    protocol: prococol used in the deletion
    reason: reason of the failure
  }
}
```

The formats of them are defined in [`rucio-transfer.json`](https://github.com/rucio/rucio/blob/master/tools/monitoring/rucio-transfer.json)
and [`rucio_deletion.json`](https://github.com/rucio/rucio/blob/master/tools/monitoring/rucio-deletion.json)
which could be imported into Kibana.

Rucio also sends messages when adding/deleting rules/DIDs and for file/dataset
access. So the monitoring is not limited to data transferring.

4) Transmit messages from message queue to Elastisearch

This could be done via Logstash. Please refer to [Elastic's documentation.](https://www.elastic.co/blog/integrating-jms-with-elasticsearch-service-using-logstash).

Alternatively you could use a simple Python script such as [`extract.py`](https://github.com/rucio/rucio/blob/master/tools/monitoring/extract.py) for
this after installing the required tools

```bash
pip install --upgrade pip
pip install elasticsearch
wget https://files.pythonhosted.org/packages/52/7e/22ca617f61e0d5904e06c1ebd5d453adf30099526c0b64dca8d74fff0cad/stomp.py-4.1.22.tar.gz
tar -zxvf stomp.py-4.1.22.tar.gz
cd stomp.py-4.1.22
python setup.py install
```

Change the configurations (message broker and elastisearch cluster) in
exporter.py and start it. It could be made as a systemd service for convenience.

5) Create Kibana dashboards based on the imported messages.

A set of pre-defined dashboards can be found
[here](https://github.com/rucio/rucio/tree/master/tools/monitoring/visualization) in
json format which could be imported to Kibana directly. But you may have to
resolve different UUIDs in Kibana.

## Access monitoring

The traces are sent by the pilots or the rucio clients whenever a file is
downloaded/uploaded. This is similar with the data transferring monitoring.

## Rucio database dumping

Besides the internal, data transferring/deletion/accessing monitoring, it's also
possible to dump the Rucio internal database directly to Elasticsearch. Then
information like data location, accounting, RSE summary could be visualised
using Kibana or Grafana.

We provide several examples of dumping Rucio DB tables using the logstash jdbc
plugin and making plots based on them.

To start a logstash pipeline, run

```bash
logstash -f rse.conf
```

Where the rse.conf contains

```json
input {
  jdbc {
    jdbc_connection_string => ""
    jdbc_user => ""
    jdbc_password => ""
    jdbc_driver_library => "/usr/share/logstash/java/postgresql-42.2.6.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    statement => "SELECT rses.rse, rse_usage.source, rse_usage.used, \
      rse_usage.free, rse_usage.files FROM rse_usage INNER JOIN rses ON \
      rse_usage.rse_id=rses.id WHERE rse_usage.files IS NOT NULL AND \
      rse_usage.files!=0;"
  }
}
output {
  elasticsearch {
    hosts => [""]
    action => "index"
    index => "rucio_rse"
    user => ""
    password => ""
  }
}
```

The rse pipeline dumps data like how large is the total space, how large is the
used space, how many files are saved on each RSE etc. Please fill in the jdbc
connection details and Elastisearch connection details in the config file.

More pipeline definitions can be found [here](https://github.com/rucio/rucio/tree/master/tools/monitoring/logstash-pipeline),
and users could design their own DB queries for their specific monitoring
needs. Also users could directly import the Elasticsearch indices and Kibana
dashboard from [these](https://github.com/rucio/rucio/tree/master/tools/monitoring/visualization/db_dump).
json files.

## Footnotes

[^1]: [https://graphiteapp.org/]
[^2]: [https://grafana.com/]
---
id: database
title: Database Operations
sidebar_label: Database Operations
---

## Supported databases

Rucio uses [SQLAlchemy](https://www.sqlalchemy.org/) as the object-relational
mapper between Python and SQL. Rucio is extensively tested against SQLite,
MySQL/MariaDB, PostgreSQL, and Oracle and should work with them out of the
box. The appropriate connection string has to be set in the `etc/rucio.cfg`, for
example:

Oracle:
`oracle://rucio:rucio@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=10121))(ENABLE=BROKEN)(CONNECT_DATA=(SERVICE_NAME=localhost)))`

MySQL/MariaDB: `mysql+pymysql://rucio:rucio@localhost/rucio`

PostgreSQL: `postgresql://rucio:rucio@localhost:6601/rucio`

SQLite: `sqlite:////tmp/rucio.db`

Please ensure correct UNIX permissions on the SQLite file, such that the web
server process can read and write to it.

## Additional options

### Connection pooling

Connection pooling is enabled by default, but can be disabled by setting the option

```dosini
poolclass = nullpool
```

in the `[database]` section in `etc/rucio.cfg`.

Other valid values are `singletonthreadpool`,
which is the default pooling mechanism when using the SQLite engine,
and `queuepool`,
which is the default otherwise.

Note that the chosen `poolclass` may conflict with other pooling options.
For instance, one cannot combine `poolclass = nullpool` with the `pool_size` option.

## Upgrading and downgrading the database schema

Rucio uses [Alembic](http://alembic.zzzcomputing.com/en/latest/) as a database
migration tool, which works hand-in-hand with
[SQLAlchemy](https://www.sqlalchemy.org/). Ensure that in `etc/alembic.ini` the
database connection string is is set to the same database connection string as
the `etc/rucio.cfg` and issue the following command to verify the changes to the
upgrade of the schema:

`alembic upgrade --sql $(alembic current | cut -d' '-f1):head`

You can edit and then apply the SQL directly on your database.

`alembic upgrade head`

We do not advise running automatic upgrades/downgrades with alembic.

Notabene, schema upgrades are reserved for feature releases and will not happen
with patch releases.

## Creating a new version as a developer

If you want to create an upgrade path for the schema, you need to generate a
schema upgrade+downgrade file:

`alembic revision -m 'schema change message'`

This will output the name of the file that has been generated with two functions
`def upgrade()` and `def downgrade()` that need to be implemented. These should
reflect the changes to the `lib/rucio/db/sqla/models.py` SQLAlchemy mapping.
3
---
id: join_rucio_mattermost
title: How to join Rucio mattermost
---

We have a dedicated Mattermost Workspace where developers and administrators exchange and discuss all things Rucio.
The Rucio Mattermost workspace is hosted on the CERN Mattermost service which requires a CERN account to join.

Please note that this is not an user support workspace, and we cannot help with experiment-specific user questions.
## Join with a CERN account

1. Open https://mattermost.web.cern.ch/rucio/ in your browser or with a
mattermost client application.

## Join without a CERN account

1. Open https://mattermost.web.cern.ch/rucio/ in your browser or with a
mattermost client application.

2. Click on "Login with CERN Single Sign-On"

3. Use one of the options on the right to sign in with your eMail, organisation,
or to sign in with a social account.
